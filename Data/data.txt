113
online forums vital resource users questions participate discussions search functionality forum sites primitive; posts searched keywords retrieved creation interactive social web forum sites users frequently connections users shared profiles critical challenge score rank forum posts account relations users paper personalized search forums leverages user similarities developed multiple relations linking users build multidimensional random walk model uniformly incorporates heterogeneous user relations forum participants multi-relational user similarity predict future interactions personalizing answer search extend methods enhance keyword search forum readers expertise scores existing forum participants leveraging author dimension retrieve relevant traditional ir scoring 
internet resource adolescents distressed social emotional social network analysis provide opportunities helping people seeking support online understand salient issues highly relevant participants personal circumstances paper stacked generalization modeling approach analyze online community supporting adolescents duress traditional predictive supervised methods rely robust hand-crafted feature space engineering mixed initiative semi-supervised topic models extracting high-level themes feature spaces strategy combines strengths types models inspired prevention science approaches deals identification amelioration risk factors predict psychological psychosocial psychiatric disorders populations teenagers treat post-facto study prevention scientists social science thematic analytic approach code stories fine-grained analysis salient social developmental psychological themes deemed relevant analyzed society models stacked generalization ensemble fares individual binary predictive models 
fledgling online communities hope achieve critical mass community sustainable concept understood community achieve critical mass community online communities sustainable achieve mass explore question analyzing growth online communities wikipedia individual communities patterns growth membership pattern growth contribution production stages community development building membership impact community production activity periods accumulating contributions true diversity participants community participation community's power users stage valuable sustainability collective contributions contributions argue critical mass established developing diverse set community heterogeneous resources purely accumulating content 
children diagnosed special united parents economic emotional costs overwhelming mixed methods approach parents children special rely facebook face-book yahoo accessing social support offer geographic communities local school services case-based communities specific conditions autism promisingly parents perceive judgment online offline talking children's special needs; perceptions nuanced posts humor achievement treatment suggestions perceived socially posts judgment violence social comparisons social media fails connecting special families time life span implications social media site design supporting special families 
twitter breaking news competing social media begun carry news examine facebook google twitter report breaking news coverage news events reported latency time reported data drawn weeks december identify major news events ranging celebrity deaths plague outbreaks sports events media carry major events twitter continues preferred medium breaking news consistently leading facebook google face-book google repost newswire stories main conveniently package multitple sources 
social media systems rely user feedback rating mechanisms personalization ranking content filtering users evaluate content contributed fellow users liking post voting comment evaluations create complex social feedback effects paper investigates ratings piece content affect author's future behavior studying comment-based news communities negative feedback leads behavioral detrimental community authors negatively-evaluated content contribute future posts lower quality perceived community authors subsequently evaluate fellow users negatively percolating effects community contrast positive feedback carry effects encourages rewarded authors write improves quality posts interestingly authors receive feedback leave community structural analysis voter network reveals evaluations polarize community positive negative votes equally split 
increase mobile user engagement photo sharing sites identify memorable pictures proposals identifying pictures relied metadata likes visual features practice techniques based inputs work: metadata sparse pictures considerable likes extracting visual features computationally expensive mobile solutions geo-referenced content increasingly premise pictures neighborhood linked neighborhood perceived people: instance distinctive beautiful urban theories proposed lynch milgram peterson aimed systematically capturing people perceive neighborhoods tested theories automatically identifying appealing city pictures gathering geo-referenced flickr pictures city london; selecting urban qualities urban theories; computing proxies qualities online social media data; ranking flickr pictures based proxies proposal enjoys main desirable properties: effective scalable aware contextual time day weather condition suggests promising directions multi-modal learning approaches automatically identify appealing city pictures 
celebrate internet's ability connect individuals facilitate collective action common goal numerous systems designed support aspects collective action systems support participatory end-to-end collective action crowd community identifies opportunities formulates goals brainstorms ideas develops plans mobilizes takes action explore possibilities barriers supporting interactions introduce wedo system aimed promoting simple forms participatory end-to-end collective action pilot deployments wedo illustrate sociotechnical systems support automated transitions phases end-to-end collective action challenges elicitation leadership accommodation existing norms remain 
amount shared social media rapidly increasing amid growing concerns online privacy study investigates controversiality social endorsement media content sharing behavior choosing sharing publicly anonymously anonymous sharing popular choice shares controversial content shard anonymously social endorsement affect sharing behavior sports-related content implications social media interface design discussed 
overload ubiquitous modern society social media users microbloggers receive endless flow rate cognitive abilities process paper conduct scale quantitative study overload evaluate impact dissemination twitter social media site model social media users processing systems queue incoming policies process queue unknown rates decide forward incoming users timestamped data tweets received forwarded users uncover key properties queueing policies estimate processing rates limits understanding users' processing behaviors infer extent users suffer overload analysis empirical evidence processing limits social media users prevalence overloading active popular social media users overloaded rate users receive impacts processing behavior including prioritize sources process process finally susceptibility social media user social contagions depends crucially rate receives exposure piece idea convention product effective users receive rates meaning exposures adopt contagion 
twitter increasingly spreading messages campaigns campaigns gain followers twitter accounts influence followers spread messages paper explore relationship followers' sentiment campaign topic rate retweeting messages generated campaign analysis followers multiple social-media campaigns statistical correlations sentiment retweeting rate based analysis conducted online intervention study followers social-media campaigns study targeting followers based sentiment campaign re-tweet rate baseline approaches 
conducted quantitative analysis ten citizen science projects hosted zooniverse platform data set activity records users collected december july examined level participation users zooniverse discussion forums relation contributions completion scientific micro- tasks zooniverse multitude projects emergence cross-projects effects identified project characteristics importantly subject domain duration project looked adoption expert terminology phenomenon dependent scientific domain project addresses communication features community study increasingly class online community insights inform design development zooniverse platform citizen science systems 
microblogging site twitter popular web destinations relative ease data access based twitter data ranging measuring spread ideas society predicting behavior real-world phenomena stock market studied twitter ecosystem itself; twitter data typically based time-window data ranging weeks months twitter evolved founding remains unclear prior hold implicit assumptions proposed systems valid paper step answering question focusing evolution twitter's users behavior set billion tweets spanning quantify users behavior site evolved observe quantify trends including spread twitter globe rise spam malicious behavior rapid adoption tweeting conventions shift desktop mobile usage interpret calibrate previous twitter future projections site 
food drink basic human society evolved food drink strong cultural aspect strong differences people traditional methods analyze cross-cultural differences based surveys reason difficult represent statistical sample global scale paper propose methodology identify cultural boundaries similarities populations scales based analysis foursquare check-ins approach economic purposes support existing marketing social applications methodology consists steps map food drink check-ins extracted foursquare users' cultural preferences identify individual preferences taste type food drink pizza sake temporal habits time day week individual restaurant bar analyze assess cultural distance countries cities city fourth apply simple clustering technique cultural distance measure draw cultural boundaries countries cities regions 
online games offer multiple players interact recordings interactions variety purposes recordings raise ownership issues arising online media paper analysis attitudes practices online gamers reported recording gameplay reported watching accessing records gameplay series scenarios hypothetical statements elicit reactions varying features production records; parties involved type records influence perceived ownership rights players sensitive recording reuse in-game textual communication recordings avatar activity virtual negative reactions elicited scenarios proposed search capability players' textual communication reuse in-game communication software publisher's advertisements additionally players skeptical institutional archiving game content embargos access brought attitudes media 
widespread usage social media effort detecting influential users propagation applications inherent limitation methods detect influential users users observable signals influence real applications including counter-campaign organization detect influencers measure late intervene method detect would-be influencers prior word usage social media compute psycholinguistic category scores word usage investigate people scores exhibited influence behaviors twitter psycholinguistic categories correlations behaviors built predictive models influence category based features experiments real dataset validates predictions reasonable accuracy 
disruptive reforms modern internet emergence online communities knowledge artefacts wikipedia openstreetmap initiatives vulnerable membership turnover study longitudinal analysis wikiprojects model impact turnover social capital losses project productivity examining social capital losses attempt provide nuanced analysis turnover context social capital modelled social network perspective loss central impact proportion wikiprojects healthy low levels membership turnover social capital losses relationship social capital losses project performance u-shaped withdrawal negative project outcomes support mediation turnover rate network density curvilinear relationship 
online social networks provide rich substrate rumor propagation received friends trusted online social networks individuals transmit friends referencing rumors snopes popular website documenting memes urban legends track propagation thousands rumors appearing facebook sample infer rates rumors categories varying truth uploaded reshared rumor cascades deeper social network reshare cascades examine individual reshares receiving comment link snopes article evolution cascade receiving comment increases likelihood reshare rumor deleted cascades accumulate hundreds snopes comments continuing propagate finally dataset rumors copied pasted status update rumors change time variants tend dominate bursts popularity 
social media continually emerging platform exchange health challenges study mental health discourse popular social media: reddit building findings health seeking sharing practices online forums social media twitter address challenges characterization self-disclosure mental illness communities reddit observe individuals discussing variety concerns ranging daily grind specific queries diagnosis treatment build statistical model examine factors drive social support mental health reddit communities develop language models characterize mental health social support observed bear emotional informational instrumental prescriptive finally study disinhibition light dissociative anonymity reddit's throwaway accounts provide promoting conversations anonymity surprisingly gather feedback involving emotionally engaging findings reveal time unique social media reddit fulfilling stigmatic illness expand understanding role social web behavioral therapy 
inherent nature social media content poses challenges practical applications sentiment analysis vader simple rule-based model sentiment analysis compare effectiveness eleven typical state-of-practice benchmarks including liwc anew inquirer sentiwordnet machine learning oriented techniques relying naive bayes maximum entropy support vector machine svm algorithms combination qualitative quantitative methods construct empirically validate gold-standard list lexical features sentiment intensity measures attuned sentiment microblog-like contexts combine lexical features consideration rules embody grammatical syntactical conventions expressing emphasizing sentiment intensity interestingly parsimonious rule-based model assess sentiment tweets vader outperforms individual human raters classification accuracy generalizes favorably contexts benchmarks 
political inclinations individuals liberal conservative shape opinions issues abortion gun control nuclear power opinions openly exerted online forums news sites parliament paper address quantifying political polarity individuals political issues classification ranking signed bipartite networks represent opinions individuals issues formulate node classification task propose linear algorithm exploits network effects learn polarity labels rankings people issues completely unsupervised manner extensive experiments demonstrate proposed method effective fast easy-to-implement solution outperforming existing baseline algorithms adapted signed networks real political forum congress datasets experiments wide variety synthetic graphs varying polarity degree distributions nodes demonstrate robustness approach 
study online social dynamics based diverge online discussions previous studies focused link structure characterize social dynamics behavior content generation discussions understood jensen-shannon js divergence measure divergence topics user-generated contents progresses time study twitter messages tweets multiple real-world events natural disasters social activism times demographics model structural user features guidance socio-psychological theories social cohesion social identity learn implications discussion divergence features correlation discussion divergence leveraging construct classifier predict future increase decrease discussion divergence achieves curve auc f- score harmonic precision recall approach systematically study collective diverging behavior independent formation design prioritize engage communities specific topics disaster response coordination specific concerns advocacy brand management 
social media offer real-time unfiltered view disasters affect communities crisis response disaster mental health and-more broadly-public health benefit automated analysis public's mental exhibited social media focus twitter data community lost mass shooting community-geographically removed shooting-that indirectly exposed common approach understanding emotional response text: linguistic inquiry word count liwc improved machine learning starting tweets flagged liwc content issue death devise categorization scheme death-related tweets induce automatic text classification content improved methodology reveals striking differences magnitude duration increases death-related talk communities detects subtle shifts nature death-related talk offer lessons gauging public response developing interventions wake tragedy 
online dating sites popular platforms people potential romantic partners online dating sites provide recommendations compatible partners based proprietary matching algorithms recommended dates match user's preference criteria recommended users user reciprocate contacted goal paper predict initial contact message user replied receiver study based scale real-world dataset major dating site china sixty registered users formulate reply prediction link prediction social networks approach machine learning framework availability amount user profile bipartite nature dating network unique opportunities challenges reply prediction extract user-based features user profiles graph-based features bipartite dating network apply variety classification algorithms compare utility features performance classifiers user-based graph-based features result performance effectively predict reciprocal links performance gain achieved feature sets classifiers considered random forests method outperforms algorithms naive bayes logistic regression knn svm methods provide valuable guidelines design performance recommendation engine online dating sites 
olympic games sporting event notable consequences economic landscape host city traditional economic assessments focus aggregated impact event national income fail provide micro-scale insights local businesses benefit increased activity games paper provide approach modeling impact olympic games local retailers analyzing dataset mined location-based social service foursquare hypothesize spatial positioning businesses mobility trends visitors primary indicators retailers rise popularity event confirm formulate retail winners prediction task context evaluate set geographic mobility metrics proximity stadiums diversity activity neighborhood nearby sociability probability customer flows event stadiums parks vital factors supervised learning techniques demonstrate success businesses hinges combination geographic mobility factors location-based social networks crowdsourced dynamic interaction users urban spaces publicly alternative medium assess economic impact scale events city 
investigates summarizing conversations occur comments uk newspaper guardian comment summarization task comments clustered ranked cluster top comments cluster overview cluster topic model clustering agreement evaluated human gold standard approach compared cosine distance clustering k-means clustering pagerank prefered ranking system compared tf-idf mutual gain maximal marginal relevance evaluated sets comments summarized journalist guardian letters 
github popular repository source code finley users company declared april repositories december publicly accessible api march publishes stream events occurring public projects interactions github users complex nature forms developers create fork repositories push code approve code pushed bookmark favorite projects follow developers track activities paper characterization github social network collaborative platform knowledge quantitative study interactions happening github analyze logs service months march september describing events users repositories growing linearly time distributions contributors project watchers project followers user power-law-like shape analyze social ties repository-mediated collaboration patterns observe remarkably low level reciprocity social connections measure activity user terms authored events observe active users followers finally provide geographic characterization centers activity investigate distance influences collaboration 
system visualizes geo-temporal twitter activity distinguishing features system offers degree user freedom subset data visualize ii focus discriminative patterns volume patterns tweets precise gps co-ordinates assigned geographical cells tweet language ii tweet topic iii day week iv time day spatial resolutions cells determined data-driven manner quad-trees recursive splitting user choose data english tweets weekend evenings topic party system implemented geo-tagged tweets qatar http://qtr qcri org/ geotagged tweets york city http://nyc qcri org/ easily extended cities/countries 
instagram form communication users easily share updates photos tweaking filters rapid growth users uploads launched october spite popular photo capturing sharing application attracted attention community paper qualitative quantitative analysis instagram computer vision techniques examine photo content based identify types active users instagram clustering reveal insights instagram studied include: popular photos categories distinct types instagram users terms posted photos user's audience followers independent his/her shared photos instagram knowledge in-depth study content users instagram 
locating timely crises mass emergencies critical forced life-altering decisions twitter broadcast situations widespread finding difficult approach improving recall sampling twitter communications lead situational awareness crisis situations create lexicon crisis-related terms frequently relevant messages posted types crisis situations demonstrate lexicon automatically identify terms crisis finally explain efficiently query twitter extract crisis-related messages emergency events experiments crisis lexicon leads substantial improvements terms recall set crisis-specific keywords manually chosen experts; helps preserve original distribution message types 
demos personalized out-of-context recommendations twitter users follow out-of-context user receive recommendation musicians follow user's tweets' contents social links connection context music setting user expressed context music existing methods fail approach exploits co-following hidden correlations user's political preference provide clues music preference user recommended music band set politicians users follow tend follow alternative b' implement framework distinct settings: recommending musicians recommending political parties tunisia framework simple amazon's users bought bought explainable out-of-context recommendations social studies music closest users political affiliation helps introduce link user unknown domain politics tunisia web-based demos publicly accessible http: //scdl qcri org/twitter/musicians/ recommending musicians http://scdl qcri org/twitter/tunisia/ recommending tunisian parties 
investigate predictability successful memes spreading patterns underlying social networks propose analyze comprehensive set features develop accurate model predict future popularity meme spreading patterns paper comprehensive comparison existing predictive frameworks categorize features groups: influence adopters community concentration characteristics adoption time series features based community structure powerful predictors future success popularity meme predictor future popularity contrary common belief methods outperform approaches task detecting popular unpopular memes 
norm online social osn users accounts multiple services study twitter users pinterest situation leads questions as: activities users perform site disjoint alternatively users perform actions multiple sites originate interlinking social networks failure understand activity multiple-sites obfuscate true dissemination dynamics social web study steps complete understanding user behavior multiple osns collect sample users accounts twitter pinterest crawling profile activity daily basis period months develop methodology comparing activity sites global patterns sites users tend post items pinterest posting twitter findings inform understanding behavior users individual sites dynamics sharing social web 
understanding social behavioral forces event participation viewpoint social science applications design personalized event recommender systems paper takes advantage data location-based social network foursquare analyze event patterns metropolitan cities forward hypotheses motivating factors user participation confirm social aspects play major role determining likelihood user participate event explicit social filtering signal accounting friends attending dominates factors popularity event proves strong attractor capture implicit social signal performing random walks dimensional graph encodes type preferences friends proves suited identify relevant niche events users findings extent temporal spatial social aspects underlie users' event preferences lead hypothesize combination factors models users' event verify supervised learning framework users london users york chicago identifies exact event user attend pool suggestions 
data massive online courses moocs offers potential gain insights online communities contribute student learning richness data trace mined attempted fine-grained content analyses student interactions moocs survey student goals intentions keeping involved mooc time automated fine-grained content analyses offer potential detect monitor evidence student engagement relates aspects behavior ultimately indicators reflect commitment remaining methodological contribution paper investigate computational linguistic models measure learner motivation cognitive engagement text forum posts validate techniques survival models evaluate predictive validity variables connection attrition time conduct evaluation moocs focusing types learning materials prior demonstrates participation discussion forums strong indicator student commitment methodology differentiate students identify danger signs struggling student support population interaction offers opportunity effective support administered theoretical practical implications discussed 
assessing political conversations social media requires deeper understanding underlying practices styles drive conversations paper computational approach assessing online conversational practices political parties deductive approach devise quantitative measures discussion theoretical constructs sociological theory measures qualitative aspects online conversational practices amenable computation evaluate computational approach applying study study online conversational practices german politicians twitter german federal election political parties share patterns behavior exhibit unique idiosyncrasies sheds light complex cultural phenomena online conversational practices amenable quantification ii social media twitter utilized political parties 
technique identify twitter users trust regular twitter users spam fake accounts technique starts initial set trusted users based twitter's verified users recursively includes users trusted users communicate conversations initiated trusted users technique produces set users times larger twitter's verified users evaluation share non-spam users set trusted users share non-spam users tweeting users 
publishers news keen amplify reach content re-sharable social media study relationship concept social deviance re-sharing news headlines network gatekeepers twitter network gatekeepers predilection selecting socially deviant news items professionals study news items major news outlets predominately network gatekeepers re-share news items reference socially deviant events time exceptions outlets suggesting complex picture newsworthiness networked gatekeepers moderated effects topicality varying motivations relationships audience 
human computation crowdsourcing systems popular powerful performing tasks malicious users started misusing systems posting malicious tasks propagating manipulated contents targeting popular web services online social networks search engines malicious users moved fiverr fast-growing micro-task marketplace workers post crowdturfing tasks astroturfing campaigns crowd workers malicious customers purchase tasks paper comprehensive analysis fiverr identify popular types crowdturfing tasks market-place conduct studies crowdturfing tasks build crowdturfing task detection classifiers filter tasks prevent active marketplace experimental proposed classification approach effectively detects crowdturfing tasks achieving accuracy finally analyze real impact crowdturfing tasks purchasing active fiverr tasks quantifying impact target site analysis current security systems inadequately detect crowdsourced manipulation confirms necessity proposed crowdturfing task detection approach 
online social networks enable fast wide diffusion rumors potential threats society understanding rumor spreading mechanism essential demo agent-pattern based model explain spiky temporal diffusion pattern rumors demo platform interface allowing users check wide set parameters contribute characteristic temporal pattern rumors 
incorporating temporal property query expansion methods based relevance feedback positive microblog search contrast word-based query expansion methods propose concept-based query expansion method based temporal relevance model temporal variation concepts terms phrases microblogs model naturally extends extremely effective existing concept-based relevance model tracking concept frequency time proposed model produces concepts frequently time period topic discriminate relevant non-relevant microblog documents experiments corpus microblog data tweets corpus proposed concept-based query expansion method improves search performance highly relevant documents 
behaviors converge language social media exception reviews tourist attractions tripadvisor ta world's largest resource travel social networking sites ta review forums facilitate direct interaction participants theory suggests language guided writers' conception audience style shift response implement model herding local transmission process exploring hypothesis reviewer influenced preceding reviews manifest stylistic feature pronouns paralinguistic devices reviewers unusual features characteristics local context extent reviewers influenced context correlated attributes shared profiles sentiment attraction reviewed language influenced asynchronous environment interpersonal interaction behaviors susceptible manipulation social media; write 
time read newspaper bombarded articles current news media massive news day tragedies happy stories people choose read articles fit current mood purpose project magnet news web tool users choose positive negative news system monitors sentiment news newspapers sentistrength sentiment method proposed literature proved efficient previous analysis 
social media applications twitter provide powerful medium users communicate observations friends witnessed live reporting events soccer games johannesburg revolutions cairo tunis reports rivaled content provided official media tapping valuable resource challenge heterogeneity noise inherent real-time text diversity languages fast-evolving linguistic norms paper seek analyze tweet stream automatically discover time event classify events based type sentiments evoke non-textual features tweeting pattern robust analyzing tweet streams independent languages used; insights users behave social media websites observe users react exciting external event decreasing volume communication users explain model users switch producing sentiments sharing others' news sentiments develop evaluate models algorithms twitter data sets focusing tweets soccer cup data set feature underlying ground truth well-defined goals serve events 
rise social media led explosion sites users join profusion social media sites impossible users actively engage simultaneously users choices sites neglect paper study users joined multiple sites study individuals distributed sites select sites join behavioral patterns exhibit selecting sites study demonstrates users tendency join popular trendiest sites explain users' selections demonstrate peer pressure influences decisions users joining emerging sites 
school-based bullying health issue adolescents wide identify differences microblogs school-based bullying twitter representing usa weibo representing china fraction victim authors weibo twitter hypothesize asian culture's emphasis saving taboo victim label victim temporal dynamics school bullying posts differences holidays length school days finally bullying posts weibo mentions family twitter emphasis family asian cultures 
popularity web social media afforded researchers unparalleled access content daily lives people human ethics guidelines actively expanding meet challenges posed web rely offline principles interaction poor fit modern technology context study identifiability authors socially sensitive content goal identity obfuscation compare identifiability content translated foreign language focusing easily person locate original source content risk authors dissemination content implications ethics guidelines 
subjective nature gender inequality motivates analysis comparison data real fictional human interaction computational extension bechdel test: popular tool assess movie male gender bias female characters provide tools quantify bechdel scores genders measure movie scripts datasets dialogues users myspace twitter comparing movies users social media movies twitter conversations consistent male bias analyzing myspace narrative twitter closer movies pass bechdel test pass link properties movies users share trailers movies analysis reveals particularities movies pass bechdel test: trailers popular female users share male users users share tend interact male users based datasets define gender independence measurements analyze gender biases society manifested digital traces online behavior profile twitter users larger gender independence urban users comparison rural additionally asymmetry genders larger parents lower students gender asymmetry varies increasing average income latitude relation gender inequality social economical cultural factors society gender roles exist fictional narratives public online dialogues 
media frames define distinctive perspectives communicating issues manifested patterns language preferences key terms phrases develop operationalization moral evaluation frames study corpus ofblogs discussing climate change compare moral evaluation frames blogs marked climate change skeptics climate change acceptors develop text visualization tool called lingoscope user observe filter contextual terms convey moral framing volumes text drill specific examples focusing climate-related topics discussed climate change skeptics versus climate change acceptors approach uncovers explores numerous topics framed moral light skeptical acceptor blogs 
social networks counter-intuitive properties including friendship paradox average friends friends variety paradoxes demonstrated online social networks paper explores origins network paradoxes mathematical properties networks behavioral origin sampling heavy-tailed distributions rise paradox median propose strong form network paradoxes based utilizing median validate empirically data online social networks user majority user's friends followers friends followers user explained statistical properties sampling explore behavioral origins paradoxes shuffle test remove correlations node degrees attributes paradoxes persist shuffled network median demonstrate strong paradoxes assortativity user attributes including degree correlation degree attribute 
online services twitter gnip offer streaming programming interfaces real-time filtering based keyword conditions services strict access constraints charge cost based usage refer streams hidden streams draw parallel well-studied hidden web restricts access contents database querying interface time users' captured complex classification models implicitly explicitly hundreds keyword-based rules rules' accuracies paper study utilize constrained streaming access interface maximize retrieved relevant items respect classifier expressed set rules variants static version assumes popularity keywords constant time dynamic version lifts assumption viewed exploration-vs -exploitation np-hard propose exact bounded approximation algorithms settings including access constraint types experimentally evaluate algorithms real twitter data 
study examines bloggers establish enhance credibility blogs series blogging practices based analysis interviews independent bloggers blog range topics audience-aware credibility theoretical construct audience-aware credibility defined bloggers signal credibility based audience provide perceived audience analysis bloggers' credibility constructs conceptualizations audience perceived blog identified types bloggers constructed audience-aware credibility distinctive ways: community builder expertise provider topic synthesizer filterer report bloggers' blogging practices establishing credibility strategies interacting audience enhance credibility contributions study expand credibility constructs social media demonstrate role credibility perceptions content contributors' online activities findings reveal multi-dimensional construct audience-aware credibility serves driving factor influencing shaping blogging practices types bloggers 
traditional mental health studies rely collected personal contact health care professional utility social media data studying depression limited evaluations mental health conditions post traumatic stress disorder ptsd condition millions worldwide rates military veterans method ptsd classifier social media simple searches twitter data reduction training data cost compared previous demonstrate utility examining differences language ptsd random individuals building classifiers separate detecting elevated rates ptsd military bases classifiers 
microblogging services twitter offer variety interactive features users engage contacts social network content produce feature favourite button twitter icon form star users click assign special status tweet evidence suggesting users increasingly favouriting reasons people favouriting utility feature offers contrasts core features retweeting studied extensively paper argue investigating motivations favouriting tweets enhance understanding people achieve twitter types content users goals mind conducted large-scale survey n= questioning twitter users aspects favouriting behaviour users aware function functionality survey responses users demonstrate motives favouriting tweets extremely heterogeneous consistent users findings reveal user favouriting re-finding tweet private conversation supported unmet twitter user interface 
studied experience loneliness communicated thousands people twitter data set public twitter posts explicit expressions loneliness qualitatively developed categorization scheme expressions context loneliness expressed twitter relates existing theories loneliness quantitative analysis data exposed categories patterns communication practices loneliness users expressing severe enduring loneliness female requests social interaction tweets studied responses expressions loneliness twitter's social settings deriving dataset examined factors correlate existence type response receive responses lonely tweets expressions enduring loneliness critically receive responses 
patients caregivers seek support online forums ailments health forums users participate existing site-defined user initiate understand mechanism user-created comparatively studied characteristics site-defined based empirical dataset user activities health forums collected medhelp org study derived categorization user-created underlying reasons leading creation investigated differences user-created site-defined respect member-ships activity levels social network metrics interaction patterns users initiate morehomophily driven communities social interactions user-created vocal socially active participated site-defined findings allowing users create improve user engagement health forums foster rapport users ultimately lead cohesive social environment supports effective sharing community building 
social studies public response social media require tracking topics twitter periods time current approaches rely streaming tweets based hashtags keywords twitter accounts approaches lead limited coverage on-topic tweets paper introduce technique topics effective topic defined set well-prepared queries cover static topic propose automatic approach adapts emerging aspects tracked broad topic time tested tracking approach broad dynamic topics hot categories: egyptian politics syrian conflict international sports measured effectiveness approach days spanning period months ensure consistency effectiveness experimental average approach achieved increase recall relative baseline boolean approach maintaining acceptable precision 
social media unprecedented amount interaction people online fundamental aspect human social behavior tendency people associate like-minded individuals forming homogeneous social circles online offline apply model distinguish social ties varying strength observe evidence homophily politics music health residential sector &amp;amp; college online offline social network college students multiplex network approach social tie strength applied mobile communication data calls text messages co-location allowing dimensionally identify relationships communication channels utilized students strong social ties characterized maximal communication channels weak ties minimal identify close friendships weaker ties facebook friendships compared reported ground truth stronger ties exhibit profile similarity weaker homogeneity social circles respect political health aspects observe strong homophily driven music residential sector college facebook friendship highly dependent residence exposure homogeneous content online offline social circles students notably political music aspects 
requests core social media systems question &amp;amp; answer sites online philanthropy communities success requests critical success community factors lead community satisfy request unknown success request depends factors critically requested ranging favors substantial monetary donations study altruistic requests online community requests contribution offer tangible return allowing disentangle requested textual social factors drawing social psychology literature extract high-level social features text operationalize social relations recipient donor demonstrate extracted relations predictive success communicating narrative essential linguistic indications gratitude evidentiality generalized reciprocity status asker increase likelihood success building understanding develop model predict success unseen requests improving baselines link findings psychology helping behavior providing basis analysis success social media systems 
focused studying complex phenomena reflection social media drawing neighborhood boundaries inferring relationships medicines diseases recognized social sciences studies conditioned gender time confounding factors studies attempt extract social media condition factors difficulty extracting factors naturalistic data complexity including analyses paper simple framework implementing common social media analyses trivial inspect condition contextual data model-discussion graphs-captures structural features relationships inferred social media context discussions derived participating discussions discussions occurring discussed conjunction implement framework tool called dgt studies analyses neighborhoods boundaries based geo-located social media data drastically varying conditioned gender time 
spectators increasingly social platforms express opinions share emotions public events reactions reveal subjective perception event extend understanding motivated develop system explore visualize volume patterns trends user sentiments evolve time previous sentiment analysis opinion mining addressed issues majority distinguish polarity categories leaving detailed insightful analysis desired paper fine-grained multi-category emotion model classify visualize users' emotional reactions public events emotionwatch tool constructs visual summaries public emotions apply olympics test report findings user study evaluating usability tool validating emotion model users prefer detailed inspection public emotions simplified analysis complexity users effectively grasp understand interpret emotional reactions emotionwatch user study design improvements future development analogous systems 
geotagged tweets foursquare check-ins forms volunteered geographic vgi play critical role numerous studies range intelligent technologies commonly sources vgi twitter flickr foursquare biased urban perspectives expense rural utilizing geostatistics-based approach demonstrate capita basis vgi datasets users quality metropolitan vgi subset user-generated content ugc urban biases exist non-geographically referenced ugc finally foursquare exclusively vgi argue foursquare location-based social networks fundamentally failed appeal rural populations 
homophily suggests people tend befriend shared traits topical overlapping social circles study people communicate online term conversation topics egocentric viewpoint dataset facebook friends favor topics form topic-based clusters; clusters dense connectivities growth rates overlap 
propose approach ranking microblog search basic idea leverage user engagement purpose ranking: microblog post received retweets/replies users ranked simply applying raw count engagement bias ranking favoring posts celebrity users posts receive disproportionate amount engagement contents posts reduce bias propose variety time window-based outlier features transfer raw engagement count score user basis evaluation real-world datasets confirms proposed approach improve microblog search 
sizes contemporary social networks surpass billions users grows fast graph algorithms analyze basic operation computation shortest paths nodes classical exact algorithms prohibitively slow graphs motivates development approximate methods landmark-based methods actively studied landmark-based estimation methods start picking fixed set landmark nodes precomputing distance node graph landmark storing precomputed distances data structure prior landmarks required achieve level precision grows size graph simultaneously size data structure proportional product size graph landmarks propose alternative landmark-based distance estimation approach reduces space requirements pruning: computing distances node subset closest land-marks evaluate method dblp orkut twitter skype social networks demonstrate estimation algorithms comparable query time superior approximation quality equivalent non-pruned landmark-based methods requiring memory disk space 
topic-independent sentiment model commonly estimate sentiment microblogs movie product reviews domain adaptation improve sentiment estimation performance examined topic-dependent models improve polarity estimation microblogs considered model trained twitter tweets target keyword model trained enlarged set tweets terms topic comparing performance topic-dependent models topic-independent model trained sample tweets topics topic-dependent models performed propose method predicting topics sentiment estimation performance topic-dependent sentiment model 
large-scale databases human activity social media captured scientific policy attention producing flood discussion paper considers methodological conceptual challenges emergent field special attention validity representativeness social media data analyses persistent issues over-emphasis single platform twitter sampling biases arising selection hashtags vague unrepresentative sampling frames socio-cultural complexity user behavior aimed algorithmic invisibility subtweeting mock-retweeting screen captures text complicate interpretation data social media challenges accounting field effects broadly consequential events diffuse network study affect society application network methods fields study human social activity paper concludes call action practical steps improve analytic capacity promising rapidly-growing field 
personal profiles social network sites tool social evaluation assessing pro file owners' characteristics personality traits narcis sism physical social attractiveness study examine users' trustworthiness evaluated naive observers based con tained facebook profiles profile cues drawing certainty reduction theory warranting theory hardwired perceptions facial displays propose framework identifies cues trustworthiness facebook profiles profile cues friends tagged photographs categories filled comments likes received friends smiling profile photographs plained variance facebook users' ceived trustworthiness friends curvilinear shaped trustworthiness perceptions flection friends trustworthiness decreased friends increased trustworthiness increased friends creased theoretical design implications findings discussed 
paper build social search engine named glaucus location-based queries compose portion mobile searches popular prevalence mobile devices existing social search engines designed location-based queries produce poor-quality queries glaucus inherently designed support location-based queries collects check-in pinpoints user visited location-based social networking services foursquare calculates expertise user query probabilistic model called location aspect model conducted types evaluation prove effectiveness engine glaucus selected users supported stronger evidence required expertise existing social search engines addition answers experts selected glaucus highly rated human judges terms answer satisfaction 
question art science community question-answering setting question people: question prior community question-answering site voting questions notion question quality mere popularity techniques latent topic models automatically predict quality questions based content system achieves prediction accuracy beating strong baselines amount examine question quality dynamics user behavior longevity questions 
numerous models metrics capture characterize static properties online social networks understanding dynamics evolution offers terms metrics models current metrics limited logical time clocks unable capture interactions external factors rely physical time clocks paper goal initial steps building set metrics characterizing social network dynamics based physical time focus attention metrics capture eagerness users building social structure propose metrics link delay triadic closure delay metrics capture time delay link triadic closure instantiate trace considered time traces metrics provide insight speed users building extending social neighborhoods apply metrics real traces social network dynamics renren facebook networks metrics consistent networks differences reveal properties system argue attributed factors network maturity environmental social contexts services offered network provider factors independent network topology captured proposed metrics finally triadic closure delays capture ease neighbor discovery social networks influenced friend recommendation systems 
marketing advertising developing managing brands represent core activities performed companies successful brands attract buyers adopters increase companies' set user-item adoption data infer brand effects users adopting items answer question develop brand item topic model bitm incorporates users' brand preferences process item adoption users evaluate model synthetic real datasets baseline models brand effects bitm determine users demonstrate brand preferences predict item adoptions accurately 
large-scale predictive social analytics proven effective decade industrial efforts understood potential inferences based online behavior analysis sentiment mining influence analysis epidemic spread majority efforts designed real-time responsiveness first-order requirement typical systems perform post-mortem analysis volumes historical data validate predictions already-occurred events observe applications real-time predictions critical delays hours minutes reduce utility examples: political campaigns react scandal spreading facebook; content distribution networks cdns prefetch videos predicted viral; online advertisement campaigns corrected enhance consumer reception paper proposes crowdcast cloud-based framework enable real-time analysis prediction streaming social data instantiation framework tune crowdcast observe twitter tweets predict youtube videos viral future crowdcast applies online machine learning map natural language tweets specific youtube video tweets refer videos weighted perceived influence sender finally video's spread predicted sociological model derived emerging structure graph video-related tweets spreading combining metrics influence live structure crowdcast outputs sets candidate videos identified viral hours monitor twitter days crowdcast's real-time predictions demonstrate encouraging correlation actual youtube viewership future 
lists social networks popular tools organize content paper proposes framework recommending lists users combining features jointly capture personal contribution two-fold develop listrec model leverages dynamically varying tweet content network twitterers popularity lists collectively model users' preference social lists topical users list network structure develop network-based model called list-pagerank model recommend auxiliary lists popular lists subscribed users evaluate listrec model twitter dataset consisting direct list subscriptions automatic evaluation technique compare performance listrec model baseline methods competing approaches model delivers precision terms prediction subscribed lists twitterers demonstrate combining weighting schemes capturing users' twitter lists evaluate list-pagerank model employ user-study based evaluation model effective recommending auxiliary lists authoritative lists subscribed users 
prevalence misinformation social media online communities undermine public security distract attention issues fact-checking interventions users cite fact-checking websites snopes factcheck org strategy users employ refute false claims peers laboratory suggests interventions effective persuading people abandon false ideas considers interventions deployed real-world conversations interventions observed twitter examine contexts consequences fact-checking interventions focus social relationship individual issues fact-check individual challenged fact-checking interventions commonly issued strangers draw user attention responses friends finally implications designing effective interventions misinformation 
college students team class projects select based performance grades trust study relationship team formation class projects social media university students gather online facebook data test predictors team formation self-organized selection team depend grades facebook-derived proxies tie strength popularity homophily theoretical implications team formation literature practical implications online educational platforms 
increasing social media platforms twitter attracted online users express attitude topics sentiment opinion action essential aspects user attitude studied separately existing investigating brings unique challenges understand user's online behavior benefit set applications online campaign recommender systems paper computational model estimates individual social media user's attitude controversial topics terms aspects relationships model simultaneously capture aspects predict action sentiment based one's opinions experiments multiple social media campaign datasets demonstrated attitude model effectively predict people's sentiment opinion action approaches treat aspects separately 
aim collectively recommend traveling paths leveraging check-in data mining moving behaviors users call traveling paths temporal transit patterns ttp capture representative traveling behaviors consecutive locations check-in data achieve goal propose temporal transit pattern mining method ttpm-method devises unsupervised mechanism automatically summarizes representative travel patterns guarantee time efficiency lower memory usage based mined ttp system ttp-rec developed ttp-rec users starting/end locations flexibility time constraint requirement expected duration trip sequence check-in traveling path mine frequent sequences ranking mechanism achieve goal ttp-rec targets travelers unfamiliar objective area/city time limitation trip 
sociolinguistic studying social power dynamics online social networks reflected users' language online social power prediction build tools marketing political campaigns build audience existing focused finding correlations status linguistic features email wikipedia discussions court hearings studies predicting status basis language twitter proved fruitful derive rich set features literature variety disciplines build classifiers assign twitter users levels status based language metrics followers klout score achieve classification accuracy individual users step reached accuracy task predicting powerful user dyadic conversation manner powerful users write differs low status users ways: extent deviate usual writing habits conversing pronoun language complexity sentiment expression emoticon extending analysis facebook assess generalisability differences similarities sites 
times social forums quora yahoo answers constitute powerful media people variety topics express intentions reveal potential intent purchase 'purchase intent' pi purchase intent defined text expression desire purchase product service future extracting posts pi user's social posts huge opportunities web personalization targeted marketing improving community observing systems paper explore detecting pis social posts classifying linguistic features statistical features pi expressions achieves improvement pi classification 'bag-of-words' based features day social-media classification tasks approach takes consideration specifics social posts limited contextual incorrect grammar language ambiguities extracting features levels text granularity word phrase based features grammatical dependency based features patterns observed pi posts identify specific features 
paper introduce measure consistency twitter user behavior measuring engagement topic measure topical engagement filter tweets collected nigerian users three-year period country's head president goodluck jonathan measure effectively identifies set highly engaged users produce disproportionate amount activity tweet volume mentions users additionally preferential within-group activity users disproportionately mentioning highly engaged users lastly potential percentage engaged users user set gauging significance events occurred nigeria period 
challenging exact disease prevalence true disease population level easy identify relevant data sources administrative clinical health data public health surveillance proxy estimate disease prevalence traditionally data sources span healthcare utilization emergency department visits pharmacy drug sales laboratory test addition incompleteness data sources timely manner timeliness factor prevalence estimation conditions infectious diseases time epidemic instance influenza pandemic estimates day non-clinical non-traditional data sources introduced public health potentials provide signals disease rate provide feedback trends disease ideally combining sources routinely identify disease efficiently building construct capable incorporating data sources coherent manner trivial pandemic infectious disease media reports deaths web traditional data source propose dynamic bayesian networks class probabilistic graphical models combine data source traditional exploration probabilistic relationships data streams initial step building framework support aggregation heterogeneous data real-time estimation disease prevalence preliminary proposed model generalizes 
nowadays health forums established internet health consumers health issues interact amount user generated content healthcare social media sites studies applied data mining artificial intelligence techniques knowledge discovery scale data emerging online health forums difficult users relevant topics peers amount traditional recommendation systems health online forums health consumers intentions participation types supports content matches solving apply naive bayes methods study classify posts comments quitstop forum online community smoking cessation intervention classifiers built text features health features user quit status classification tasks investigated: classification user intentions classification types social support exchanged interactions developed classifiers posts comments separately conducted experiments compare classifiers text health feature sets thread title post content achieve classification accuracy posts comments user intention classification text features hand content post comment performs classification social support types post integrating health features post author boost text classifications user intention support type user health features improving text classifiers comments 
tools mining social media web data yielded tremendous advances public health capabilities received considerable attention infectious disease surveillance focus influenza collier culotta eiji aramaki morita paul dredze studies validated twitter influenza surveillance mined trends closely tracking public health agency data centers disease control prevention cdc trends produced real time weeks cdc influenza reports gap remains promising utilization public health officials lack requisite technology resources produce trends commercial websites offer services including influenza surveillance health trends social media mappyhealth crowdbreaks sickweather sites focused methods yield capabilities sites healthmap org freifeld al flu chunara al columbia's influenza forecasts shaman al focus social media data excellent resources gap remains delivering public health social media healthtweets org platform sharing twitter data researchers public officials demo paper data collection processing features site goal service transition practice 
paper seeks identify characterize health-related topics discussed chinese microblogging website sina weibo identified messages health-related keywords filtered dataset messages spanning applied probabilistic topic models dataset identified prominent health topics variety health topics discussed sina weibo flu-related topics correlated monthly influenza rates china 
improving public health major responsibility government major citizens scientific communities extremes hand tremendous progress understanding spread remedies common regularly occurring diseases dengue malaria japanese encephalistis je hand public agencies treat diseases ad hoc manner learning experiences previous alerted reported arisen disease season reactively initiate actions document disease impact deaths period forget learning season opportunity reduce preventable deaths sickness economic impact scientific progress enabled gap universal prominent developing countries india paper public agencies provide historical disease impact openly analyzed statistical machine learning techniques correlated emerging practices disease control simulated setting optimize social benefits provide timely guidance disease seasons regions illustrate data mosquito-borne communicable diseases; published public health efficacy dengue control methods apply simulated typical city maximal benefits resources exercise helps strategies regions data recorded city agencies prevention methods medical community focus wider impact 
countries implemented form emergency medical services ems people urgent medical typically ambulances serve specific geographic region ambulances limited emergency calls responded nature emss managed coordinated conventionally operators special call centres paper model task compare strategies employed completely automatic ambulance fleet management system decision support tool operators continuous optimization ambulance distribution region dynamic reassignment ambulances incoming requests benefit patients economically provider ems 
paper approach mine large-scale knowledge graphs discover inference paths query expansion nlidb natural language interface databases addressing nlidb applications effectively handle relevant concepts domain correspond structured fields target database preliminary observations performance approach applied freebase conclude discussions steps evaluate extend approach 
people constrained picture based communication expression question answering qa retrieval ir scenario highly limited traditionally alternative augmentative communication aac methods gestures communication boards utilised systems users produce utterances sentences consist multiple words; generate automatically promising direction data context paper provide dedicated access method open-domain qa ir context propose method user search additional symbols communication board real-time access data sources context based filtering desired symbol missing user select symbol desired concept system searches images internet wikipedia purpose retrieving symbol picture querying candidates performed estimating semantic relatedness text fragments explicit semantic analysis esa 
implemented approach transform natural language sentences sparql background knowledge ontologies lexicons eligible technologies data storage possibilities analyzed evaluated contributions paper twofold firstly motivation current natural language access industry data scenarios proposed solution required architectural approach based automatic sparql query construction effective natural language queries analyze performance rdbms rdf triple stores knowledge representation proposed approach evaluated basis query catalog query efficiency accuracy data storage performance natural language access industry data ontologies lexicons simple effective approach improve diagnosis process data search broad range users virtual rdf graphs support db-driven knowledge graph representation process perform efficient industry conditions terms performance scalability 
wild data wbd data hard extract understand heterogeneous nature volume typically schema multiple sources challenge extraction integration subduing wbd techniques resources popular processing natural language text approach applicable data graph objects relations tabular data transformed graph start applying topic models contextualize data identify potential types graph's nodes mapping types ontologies freebase dbpedia assemble coarse clusters objects interpret link perform entity disambiguation record linking 
kelvin system extracting entities relations text collections tac knowledge base population cold start task national institute standards technology cold start task starts empty knowledge based defined ontology entity types properties relations evaluations collection text local web news de-emphasize entities background knowledge base wikipedia features kelvin cross-document entity coreference module based entity mentions removal suspect intra-document conference chains slot consolidator entities application inference rules expand asserted set analysis browsing tools supporting development 
wikipedia enormous amount background knowledge reason semantic relatedness entities propose wikipedia-based distributional semantics entity relatedness diser represents semantics entity distribution dimensional concept space derived wikipedia diser measures semantic relatedness entities quantifying distance high-dimensional vectors diser builds model annotated entities improves existing approaches distinguish entity surface form evaluate approach benchmark relative entity relatedness scores entity pairs approach improves accuracy art methods computing entity relatedness evaluation diser entity disambiguation task dataset sentences highly ambiguous entity mentions improvement precision performing methods provide resource entities entity graph constructed nodes represent wikipedia entities relatedness scores reflected edges wikipedia millions entities required efficient computation relatedness scores trillions entity-pairs 
system discovering characterizing emerging events define event emergence developing situation comprised series sub-events detect sub-events continuous textual input stream techniques: frequency-based detection sub-events entailed emerging event; anomaly-based detection sub-events indicative emerging event identifying emerging events detected sub-events involves connecting sub-events relevant emerging events event models estimating likelihood emerging events sub-event emerging events supports event models varying degrees adopt coherent compact model probabilistically identifies emerging events innovative aspect well-defined framework statistical data techniques informed event semantics inference techniques vice versa grounded semantics knowledge representation enables produce reliable purely statistical approach 
paper address task extracting risk events probabilities free text focusing biomedical domain initial motivation enable determination parameters bayesian belief network approach specific investigate task sequence tagging label spans text events construct probability statements form a/b approach outperforms entity extraction baseline annotated medical risk event corpus explore semi-supervised methods lead modest improvement encouraging direction 
paper focus extracting structured labeled data short unstructured ad-postings online sources craigslist ads posted topics job postings rentals car sales fundamental challenge addressing ad-postings highly unstructured short-text postings written informal manner inherent grammar well-defined dictionary paper propose unsupervised supervised algorithms extracting structured data unstructured ads form key pairs naturally represent topic-specific features ads unsupervised algorithm centered building affinity graph topic-specific corpus ads edge weights represent affinities words; key extraction algorithm identifies specific affinity graph classes key attributes supervised algorithm conditional random field based training algorithm identify specific structured key pairs based pre-defined topic-specific structural data representations ads based corpus car apartment ad-postings craigslist unsupervised algorithm reported accuracy car apartment ads supervised algorithm demonstrated improved performance accuracies 
paper system jointly extracts entities appearing images mentioned accompanying captions input entity linking program takes segmented image caption consists sequence processing steps: part-of-speech tagging dependency parsing coreference resolution enables identify entities textual relations captions program image regions labelled set predefined categories computes wordnet similarities labels entity names finally program links entities detected text images applied system segmented annotated iapr tc- dataset enriched entity annotations correct assignment rate 
tackle saliency detection images propose learn adaptive mid-level features represent image local efficient calculate multi-scale multi-level saliency maps simple k-means algorithm learn adaptive low-level filters convolve image produce response maps low-level features intrinsically capture texture color simultaneously adopt additional threshold pooling techniques generate mid-level features robustness image local representation define set hand-crafted filters multiple scales multiple levels calculate local contrasts result intermediate saliency maps finally fused resultant saliency map vision prior benefiting filters resultant saliency map captures subtle textures object discovers salient object image feature learning saliency map calculation convolution operation unify stages framework deep architecture experiments challenging benchmarks demonstrate effectiveness proposed method 
complex decision scenarios require maintaining level concentration acquiring knowledge context task hand focus attention contextual factors operators interact conversely determining optimal interact augment operators' cognition challenges exist determining efficient mathematical frameworks sound metrics infer reason assess level attention spatio-temporal complex solving hybrid human-machine systems paper proposes computational framework based bayesian approach ban infer users' focus attention based physical expression generated embodied interaction support decision-making unobtrusive manner experiments involving interaction modalities vision-based gesture interaction glove-based gesture interaction speech feet body balance conducted assess proposed framework's feasibility including likelihood assessed attention enhanced ban task performance confirm physical expressions determining quality solutions spatio-navigational type 
self-play reinforcement learning proved successful perfect two-player games carrying theoretical guarantees practical success games imperfect lacking paper evaluate self-play monte-carlo tree search mcts limit texas hold'em kuhn poker introduce variant established ucb algorithm provide empirical demonstrating ability approximate nash equilibria 
disparity size game solve size largest instances solvable algorithms; popular variant poker nodes game tree approximate equilibrium-finding algorithms scale games nodes approximate equilibrium strategies games leading approach create strategic approximation game called abstraction solve game leading abstraction algorithm imperfect-information games generates abstractions imperfect recall distribution aware k-means earth mover's distance metric cluster distribution-aware abstraction round distributions future strength opposed expectation strength leading algorithm considers distributions future strength final round game benefit trajectory distributions strength future rounds final round abstraction algorithm takes future rounds account called potential aware algorithm computing potential-aware imperfect-recall abstractions earth mover's distance experiments no-limit texas hold'em algorithm improves performance approach 
regret matching widely-used algorithm learning proving regrets actions setting game transferred warm start regrets solving setting structure payoffs written function parameters prove carefully discounting prior regrets knowledge principled warm-starting method no-regret learning extends warm-starting widely-adopted counterfactual regret minimization cfr algorithm incomplete-information games; experimentally study optimizing parameter vector player two-player zero-sum game optimizing bet sizes poker propose custom gradient descent algorithm provably locally optimal parameter vector leveraging warm-start theory save regret-matching iterations step optimizes parameter vector simultaneously finding equilibrium experiments no-limit leduc hold'em no-limit texas hold'em optimize bet sizing amounts action abstraction algorithm algorithm selecting discrete actions continuum actions—a key preprocessing step solving games current equilibrium-finding algorithms convergence guarantees extensive-form games 
discovery ago interior methods deliver competitive algorithms scale optimization surprisingly games incomplete formulated linear program interior methods discarded favor attractive methods paper describes specialized interior methods scale games 
opponent modeling essential approach building competitive computer agents imperfect games paper approach develop opponent modeling techniques approach applies neural networks separately trained dataset build k-model clustering opponent models kullback-leibler kl divergence exploit safety mode opponent modeling parameter controls max divergence model's centre units belong approach proved provide lower bound expected payoff minimax payoff correctly clustered players players incorrectly clustered lower bound unlimited approximated sufficient history data experiments agent model improved classification efficiency opponent modeling comparing relative researches agent performs playing poker agent hitsz_cs_ participate annual computer poker competition 
online search games core artificial intelligence advances search perfect games chess checkers backgammon led ai capable defeating world's top human experts search imperfect games poker bridge skat challenging complexities introduced hidden paper online outcome sampling oos imperfect search algorithm guaranteed converge equilibrium strategy two-player zero-sum games oos avoids common encountered existing search algorithms experimentally evaluate convergence rate practical performance benchmark strategies liar's dice variant goofspiel set monte carlo tree search ismcts exploitability strategies produced oos decreases amount search time increases practice oos performs ismcts head-to-head play producing strategies lower exploitability search time 
paper proposes approach computing approximate equilibrium game imperfect approach involves decomposition; breaking subproblems solved independently compare approach decomposition approaches illustrate approach guaranteed equilibrium applied recovery 
paper describes baseline question answering system swedish system includes modules carry question analysis hypothesis generation reranking answers trained evaluated questions data set inspired swedish television quiz kvitt eller dubbelt swedish wikipedia knowledge source paragraph retrieval corpus acceptable coverage answers targeting kvitt eller dubbelt questions single-word answer questions question hypothesis generation module retrieves list paragraphs ranks vector space model score extract set candidates question analysis performs lexical answer type prediction compute baseline ranking sorted answer candidates frequencies relevant paragraphs reranker module previous stages estimate correctness generated answer candidates grammatical dependency parser correctness estimate re-weight baseline ranking -fold cross-validation median ranking correct candidate rank baseline version reranker 
true natural language understanding requires ability identify understand metaphorical utterances ubiquitous human communication kinds identifying metaphors arbitrary text unsolved analyzing meaning current methods transferred languages development extensive language-specific knowledge bases semantic resources paper language-independent ensemble-based approach identifying linguistic metaphors natural language text system's architecture runs multiple corpus-based metaphor identification algorithms parallel combines architecture easy integration metaphor identification schemes developed approach achieves state-of-the-art multiple languages represents improvement existing methods 
hybrid reasoners combine multiple types reasoning subsumption prolog-style resolution outline system combines natural deduction subsumption reasoning inference graphs implementing logic arbitrary indefinite objects 
premise access natural language meaning result hybrid human-computer effort belongs cognitive computing paper explores nature composable properties size taste component properties composable property expressed natural language expressions size tasty salty sweetish taste stands combination component properties range emulate human understanding composable properties computer analyze varying component properties animal understood animal salient component property component properties explicitly mentioned computer understanding composable property size step direction machine understanding implicit meaning holy grail deep meaning analysis ultimate goal strand 
growing sparse coding based methods image classification retrieval progress direction limited computational cost generating image's sparse representation overcome leverage sparsity-based dictionary learning hash-based feature selection build unsupervised efficiently pick query image's high-level features; selected set features effectively pinpoint images visually perceived query method adaptive retrieval database moment preliminary based feature map method's efficiency accuracy visual cognitive perspective 
position paper summarizes initial future opportunities augmenting human mental functioning real-time analysis measurements reflecting person's physiological mental eeg heart rate blood pressure galvanic skin response measurements collected cheap wearable devices market; devices development electronic tattoos promise increase measurement quality ease result wider adoption wearable technologies future traditional measurements human behavior speech text collected mobile devices smart phones combined data collected wearable devices produce accurate inference person's mind behavior demo context study system eeg signal subject driving car envision detecting situations operator danger system occasions system danger operator based android phone low-cost neurosky eeg device explore applications improve road safety review existing focused interruptions activities speech text analysis combined physiological data accurately classify person's mental decisions interruptions incoming phone call driver middle changing lanes merging busy highway dangerous 
approval voting systems voter decides subset candidates he/she approves focus optimization finding committee fixed size minimizing maximal hamming distance vote paper ptas resolve question raised carragianis al [aaai' result adapting techniques developed li al [jacm' originally constrained closest string technique relies extracting structural properties constant size subsets votes 
299
neurons: neural-based approach answering questions images address question answering task real-world images set visual turing test combining advances image representation natural language processing propose neural-image-qa end-to-end formulation trained jointly contrast previous efforts facing multi-modal language output answer conditioned visual natural language input image question approach neural-image-qa doubles performance previous approach provide additional insights analyzing contained language provide human baseline study human consensus ambiguities inherent challenging task propose metrics collect additional answers extends original daquar dataset daquar-consensus image representation;natural language processing;recurrent neural nets;daquar dataset;daquar-consensus;image representation;language output;multimodal problem;natural language processing;neural-based approach;neural-image-qa;neurons;question answering task;real-world images;recurrent neural network;visual turing test;computer architecture;knowledge discovery;natural languages;recurrent neural networks;semantics;visualization 
segment-phrase table semantic segmentation visual entailment paraphrasing introduce segment-phrase table spt collection bijective associations textual phrases segmentations leveraging progress object recognition natural language semantics build high-quality segment-phrase table minimal human supervision importantly demonstrate unique unleashed rich bimodal resource vision natural language understanding fine-grained textual labels facilitate contextual reasoning helps satisfying semantic constraints image segments feature enables achieve state-of-the-art segmentation benchmark datasets association high-quality segmentations textual phrases aids richer semantic understanding reasoning textual phrases leveraging feature motivate visual entailment visual paraphrasing demonstrate utility dataset document image processing;image segmentation;inference mechanisms;natural language processing;bijective association;contextual reasoning;fine-grained textual label;natural language semantics;object recognition;segment-phrase table;semantic constraint;semantic segmentation;textual phrase;visual entailment;visual paraphrasing;buildings;cognition;image segmentation;pragmatics;semantics;training;visualization 
aligning books movies: story-like visual explanations watching movies reading books books rich source fine-grained character object scene high-level semantics thinking feeling evolve story paper aims align books movie releases provide rich descriptive explanations visual content semantically captions current datasets align movies books propose neural sentence embedding trained unsupervised corpus books video-text neural embedding computing similarities movie clips sentences book propose context-aware cnn combine multiple sources demonstrate quantitative performance movie/book alignment qualitative examples showcase diversity tasks model entertainment;neural nets;text analysis;video signal processing;context-aware cnn;descriptive explanations;fine-grained information;high-level semantics;large corpus;movie clips;movie releases;movie-book alignment;neural sentence embedding;similarity computing;story-like visual explanations;video-text neural embedding;visual content;grounding;motion pictures;roads;semantics;videos;visualization;voltage control 
learning query image similarities ranking canonical correlation analysis fundamental image search learn ranking functions similarity query image topic evolved paradigms: feature-based vector model image ranker learning relies image surrounding texts learns ranker based human labeled query-image pairs paradigms limitation vector model sensitive quality text descriptions learning paradigm difficult scaled human labeling expensive demonstrate paper limitations mitigated jointly exploring subspace learning click-through data propose ranking canonical correlation analysis rcca learning query image similarities rcca initially common subspace query image views maximizing correlations simultaneously learns bilinear query-image similarity function adjusts subspace preserve preference relations implicit click-through data subspace finalized query-image similarity computed bilinear similarity function mappings subspace large-scale click-based image dataset queries images rcca powerful image search superior performance state-of-the-art methods keyword-based query-by-example tasks image retrieval;learning artificial intelligence ;matrix algebra;statistical analysis;text analysis;rcca;bilinear query-image similarity function;click-through data;feature-based vector model;human labeled query-image pairs;image ranker learning;image search;keyword-based task;large-scale click-based image dataset;query learning;query-by-example task;ranking canonical correlation analysis;ranking function learning;text descriptions;bipartite graph;computational modeling;correlation;feature extraction;learning systems;semantics;visualization 
learning moving current dominant paradigm feature learning computer vision relies training neural networks task object recognition millions hand labelled images learn features diverse set visual tasks form supervision biology living organisms developed ability visual perception purpose moving acting drawing inspiration observation investigated awareness egomotion motion supervisory signal feature learning opposed knowledge class labels egomotion freely mobile agents training images features learnt egomotion supervision compare favourably features learnt class-label supervision tasks scene recognition object recognition visual odometry keypoint matching computer vision;feature extraction;image motion analysis;learning artificial intelligence ;object recognition;biology;class labels;computer vision;egomotion;feature learning;hand labelled images;living organisms;mobile agents;object recognition;self motion;supervisory signal;training neural networks;visual perception;visual tasks;cameras;feature extraction;neural networks;object recognition;streaming media;training;visualization 
object detection generalization efficiency balanced co-occurrence features paper propose high-accuracy object detector based co-occurrence features firstly introduce kinds local co-occurrence features constructed traditional haar lbp hog boosted detectors learned weak classifier corresponds local image region co-occurrence feature addition propose generalization efficiency balanced geb framework boosting training feature selection procedure discrimination ability generalization power computation cost candidate features evaluated decision result boosted detector achieves accuracy efficiency performance competitive state-of-the-art methods pedestrian detection object detection tasks gradient methods;image classification;object detection;geb;hog;haar;lbp;boosted detectors;cooccurrence feature;discrimination ability;generalization efficiency balanced framework;local image region;pedestrian object detection tasks;weak classifier;boosting;detectors;feature extraction;histograms;object detection;robustness;training 
mining and-or graphs graph matching object discovery paper reformulates theory graph mining technical basis graph matching extends scope applications computer vision set attributed relational graphs args propose hierarchical and-or graph aog model pattern maximal-size common subgraphs embedded args develop method mine aog model unlabeled args method solution mining hierarchical models unannotated visual data exhaustive search objects apply method rgb/rgb-d images videos demonstrate generality wide range applicability code https://sites google com/site/quanshizhang/mining-and-or-graphs computer vision;data mining;graph theory;object detection;arg;aog;attributed relational graph;computer vision;graph matching;graph mining;hierarchical and-or graph;object discovery;computer vision;data mining;data models;feature extraction;image edge detection;videos;visualization 
pose induction object categories address task predicting pose objects unannotated object categories seed set annotated object classes generalized classifier reliably induce pose single instance category availability collection instances approach jointly reasons instances improve initial estimates empirically validate components algorithm quantitatively method produces reliable pose estimates qualitative diverse set classes demonstrate applicability system learning shape models object classes object recognition;pose estimation;generalized classifier;novel object category;pose estimates;pose induction;unannotated object category;animals;azimuth;shape;three-dimensional displays;training;visualization 
dynamic texture recognition orthogonal tensor dictionary learning dynamic textures dts video sequences stationary properties exhibit repetitive patterns space time paper aims investigating sparse coding based approach characterizing local dt patterns recognition dimensionality dt sequences existing dictionary learning algorithms suitable purpose computational costs poor scalability overcome obstacles proposed structured tensor dictionary learning method sparse coding learns dictionary structured orthogonality separability proposed method fast scalable high-dimensional data existing addition based proposed dictionary learning method dt descriptor developed adaptivity discriminability scalability existing approaches advantages demonstrated experiments multiple datasets image coding;image recognition;image sequences;image texture;tensors;dt descriptor;dynamic texture recognition;local dt patterns;orthogonal tensor dictionary learning algorithm;orthogonality;separability;sparse coding based approach;stationary properties;structured tensor dictionary learning method;video sequences;computational modeling;dictionaries;encoding;feature extraction;learning systems;scalability;tensile stress 
convolutional channel features deep learning methods powerful tools suffer expensive computation limited flexibility alternative combine light-weight models deep representations successful exist visual unified framework absent paper revisit approaches computer vision filtered channel features convolutional neural networks cnn absorb merits proposing integrated method called convolutional channel features ccf ccf transfers low-level features pre-trained cnn models feed boosting forest model combination cnn features boosting forest ccf benefits richer capacity feature representation compared channel features lower cost computation storage compared end-to-end cnn methods ccf serves tailoring pre-trained cnn models diverse tasks fine-tuning network task achieving state-of-the-art performances pedestrian detection detection edge detection object proposal generation computer vision;feature extraction;neural nets;object detection;ccf;cnn;boosting forest model;computer vision;convolutional channel features;convolutional neural networks;deep learning methods;edge detection;face detection;filtered channel features;object proposal generation;pedestrian detection;boosting;computational modeling;feature extraction;image edge detection;object detection;proposals;support vector machines 
local convolutional features unsupervised training image retrieval patch-level descriptors underlie computer vision tasks stereo-matching content-based image retrieval introduce deep convolutional architecture yields patch-level descriptors alternative popular sift descriptor image retrieval proposed family descriptors called patch-ckn adapt introduced convolutional kernel network ckn unsupervised framework learn convolutional architectures comparison framework benchmark current deep convolutional approaches patch-ckn patch image retrieval including romepatches\ dataset patch-ckn descriptors yield competitive compared supervised cnn alternatives patch image retrieval computer vision;content-based retrieval;convolution;image retrieval;stereo image processing;unsupervised learning;patch-ckn descriptors;romepatches dataset;computer vision tasks;content-based image retrieval;convolutional kernel network;deep convolutional architecture;local convolutional features;patch retrieval;patch-level descriptors;stereo-matching;unsupervised training;computer architecture;detectors;image retrieval;kernel;lighting;pipelines;three-dimensional displays 
ride: reversal invariant descriptor enhancement fine-grained object recognition datasets image orientation left/right vary sample sample handcrafted descriptors sift reversal invariant stability image representation based limited popular solution augment datasets adding left-right reversed copy original image strategy improves recognition accuracy extent brings price doubled time memory consumptions paper ride reversal invariant descriptor enhancement fine-grained object recognition ride generalized algorithm cancels impact image reversal estimating orientation local descriptors guarantees produce identical representation image left-right reversed copy experimental reveal consistent accuracy gain ride types descriptors provide insightful discussions mechanism ride generalization applications image enhancement;image representation;object recognition;transforms;ride;sift;consistent accuracy gain;fine-grained object recognition dataset;generalized algorithm;handcrafted descriptor;identical representation;image orientation;image representation;image reversal;left-right reversed copy;local descriptor;memory consumption;recognition accuracy;reversal invariant descriptor enhancement;stability;working mechanism;birds;feature extraction;histograms;image representation;indexes;object recognition;training 
discrete tabu search graph matching graph matching fundamental computer vision paper propose graph matching algorithm based tabu search proposed method solves graph matching casting equivalent weighted maximum clique association graph penalize introducing negative weights subsequent tabu search optimization overcoming convention positive weights method's distinct feature utilizes history search strategic decisions optimal solution effectively escaping local optima practice achieving superior proposed method existing algorithms enables direct optimization original discrete space encouraging artificially enforcing hard one-to-one constraint solution experiments demonstrate robustness algorithm variety settings state-of-the-art code http://cv snu ac kr/research/~dtsgm/ computer vision;graph theory;optimisation;search problems;association graph;computer vision;discrete tabu search;equivalent weighted maximum clique problem;graph matching algorithm;subsequent tabu search optimization;computer vision;image edge detection;monte carlo methods;optimization;programming;search problems;space exploration 
discriminative learning deep convolutional feature descriptors deep learning revolutionalized image-level tasks classification patch-level tasks correspondence rely hand-crafted features sift paper convolutional neural networks cnns learn discriminant patch representations train siamese network pairs non- patches deal potential pairs combination stochastic sampling training set aggressive mining strategy biased patches hard classify distance training testing develop -d descriptors euclidean distances reflect patch similarity drop-in replacement task involving sift demonstrate consistent performance gains art generalize scaling rotation perspective transformation non-rigid deformation illumination descriptors efficient compute amenable modern gpus publicly data mining;image classification;image sampling;learning artificial intelligence ;neural nets;stochastic processes;transforms;cnn;euclidean distance;l distance;sift;siamese network training;convolutional neural networks;deep convolutional feature descriptors;discriminant patch representations;discriminative learning;image classification;mining strategy;stochastic sampling;computational modeling;computer architecture;computer vision;measurement;semantics;three-dimensional displays;training 
amodal completion size constancy natural scenes enriching current object detection systems veridical object sizes relative depth estimates single image technical challenges occlusions lack calibration data scale ambiguity object size distance addressed generality previous propose tackle issues building advances object recognition created large-scale datasets introduce task amodal bounding box completion aims infer extent object instances image propose probabilistic framework learning category-specific object size distributions annotations leverage conjunction amodal completions infer veridical sizes objects images finally introduce focal length prediction approach exploits scene recognition overcome inherent scale ambiguities demonstrate qualitative challenging real-world scenes learning artificial intelligence ;natural scenes;object detection;object recognition;probability;realistic images;amodal bounding box completion;amodal completion;focal length prediction approach;large-scale dataset;learning category-specific object size distribution;natural scene;object detection system;object instance;object recognition;probabilistic framework;real-world scene;relative depth estimate;scale ambiguity;scene recognition;size constancy;veridical object size;veridical size;buildings;cameras;computer vision;image recognition;object recognition;solid modeling;three-dimensional displays 
learning position common issue deformable object detection finding position issue outspoken detection pose estimation objects three-dimensional space methods extract shape object cad models limits applicability categories models represent object predefined simple shape cuboid extends applicability model pre-defined shape simple properly represent object paper propose method detection pose estimation objects cad model starting simple shape learn weakly supervised manner locations fit training data method builds iterative estimation locations introduce speedups method fast practical experiments evaluate model detection pose estimation cars method obtains comparable art faster approaches additional iterative methods;object detection;pose estimation;shape recognition; object detection; locations;cars detection;deformable object detection;face detection;iterative estimation;parts position;pose estimation;computational modeling;design automation;shape;solid modeling;three-dimensional displays;training 
query adaptive similarity measure rgb-d object recognition paper studies improving top- accuracy rgb-d object recognition impressive top- accuracies achieved existing methods top- accuracies satisfactory reasons two-fold: existing similarity measures sensitive object pose scale intra-class variations effectively fusing rgb depth cues address paper proposes similarity measure based dense matching objects comparison warped aligned tolerate variations rgb depth fusion argue constant golden weight exist modalities varying contributions comparing objects categories capture dynamic characteristic matchers equipped fusion weights constructed explore responses dense matching fusion configurations response scores finally merged learning-to-combination generalization ability practice proposed approach win public benchmarks achieves top- test accuracy washington rgb-d object dataset improvement state-of-the-art image fusion;image matching;learning artificial intelligence ;object recognition;query processing;rgb-d object recognition;dense matching;fusion weight;learning-to-combination;query adaptive similarity measure;art;cameras;feature extraction;image color analysis;market research;object recognition;shape 
listening eyes: practical visual speech recognition system deep boltzmann machines paper feature learning method visual speech recognition deep boltzmann machines dbm existing visual feature extraction techniques solely extracts features video sequences method explore acoustic visual learn visual feature representation training stage test stage audio visual signals videos generating missing audio feature visual audio features joint representation carried experiments scale audio-visual data corpus experimental proposed techniques outperforms performance hadncrafted features features learned commonly deep learning techniques boltzmann machines;feature extraction;image representation;image sequences;speech recognition;dbm;acoustic information;audio signals;audio visual data corpus;deep boltzmann machines;deep learning techniques;feature learning method;joint representation;practical visual speech recognition system;video sequences;visual feature extraction techniques;visual feature representation;visual information;visual signals;acoustics;feature extraction;hidden markov models;noise measurement;speech;speech recognition;visualization 
cluster-based set saliency propose cluster-based approach set saliency detection challenge sets lack topological set decomposed clusters fuzzy clustering evaluate cluster uniqueness spatial distribution cluster combine values cluster saliency function finally probabilities belonging cluster assign saliency approach detects fine-scale salient features uninteresting regions consistently lower saliency values evaluate proposed saliency model testing saliency-based keypoint detection detection benchmark evaluation method achieves balance false positive false negative error rates topological computer graphics;feature extraction;fuzzy set theory;pattern clustering;probability; detection;cluster uniqueness evaluation;cluster-based approach;false negative error rates;false positive error rates;fine-scale salient feature detection;fuzzy clustering;point set saliency detection;probabilities;saliency-based key-point detection;spatial distribution evaluation;uninteresting region detection;computational modeling;distribution functions;graphical models;robustness;shape;surface treatment;three-dimensional displays 
comprehensive multi-illuminant dataset benchmarking intrinsic image algorithms paper provide real photo dataset precise ground-truth intrinsic image prior ground-truth datasets restricted simple illumination conditions scene geometries enhanced image synthesis methods dataset provided paper based complex multi-illuminant scenarios multi-colored illumination conditions challenging cast shadows provide per-pixel intrinsic ground-truth data scenarios reflectance specularity shading illumination scenes preliminary depth evaluate state-of-the-art intrinsic image recovery methods dataset image colour analysis;image restoration;lighting;natural scenes;full per-pixel intrinsic ground-truth data;image synthesis methods;intrinsic image algorithm benchmarking;multicolored illumination conditions;multiilluminant dataset;preliminary depth information;real photo dataset;reflectance;scene geometries;shading;specularity;state-of-the-art intrinsic image recovery methods;cameras;complexity theory;geometry;image color analysis;light sources;lighting;shape 
patchmatch-based automatic lattice detection near-regular textures investigate automatically inferring lattice structure near-regular textures nrt real-world images technique leverages patchmatch algorithm finding k-nearest-neighbor knn correspondences image knns recover initial estimate wallpaper basis vectors seed vertices texture lattice iteratively expand lattice solving mrf optimization discretize space solutions mrf knns allowing efficiently accurately optimize mrf energy function particle belief propagation algorithm demonstrate technique benchmark nrt dataset wide range images geometric photometric variations method outperforms art terms texel detection rate texel localization score image matching;image texture;optimisation;mrf energy function;mrf optimization problem;geometric variation;k-nearest-neighbor;knn;near-regular texture;particle belief propagation algorithm;patchmatch-based automatic lattice detection;photometric variation;real-world image;texel detection rate;texel localization score;belief propagation;deformable models;detectors;distortion;feature extraction;lattices;proposals 
data-driven metric comprehensive evaluation saliency models decades hundreds saliency models proposed fixation prediction dozens evaluation metrics existing metrics heuristically designed draw conflict conclusions comparing saliency models consequence confusing selection metrics comparing models state-of-the-arts address propose data-driven metric comprehensive evaluation saliency models heuristically designing metric conduct extensive subjective tests saliency maps assessed human-being based user data collected tests representative evaluation metrics directly compared quantizing performances assessing saliency maps propose learn data-driven metric convolutional neural network compared existing metrics experimental data-driven metric performs consistently human-being evaluating saliency maps saliency models computer vision;neural nets;comprehensive evaluation;convolutional neural network;data-driven metric;fixation prediction;saliency maps;saliency models;benchmark testing;computational modeling;gsm;integrated circuit modeling;measurement;predictive models;solid modeling 
matrix decomposition perspective multiple graph matching graph matching wide spectrum real-world applications np-hard vision tasks realistic arises finding global node mappings batch corrupted weighted graphs paper attempt connect graph matching multi-graph matching matrix decomposition model relevant on-the-shelf convex optimization algorithms method aims extract common inliers synchronized permutations disordered weighted graphs presence deformation outliers proposed framework variants derived hope accommodating types noises experimental synthetic data real images empirically proposed paradigm exhibits behaviors performs competitively state-of-the-arts computer vision;convex programming;graph theory;image matching;matrix algebra;np-hard problem;convex optimization algorithms;corrupted weighted graphs;global node mappings;matrix decomposition model;matrix decomposition perspective;multiple graph matching;synthetic data;computer vision;encoding;image edge detection;iterative methods;matrix decomposition;optimization;robustness 
fast effective gradient minimization region fusion gradient minimization applied input signal control non-zero gradients reducing gradients signal noise preserving signal features computer vision gradient minimization applications image denoising mesh denoising image enhancement minimizing norm np-hard non-convex property result existing methods rely approximation strategies perform minimization paper method perform gradient minimization fast effective method descent approach based region fusion converges faster methods providing approximation optimal norm addition method applied images mesh topologies effectiveness approach demonstrated examples gradient methods;image denoising;image enhancement;image fusion;minimisation; images; mesh denoising; mesh topologies;np-hard problem;computer vision;gradient minimization;image denoising;image enhancement;region fusion;computer vision;linear programming;minimization;nickel;optimization;silicon;three-dimensional displays 
generic promotion diffusion-based salient object detection propose generic scheme promote diffusion-based salient object detection algorithm original re-synthesize diffusion matrix construct seed vector analysis mechanism diffusion matrix reveals close relationship saliency diffusion spectral clustering analysis propose re-synthesize diffusion matrix discriminative eigenvectors adaptive re-weighting propose generate seed vector based diffusion maps avoiding extra computation color-based seed search instance inverse normalized laplacian matrix original diffusion matrix promote salient object detection algorithm leads superior performance experimentally demonstrated eigenvalues eigenfunctions;image colour analysis;matrix algebra;object detection;search problems;color-based seed search;diffusion matrix;diffusion-based salient object detection algorithm;discriminative eigenvectors;inverse normalized laplacian matrix;saliency diffusion;spectral clustering;algorithm design analysis;computer vision;eigenvalues eigenfunctions;image color analysis;laplace equations;object detection;visualization 
nighttime haze removal glow multiple light colors paper focuses dehazing nighttime images existing dehazing methods models formulated haze daytime daytime models assume single uniform light color attributed light source directly visible scene nighttime scenes commonly visible lights sources varying colors light sources introduce noticeable amounts glow daytime haze address effects introduce nighttime haze model accounts varying light sources glow model linear combination terms: direct transmission airlight glow glow term represents light light sources scattered reaching camera based model propose framework reduces glow image nighttime image consists direct transmission airlight compute spatially varying atmospheric light map encodes light colors locally atmospheric map predict transmission nighttime scene reflection image demonstrate effectiveness nighttime haze model correction method examples compare existing daytime nighttime dehazing methods' cameras;computer vision;image colour analysis;light sources;atmospheric light map;camera;daytime haze;light color nighttime scene reflection image;multiple light color;nighttime haze model;nighttime haze removal;visible light source;atmospheric modeling;cameras;computational modeling;image color analysis;light sources;scattering;standards 
conformal low-rank sparse representation image restoration obtaining dictionary key sparse representation applied computer vision image processing image restoration expected preserving data structure sparse coding dictionary learning enhance recovery performance existing dictionary learning methods handle training samples individually missing relationships samples result dictionaries redundant atoms poor representation ability paper propose sparse representation approach called conformal low-rank sparse representation clrsr image restoration achieve compact representative dictionary conformal property introduced preserving angles local geometry formed neighboring samples feature space imposing low-rank constraint coefficient matrix lead faithful subspaces capture global structure data apply clrsr model image restoration tasks demonstrate effectiveness computer vision;feature extraction;image representation;image restoration;learning artificial intelligence ;matrix algebra;clrsr;coefficient matrix;computer vision;conformal low-rank sparse representation;conformal property;data structure;dictionary learning method;feature space;image processing;image restoration;local geometry;sparse coding;dictionaries;image coding;image reconstruction;image resolution;image restoration;manifolds;sparse matrices 
patch based nonlocal self-similarity prior learning image denoising patch based image modeling achieved success low level vision image denoising image nonlocal self-similarity nss prior refers local patch nonlocal patches image enhanced denoising performance existing methods nss input degraded image exploited utilize nss clean natural images paper propose patch pg based nss prior learning scheme learn explicit nss models natural images performance denoising pgs extracted training images putting nonlocal patches pg based gaussian mixture model pg-gmm learning algorithm developed learn nss prior demonstrate owe learned pg-gmm simple weighted sparse coding model closed-form solution perform image denoising effectively psnr measure fast speed visual quality competing methods gaussian processes;image denoising;image enhancement;learning artificial intelligence ;mixture models;pg-gmm learning algorithm;pg-based gaussian mixture model learning algorithm;pg-based nss prior learning scheme;psnr;image nss prior;image denoising;image enhancement;nonlocal self-similarity prior learning;nonlocal patch;patch based nss prior learning scheme;weighted sparse coding model;dictionaries;image denoising;image restoration;noise measurement;noise reduction;training;wavelet transforms 
automatic thumbnail generation based visual representativeness foreground recognizability automatic thumbnail generation technique based essential considerations: visually represent original photograph foreground recognized cropping downsizing steps thumbnailing factors image indexing purpose thumbnails previous methods designed highlight salient content disregarding effects downsizing propose set image features modeling considerations thumbnails learn balance relative effects thumbnail generation training image pairs composed photographs thumbnails created expert photographer experiments effectiveness approach variety images advantages techniques image recognition;learning artificial intelligence ;automatic thumbnail generation technique;foreground recognizability;image indexing;image recognition;visual representativeness;agriculture;feature extraction;image color analysis;image edge detection;measurement;training;visualization 
salicon: reducing semantic gap saliency prediction adapting deep neural networks saliency context salicon ongoing effort aims understanding predicting visual attention conventional saliency models typically rely low-level image statistics predict human fixations models perform chance gap model prediction human behavior gap limited capability models predicting eye fixations strong semantic content so-called semantic gap paper focused study narrow semantic gap architecture based deep neural network dnn leverages representational power high-level semantics encoded dnns pretrained object recognition key components fine-tuning dnns convolutionally objective function based saliency evaluation metrics integrating image scales compare method saliency models public eye tracking benchmark datasets demonstrate dnns automatically learn features saliency prediction surpass margin state-of-the-art addition model ranks top metrics mit challenge set feature extraction;gaze tracking;learning artificial intelligence ;neural nets;object recognition;dnn;mit challenge set;salicon;deep neural networks;eye fixation prediction;feature learning;high-level semantics representational power;human fixation prediction;image scales;low-level image statistics;object recognition;public eye tracking benchmark datasets;saliency evaluation metrics;saliency context;saliency prediction;semantic content;semantic gap reduction;visual attention prediction;computational modeling;measurement;neurons;object recognition;predictive models;semantics;spatial resolution 
sparsity measure tensor recovery paper propose sparsity regularizer measuring low-rank structure underneath tensor proposed sparsity measure natural physical meaning intrinsically size fundamental kronecker basis express tensor embedding sparsity measure tensor completion tensor robust pca frameworks formulate models enhance capability tensor recovery introducing relaxation forms proposed sparsity measure adopt alternating direction method multipliers admm solving proposed models experiments implemented synthetic multispectral image data sets substantiate effectiveness proposed methods image representation;image resolution;principal component analysis;tensors;admm;kronecker basis;alternating direction method multipliers;multispectral image data set;principal component analysis;sparsity measure;synthetic image data set;tensor completion;tensor recovery;tensor robust pca framework;brain modeling;computational modeling;computer vision;current measurement;principal component analysis;robustness;tensile stress 
oriented object proposals paper propose approach generate oriented object proposals oops reduce detection error caused orientations object propose efficiently locate object regions pixelwise object probability measuring objectness set sampled windows formulate proposal generation generative probabilistic model object proposals shapes sizes orientations produced locating local maximum likelihoods approach main advantages helps object detector handle objects orientations shapes proposals vary fit objects proposals tighter sampling windows fixed sizes avoids massive window sampling reducing proposals maintaining recall experiments pascal voc dataset proposed oop outperforms state-of-the-art fast methods experiments rotation invariant property helps class-specific object detector achieve performance state-of-the-art proposal generation methods object rotation scenarios scenarios generating oops fast takes image image sampling;maximum likelihood estimation;object detection;probability;shape recognition;oop;pascal voc dataset;class-specific object detector;detection error;generative probabilistic model;local maximum likelihoods;object orientations;object rotation scenarios;oriented object proposals;pixelwise object probability;window sampling;detectors;histograms;image color analysis;object detection;proposals;search problems;shape 
learning nonlinear spectral filters color image reconstruction paper idea learning optimal filters color image reconstruction based concept nonlinear spectral image decompositions proposed guy gilboa multiscale image decomposition approach based total variation regularization bregman iterations represent input data sum image layers features scales filtered images weighted linear combinations frequency layers introduce idea learning optimal filters task image denoising propose idea mixing frequency components color channels numerical experiments demonstrate learning optimal weights improve comparison standard variational approach achieves state-of-the-art image denoising image colour analysis;image denoising;image filtering;image reconstruction;iterative methods;learning artificial intelligence ;spectral analysis;bregman iteration;color channel;color image reconstruction;filtered image;frequency component;frequency layer;image denoising;image layer;learning nonlinear spectral filter;learning optimal filter;multiscale image decomposition approach;nonlinear spectral image decomposition;optimal weight;total variation regularization;weighted linear combination;color;correlation;image color analysis;image denoising;image reconstruction;noise reduction;tv 
white: ground truth colors color constancy correction limitation color constancy inability establish ground truth colors evaluating corrected images existing datasets images scenes color chart included chart's neutral colors grayscale patches provide ground truth illumination estimation correction corrected neutral colors lie achromatic camera's color space r=g=b correct rgb values color patches result methods estimate diagonal matrix ensures neutral colors correct paper overcome limitation illuminations diagonal matrix capable correcting neutral colors colors scene finding ground truth rgb values color chart camera's color space correct images existing datasets correct colors color corrected datasets modify existing color constancy algorithms perform image correction image colour analysis;lighting;matrix algebra;natural scenes;color chart scenes;color constancy correction;color patch;color space;corrected image evaluation;corrected neutral colors;diagonal matrix estimation;ground truth rgb values;ground truth colors;illumination correction;illumination estimation;image correction;image datasets;cameras;computational modeling;estimation;image color analysis;lighting;sensitivity;transforms 
rgb-guided hyperspectral image upsampling hyperspectral imaging lack spatial resolution limitations hardware design imaging sensors contrary imaging sensors capture rgb image resolution multiple times larger hyperspectral image paper algorithm enhance upsample resolution hyperspectral images algorithm consists stages: spatial upsampling stage spectrum substitution stage spatial upsampling stage guided resolution rgb image scene spectrum substitution stage utilizes sparse coding locally refine upsampled hyperspectral image dictionary substitution experiments algorithm highly effective outperformed state-of-the-art matrix factorization based approaches image colour analysis;image enhancement;image resolution;image sampling;image sensors;dictionary substitution;high resolution rgb image;hyperspectral image upsampling;imaging sensor;sparse coding;spatial resolution;spatial upsampling stage;spectrum substitution stage;dictionaries;hyperspectral imaging;image reconstruction;spatial resolution;training 
projection manifold elongated structures accurate extraction detection elongated structures images image stacks critical prerequisite applications machine learning-based approaches deliver superior performance methods essentially classify individual locations explicitly model strong relationship exists neighboring result isolated erroneous responses discontinuities topological errors score maps solve projecting patches score map nearest neighbors set ground truth training patches algorithm induces global spatial consistency classifier score map returns provably geometrically consistent apply algorithm challenging datasets domains compares favorably state-of-the-art methods feature extraction;image classification;learning artificial intelligence ;object detection;structural engineering computing; image; image stack;classifier score map;discontinuities;elongated structure detection;elongated structure manifold;extraction;global spatial consistency;ground truth training patch;individual location classification;isolated erroneous response;machine learning-based approach;patch projection;score map;topological error;biomembranes;feature extraction;image segmentation;manifolds;three-dimensional displays;training;transforms 
naive bayes super-resolution forest paper fast high-performance method super resolution external learning contribution leading excellent performance bimodal tree clustering exploits antipodal invariance coarse-to-high-res mapping natural image patches scalability finer partitions underlying coarse patch space training ensemble bimodal trees computed providing linearizations mapping main contribution fast inference algorithm selects suitable mapping function tree ensemble patch adopting local naive bayes formulation experimental validation promising scalability properties reflect suitability proposed model generalized tasks method magnitude faster performs objectively subjectively current art bayes methods;image resolution;inference mechanisms;learning artificial intelligence ;trees mathematics ;bimodal tree;external learning;inference algorithm;mapping function;naive bayes superresolution forest;dictionaries;image reconstruction;image resolution;manifolds;principal component analysis;training;vegetation 
pop image fusion -- derivative domain image fusion reintegration applications multiple images fused form single summary greyscale colour output including computational photography rgb-nir diffusion tensor imaging medical remote sensing intuitively image fusion carried derivative domain composite fused derivative accounts detail images gradient field reintegrated reintegration step hallucinates detail appearing input image bands including halo bending artifacts paper avoid hallucinated details avoiding reintegration step builds directly socolinsky wolff derive equivalent gradient field per-pixel di zenzo structure tensor defined product image jacobian x-and y-derivatives projection original image principal characteristic vector outer product pop jacobian generates equivalent gradient field derived fused image derivative structure seek projection meaningful jacobian non-zero derivatives diffuse projection directions bilateral filter calculate fused image pop fused image maximal fused detail avoids hallucinated artifacts experiments demonstrate method delivers art image fusion performance jacobian matrices;gradient methods;image colour analysis;image fusion;principal component analysis;tensors;pop;pop image fusion;bilateral filter;colour output;composite fused derivatives;computational photography;derivative domain image fusion;diffusion tensor imaging;equivalent gradient field;gradient field;hallucinated details;image jacobian;nonzero derivatives;per-pixel di zenzo structure tensor;principal characteristic vector outer product;remote sensing;x-derivatives;y-derivatives;discrete wavelet transforms;eigenvalues eigenfunctions;image fusion;jacobian matrices;matrix decomposition;periodic structures;tensile stress 
adaptive spatial-spectral dictionary learning hyperspectral image denoising hyperspectral imaging beneficial diverse range applications diagnostic medicine agriculture surveillance hyperspectral images times suffer degradation limited light introduces noise imaging process paper propose effective model hyperspectral image hsi denoising considers underlying characteristics hsis: sparsity spatial-spectral domain correlation spectra non-local self-similarity space exploit correlation spectra non-local self-similarity space noisy hsi learn adaptive spatial-spectral dictionary employ local non-local sparsity hsi learned spatial-spectral dictionary design hsi denoising model effectively solved iterative numerical algorithm parameters adaptively adjusted clusters noise levels experimental hsi denoising proposed method provide substantial improvements current state-of-the-art hsi denoising methods terms objective metric subjective visual quality geophysical image processing;image denoising;iterative methods;hsi denoising model;adaptive spatial-spectral dictionary learning;diagnostic medicine;hyperspectral image denoising;iterative numerical algorithm;adaptation models;dictionaries;hyperspectral imaging;noise reduction;numerical models;principal component analysis;spectral analysis 
connected guided image filtering paper linear time connected guided filter introducing minimum spanning tree mst guided filter gf intensity based filtering kernel gf apt overly smooth edges fixed-shape local box support region gf geometric-adaptive filter introduces extra spatial term tree similarity filtering kernel gf substitutes box window implicit support region establishing all-pairs-connections pixels image assigning spatial-intensity-aware similarity connections adaptive implicit support region composed pixels kernel weights entire image domain advantage predefined local box window structure image reason that: mst efficiently structure image kernel weight filter considers tree distance defined mst reasons filter achieves edge-preserving demonstrate strength proposed filter applications experimental method produces state-of-the-art methods image filtering;trees mathematics ;mst;edge preserving image filter;fixed-shape local box support region;fully connected guided image filtering;intensity based filtering kernel;minimum spanning tree;spatial-intensity-aware similarity;computer vision;estimation;image edge detection;kernel;linear programming;optimization;smoothing methods 
segment graph based image filtering: fast structure-preserving smoothing paper design edge-aware structure named segment graph represent image develop double weighted average image filter sgf based segment graph sgf tree distance segment graph define internal weight function filtering kernel enables filter smooth high-contrast details textures preserving major image structures external weight function introduce user smoothing window balance smoothing effects node segment graph set threshold adjust edge-preserving performance advantages sgf flexible applications overcome halo\ leak\ appearing state-of-the-art approaches finally importantly develop linear algorithm implementation sgf time complexity gray-scale dimensional images kernel size intensity range typically fastest edge-preserving filters cpu implementation achieves megapixel performing filtering -channel color images strength proposed filter demonstrated applications including stereo matching optical flow joint depth map upsampling edge-preserving smoothing edges detection image abstraction texture editing computational complexity;edge detection;image colour analysis;image filtering;image representation;image segmentation;smoothing methods;trees mathematics -channel color images;cpu implementation;o time complexity;sgf;double weighted average image filter;edge detection;edge-aware structure;edge-preserving filters;edge-preserving performance;edge-preserving smoothing;external weight function;filtering kernel;gray-scale images;halo problem;high dimensional images;high-contrast details;image abstraction;image denoising;image representation;image structures;internal weight function;joint depth map upsampling;leak problem;named segment graph;optical flow;segment graph based image filtering;stereo matching;structure-preserving smoothing;texture editing;tree distance;image edge detection;image segmentation;kernel;nonlinear optics;optical imaging;silicon;smoothing methods 
deep networks image super-resolution sparse prior deep learning techniques applied computer vision including low-level image restoration image super-resolution models based deep neural networks proposed attained superior performance overshadows previous handcrafted models question arises large-capacity data-driven models dominant solution ill-posed super-resolution paper argue domain expertise represented conventional sparse coding model valuable combined key ingredients deep learning achieve improved sparse coding model designed super-resolution incarnated neural network trained cascaded structure interpretation network based sparse coding leads efficient effective training reduced model size model evaluated wide range images advantage existing state-of-the-art methods terms restoration accuracy human subjective quality computer vision;image resolution;learning artificial intelligence ;neural nets;computer vision;data-driven models;deep learning techniques;deep neural networks;human subjective quality;image superresolution;low-level image restoration problems;restoration accuracy;sparse coding model;dictionaries;encoding;image coding;machine learning;neural networks;neurons;training 
convolutional color constancy color constancy inferring color light illuminated scene illumination color removed underconstrained solved modeling statistical regularities colors natural objects illumination contrast paper reformulate color constancy spatial localization task log-chrominance space allowing apply techniques object detection structured prediction color constancy directly learning discriminate correctly white-balanced images white-balanced images model improve performance standard benchmarks image colour analysis;learning artificial intelligence ;object detection;statistical analysis; spatial localization task;convolutional color constancy;illumination color;learning;log-chrominance space;object detection;statistical regularity;white-balanced image;cognition;convolution;histograms;image color analysis;lighting;object detection;training 
learning ordinal relationships mid-level vision propose framework infers mid-level visual properties image learning ordinal relationships estimating metric quantities directly system proposes pairwise relationship estimates input image sparse probabilistic ordinal measurements globalized create dense output map continuous metric measurements estimating relationships pairs advantages metric estimation: solves simpler metric regression humans relative judgements data collection easier ordinal relationships invariant monotonic transformations data increasing robustness system providing qualitatively demonstrate frame-work mid-level vision tasks: intrinsic image decomposition depth rgb image train systems architecture data modalities provide analysis models learn simple rules ordinal decisions apply algorithm depth estimation intrinsic image decomposition state-of-the-art computer vision;estimation theory;probability;rgb image;continuous metric measurement;depth estimation;intrinsic image decomposition;midlevel vision;midlevel visual property;ordinal relationship;sparse probabilistic ordinal measurement;context;estimation;image decomposition;image edge detection;image segmentation;marine vehicles;measurement 
thin structure estimation curvature regularization applications vision require estimation thin structures boundary edges surfaces roads blood vessels neurons previous approaches simultaneously detect delineate thin structures sub-pixel localization real-valued orientation estimation ill-posed requires regularization propose objective function combining detection likelihoods prior minimizing curvature center-lines surfaces simple block-coordinate descent develop algorithm perform joint optimization location detection variables effectively lower bound optimization algorithm applies quadratic absolute curvature proposed vision framework higher-level applications illustrate advantage approach range examples computational geometry;computer vision;estimation theory;object detection;optimisation;absolute curvature;computer vision;curvature regularization;detection likelihood;lower bound optimization algorithm;quadratic curvature;real-valued orientation estimation;subpixel localization;thin structure estimation;context;estimation;image edge detection;optimization;surface reconstruction;surface treatment;three-dimensional displays 
harf: hierarchy-associated rich features salient object detection state-of-the-art salient object detection models perform simple scenes complex difficulties highlighting salient objects completely background lack robust features saliency prediction address issue paper proposes hierarchy-associated feature construction framework salient object detection based integrating elementary features multi-level regions hierarchy multi-layered deep learning features introduced incorporated elementary features framework compact integration scheme leads rich feature representation represent context object/background discriminative robust salient object detection extensive experiments challenging benchmark datasets demonstrate proposed approach outperforms state-of-the-art salient object detection feature extraction;image representation;learning artificial intelligence ;object detection;harf;compact integration scheme;elementary features;feature representation;hierarchy-associated feature construction framework;hierarchy-associated rich features;multilayered deep learning features;multilevel region;saliency prediction;salient object detection;biological system modeling;computational modeling;feature extraction;image segmentation;object detection;robustness;visualization 
deep colorization paper investigates colorization converts grayscale image colorful version difficult requires manual adjustment achieve artifact-free quality instance requires human-labelled color scribbles grayscale target image careful selection colorful reference images capturing scene grayscale target image previous methods paper aims high-quality fully-automatic colorization method assumption perfect patch matching technique extremely large-scale reference database sufficient color images reliable solution colorization patch matching noise increase respect size reference database practice inspired success deep learning techniques provide amazing modeling large-scale data paper re-formulates colorization deep learning techniques directly employed ensure artifact-free quality joint bilateral filtering based post-processing step proposed numerous experiments demonstrate method outperforms state-of-art algorithms terms quality speed image colour analysis;image filtering;image matching;learning artificial intelligence ;colorful image;colorful reference images;deep colorization;deep learning techniques;extremely large-scale reference database;grayscale image;high-quality fully-automatic colorization method;human-labelled color;joint bilateral filtering based post-processing;patch matching noise;patch matching technique;databases;feature extraction;gray-scale;image color analysis;machine learning;neural networks;neurons 
image matting kl-divergence based sparse sampling previous sampling-based image matting methods typically rely heuristics collecting representative samples regions performance deteriorates underlying assumptions satisfied alleviate paper approach formulate sampling sparse subset selection propose pick set candidate samples explains unknown pixels distance measure comparing samples based kl-divergence distributions features extracted vicinity samples standard benchmark dataset image matting demonstrate approach accurate compared state-of-the-art methods feature extraction;image sampling;kl-divergence based sparse sampling;feature extraction;sampling-based image matting methods;sparse subset selection problem;atmospheric measurements;feature extraction;image color analysis;linear programming;mathematical model;particle measurements;robustness 
intrinsic decomposition image sequences local temporal variations method intrinsic image decomposition aims decompose images reflectance shading layers input sequence images varying illumination acquired static camera indoor scene moving light source outdoor timelapse leverage local color variations observed time infer constraints reflectance solve ill-posed image decomposition derive adaptive local energy observations local neighborhood time integrate distant pairwise constraints enforce coherent decomposition surfaces consistent shading method solely based multiple observations lambertian scene varying illumination require user interaction scene geometry explicit lighting model compare intrinsic decomposition methods synthetic captured datasets cameras;geometry;image colour analysis;image segmentation;image sequences;lighting;lambertian scene;coherent decomposition;distant pairwise constraints;ill-posed image decomposition problem;indoor scene;intrinsic image sequence decomposition methods;local color variations;local temporal variations;outdoor timelapse;scene geometry;shading layers;static camera;user interaction;geometry;image color analysis;image decomposition;image sequences;light sources;lighting;robustness 
low-rank tensor approximation laplacian scale mixture modeling multiframe image denoising patch-based low-rank models effective exploiting spatial redundancy natural images application image denoising two-dimensional low-rank model exploit spatio-temporal correlation larger data sets multispectral images mris propose low-rank tensor approximation framework laplacian scale mixture lsm modeling multi-frame image denoising patches form tensor d-order high-order singular decomposition hosvd applied tensor task multiframe image denoising formulated maximum posterior map estimation lsm prior tensor coefficients unknown sparse coefficients hidden lsm parameters efficiently estimated method alternating optimization derived closed-form solutions subproblems experimental spectral dynamic mri images proposed algorithm preserve sharpness image structures outperform existing state-of-the-art multiframe denoising methods bm tensor dictionary learning laplace equations;estimation theory;image denoising;mixture models;optimisation;redundancy;singular decomposition;spectral analysis;tensors; mri;bm d;hosvd;lsm modeling;lsm prior;laplacian scale mixture modeling;map estimation problem;alternating optimization;closed-form solution;dynamic mri image;grouped tensor;hidden lsm parameter;high-order singular decomposition;image structure;low-rank tensor approximation framework;maximum posterior estimation problem;multiframe denoising method;multiframe image denoising;multispectral image;natural image;patch-based low-rank model;sparse coefficient;spatial redundancy;spatio-temporal correlation;spectral mri image;tensor coefficient;tensor dictionary learning;two-dimensional low-rank model;dictionaries;image denoising;laplace equations;noise reduction;silicon;tensile stress;three-dimensional displays 
learning parametric distributions image super-resolution: patch matching meets sparse coding existing approaches image super-resolution sr data-driven based internet-scale matching web image retrieval model-based formulated maximizing posterior estimation conceptually simple heuristic constrained fundamental limit frequency aliasing paper propose develop hybrid approach sr combining lines ideas parameters underlying sparse distributions desirable hr image patches learned pair lr image retrieved hr images hybrid approach interpreted attempt reconciling difference parametric nonparametric models low-level vision tasks experimental proposed hybrid sr method performs existing state-of-the-art methods terms subjective objective image qualities internet;computer vision;image matching;image resolution;image retrieval;learning artificial intelligence ;hr image patch;internet-scale matching;lr image;web image retrieval;frequency aliasing;image super-resolution;learning parametric distribution;low-level vision task;nonparametric model;objective image quality;patch matching;retrieved hr image;sparse coding;sparse distribution;subjective image quality;dictionaries;image reconstruction;image resolution;image retrieval;laplace equations;visualization 
improving image restoration soft-rounding classes images text barcode pattern images property pixels distinct subset values knowledge benefit restoration images considered current restoration methods effective efficient approach incorporate knowledge distinct pixel values pristine images regularized squares restoration framework introduce regularizer attains designated pixel values quadratic penalty function intervals incorporated regularized squares restoration framework regularizer leads simple efficient step resembles extends rounding operation term soft-rounding apply soft-rounding enhanced solution restoration binary text/barcode images pattern images multiple distinct pixel values experimental soft-rounding enhanced restoration methods achieve improvement visual quality quantitative measures psnr ssim regularizer benefit restoration natural images image enhancement;image resolution;image restoration;least squares approximations;text analysis;psnr;ssim;binary barcode image restoration;binary text image restoration;general natural image restoration;multiple distinct pixel value;quadratic penalty function;regularized square restoration framework;soft-rounding enhanced restoration method;visual quality;convolution;histograms;image restoration;kernel;noise measurement;transforms;visualization 
difference: direct pre-image reconstruction pose estimation differentiating hog histogram oriented gradient hog descriptor led advances computer vision decade art approaches realize feature computation piecewise differentiable pipelines build hog differentiable lends advanced introspection opportunities end-to-end optimization implementation δhog based auto-differentiation toolbox chumpy applications pre-image visualization pose estimation extends existing differentiable renderer opendr pipeline applications improve respective state-of-the-art hog approaches computer vision;gradient methods;image reconstruction;optimisation;pose estimation;hog differentiation;computer vision;end-to-end optimization;histogram oriented gradient;pose estimation;pre-image reconstruction;computational modeling;design automation;histograms;pipelines;solid modeling;visualization 
efficient statistical method image noise level estimation paper address estimating noise level single image contaminated additive zero-mean gaussian noise provide rigorous analysis statistical relationship noise variance eigenvalues covariance matrix patches image state-of-the-art noise estimation methods underestimate noise level image derive nonparametric algorithm efficient noise level estimation based observation patches decomposed clean image lie low-dimensional subspace performance method guaranteed theoretically empirically method outperforms existing state-of-the-art algorithms estimating noise level executing time experiments demonstrate denoising algorithm bm algorithm achieves optimal performance noise variance estimated algorithm covariance matrices;eigenvalues eigenfunctions;image denoising;nonparametric statistics;statistical analysis;bm denoising algorithm;additive zero-mean gaussian noise;clean image patches;covariance matrix;eigenvalues;image noise level;image noise level estimation;low-dimensional subspace;noise variance;noise variance estimation;nonparametric algorithm;optimal performance;statistical method;covariance matrices;eigenvalues eigenfunctions;estimation;gaussian distribution;mathematical model;noise level;random variables 
contour detection characterization asynchronous event sensors bio-inspired asynchronous event-based dynamic vision sensor records temporal luminance scene temporal resolution events triggered luminance events occur boundary objects detection contours essential step interpretation scene paper approach learn location contours border ownership structured random forests event-based features encode motion timing texture spatial orientations classifier integrates elegantly time utilizing classification computed finally contour detection boundary assignment demonstrated layer-segmentation scene experimental demonstrate performance boundary detection segmentation brightness;computer vision;image classification;image resolution;image segmentation;object detection;bio-inspired asynchronous event-based dynamic vision sensor record;boundary assignment;boundary detection;boundary segmentation;contour detection;scene layer-segmentation;structured random forest;temporal resolution;computer vision;feature extraction;image edge detection;image segmentation;motion segmentation;sensors;voltage control 
class-specific image deblurring image deblurring fundamental blur kernel suppresses spatial frequencies difficult recover reliably paper explore potential class-specific image prior recovering spatial frequencies attenuated blurring process devise prior based class-specific subspace image intensity responses band-pass filters learn aggregation subspaces frequency bands serves class-specific prior restoration frequencies recovered generic image priors extensive validation method equipped prior yields image quality state-of-the-art methods db terms image psnr image categories including portraits cars cats pedestrians household objects band-pass filters;image filtering;image restoration;band-pass filters;class-specific image deblurring;class-specific image prior;class-specific subspace;frequency restoration;image psnr;image intensity responses;image quality;spatial frequency recovery;band-pass filters;frequency-domain analysis;image edge detection;image restoration;kernel;minimization;training 
high-for-low low-for-high: efficient boundary detection deep object features applications high-level vision current boundary detection systems rely exclusively low-level features color texture perception studies humans employ object-level reasoning judging pixel boundary inspired observation predict boundaries exploiting object-level features pretrained object-classification network method viewed high-for-low\ approach high-level object features inform low-level boundary detection process model achieves state-of-the-art performance established boundary detection benchmark efficient additionally semantic nature boundaries aid high-level vision tasks demonstrate boundaries improve performance state-of-the-art methods semantic boundary labeling semantic segmentation object proposal generation view process low-for-high'\ scheme low-level boundaries aid high-level vision tasks contributions boundary detection system accurate efficient generalizes multiple datasets improve existing state-of-the-art high-level vision methods distinct tasks edge detection;feature extraction;image classification;image colour analysis;image texture;boundary prediction;deep object features;high-for-low approach;high-level vision tasks;low-level boundary detection process;low-level color feature;low-level texture feature;object proposal generation;object-level features;pretrained object-classification network;semantic boundary labeling;semantic segmentation;convolutional codes;feature extraction;image edge detection;interpolation;labeling;semantics;training 
variational depth superresolution example-based edge representations paper propose method depth image superresolution combines advances based upsampling variational superresolution based blur kernel traditional depth superresolution approaches additional resolution intensity images guidance superresolution method learn dictionary edge priors external database low resolution examples variational sparse coding approach dictionary infer strong edge priors additionally traditional sparse coding constraints difference overlap neighboring edge patches minimized optimization edge priors variational superresolution anisotropic guidance regularization sparse coding variational superresolution depth solved based primal-dual formulation exhaustive numerical visual evaluation method outperforms existing approaches multiple real synthetic datasets image representation;image resolution;image sampling;anisotropic guidance;blur kernel;depth image superresolution;depth superresolution approach;edge prior;example based upsampling;example-based edge representation;external database;high resolution intensity image;higher regularization;neighboring edge patch;numerical evaluation;primal-dual formulation;real dataset;resolution example;sparse coding constraint;synthetic dataset;variational depth superresolution;variational sparse coding approach;variational superresolution;visual evaluation;dictionaries;encoding;energy resolution;image edge detection;image reconstruction;spatial resolution 
conditioned regression models non-blind single image super-resolution single image super-resolution task field computer vision practical applications current state-of-the-art methods typically rely machine learning algorithms infer mapping low-to high-resolution images methods single fixed blur kernel training assume exact kernel underlying image formation process test images setting realistic practical applications blur typically test image paper loosen restrictive constraint propose conditioned regression models including convolutional neural networks random forests effectively exploit additional kernel training inference training single model previous methods re-trained blur kernel individually achieve demonstrate evaluations empirically proposed conditioned regression models effectively handle scenarios blur kernel image ii outperform approaches trained single kernel computer vision;image resolution;image restoration;learning artificial intelligence ;neural nets;regression analysis;computer vision;conditioned regression models;convolutional neural networks;fixed blur kernel;high-resolution images;image formation process;low-resolution images;machine learning algorithms;nonblind single image superresolution;random forests;adaptation models;computer vision;dictionaries;image resolution;kernel;neural networks;training 
video super-resolution deep draft-ensemble learning propose direction fast video super-resolution videosr sr draft ensemble defined set high-resolution patch candidates final image deconvolution method main components -- sr draft ensemble generation optimal reconstruction component renovate traditional feedforward reconstruction pipeline greatly enhance ability compute super resolution motion variation errors arising process combine sr drafts nonlinear process deep convolutional neural network cnn analyze framework proposed explain unique advantages compared previous iterative methods update modules passes promising experimental natural video sequences deconvolution;image motion analysis;image reconstruction;image resolution;image sequences;neural nets;video signal processing;cnn;sr draft ensemble generation;videosr;deep convolutional neural network;deep draft-ensemble learning;feedforward reconstruction pipeline;high-resolution patch candidates;image deconvolution;motion variation;natural video sequences;video super-resolution;deconvolution;feedforward neural networks;image reconstruction;image resolution;kernel;motion estimation;optical imaging 
pan-sharpening hyper-laplacian penalty pan-sharpening task fusing spectral low resolution multispectral images spatial resolution panchromatic image approaches trade-off spectral spatial quality computational efficiency method pan-sharpening sparsity-promoting objective function preserves spatial spectral content efficient optimize objective incorporates -norm leverage computationally efficient methods alternating direction method multipliers additionally objective penalizes image gradients enforce resolution fidelity exploits fourier domain forfurther computational efficiency visual quality metrics demonstrate proposed objective function achieve spatial spectral resolution previous well-known methods competitive computational efficiency fourier analysis;image fusion;image resolution;fourier domain;computational efficiency;high resolution fidelity;high resolution panchromatic image;hyper-laplacian penalty;image gradients;l -norm;low resolution multispectral images;multipliers alternating direction method;objective function;pan-sharpening;sparsity-promoting objective function;spatial content preservation;spectral content preservation;spectral fusion;visual quality metrics;image reconstruction;laplace equations;linear programming;mathematical model;spatial resolution;tv 
video restoration yin-yang phasing common video degradation untreated literature call yin-yang phasing yyp yyp characterized involuntary dramatic flip-flop intensity chromaticity object video plays temporal artifacts occur ill illumination conditions triggered object or/and camera motions mislead settings camera's auto-exposure white paper investigate propose video restoration technique suppress yyp artifacts retain temporal consistency objects appearance inter-frame spatially-adaptive optimal tone mapping video quality improved image enhancer designed weber's perception principle exploiting second-order statistics scene experimental encouraging effective practical solution common surprisingly understudied cameras;image enhancement;image motion analysis;image restoration;video signal processing;weber perception principle;yyp;yin-yang phasing;camera auto-exposure;camera motions;illumination conditions;image enhancer;optimal tone mapping;second-order statistics;temporal consistency;video degradation problem;video quality;video restoration technique;cameras;degradation;histograms;image restoration;lighting;spatial resolution;visualization 
rolling shutter super-resolution classical multi-image super-resolution sr algorithms designed ccd cameras assume motion images global cmos sensors increasingly started replace expensive ccd counterparts applications respect assumption motion camera relative scene exposure duration image row-wise acquisition mechanism paper study hitherto unexplored topic multi-image sr cmos cameras initially develop sr observation model accounts row-wise distortions called rolling shutter\ rs observed images captured non-stationary cmos cameras propose unified rs-sr framework rs-free high-resolution image row-wise motion distorted low-resolution images demonstrate efficacy proposed scheme synthetic data real images captured hand-held cmos camera quantitative qualitative assessments reveal method advances state-of-the-art cmos image sensors;image capture;image motion analysis;image resolution;cmos camera;rs-sr framework;sr observation model;image capture;multiimage sr;rolling shutter superresolution;row-wise motion;cmos integrated circuits;cameras;charge coupled devices;image resolution;sensor arrays;signal resolution 
learning large-scale automatic image colorization automated method image colorization learns colorize examples method exploits learch framework train quadratic objective function chromaticity maps comparable gaussian random field coefficients objective function conditioned image features random forest objective function admits correlations spatial scales control spatial error colorization image images colorized minimizing objective function demonstrate method outperforms natural baseline large-scale experiments images real scenes demanding loss function demonstrate learning model conditioned scene produces improved incorporate desired color histogram objective function lead improvements image processing;learch framework;chromaticity maps;color histogram;large-scale automatic image colorization learning;quadratic objective function;random forest;spatial error control;color;histograms;image color analysis;linear programming;optimization;regression tree analysis;standards 
compression artifacts reduction deep convolutional network lossy compression introduces complex compression artifacts blocking artifacts ringing effects blurring existing algorithms focus removing blocking artifacts produce blurred output restores sharpened images accompanied ringing effects inspired deep convolutional networks dcn super-resolution formulate compact efficient network seamless attenuation compression artifacts demonstrate deeper model effectively trained features learned shallow network easy hard\ idea systematically investigate practical transfer settings effectiveness transfer learning low level vision method superior performance state-of-the-arts benchmark datasets real-world twitter image coding;image resolution;image restoration;neural nets;blocking artifacts;blurring;complex compression artifacts;compression artifacts reduction;deep convolutional network;image restoration;image superresolution;lossy compression;low level vision problems;ringing effect;transfer learning;feature extraction;image coding;image reconstruction;image resolution;image restoration;noise measurement;transform coding 
multiple-hypothesis affine region estimation anisotropic log filters propose method estimating multiple-hypothesis affine regions keypoint anisotropic laplacian-of-gaussian log filter conventional affine region detectors hessian/harris-affine iterate affine region fits image patch iterative searching adversely initial avoid multiple detections single keypoint demonstrate responses anisotropic log filters efficiently computed factorizing manner spectral sift log filters densely sampled parameter space reconstructed weighted combination limited representative filters called eigenfilters\ singular decomposition reconstructed filter responses sampled parameters interpolated continuous representation series proper functions efficient multiple extrema searching continuous space experiments revealed method repeatability conventional methods image filtering;image matching;image representation;singular decomposition;hessian/harris-affine;affine region detectors;anisotropic laplacian-of-gaussian;anisotropic log filters;continuous representation;continuous space;eigenfilters;image patch;iterative searching;multiple extrema searching;multiple-hypothesis affine region estimation;multiple-hypothesis affine regions;parameter space;reconstructed filter responses;representative filters;sampled parameters;singular decomposition;spectral sift;convolution;detectors;eigenvalues eigenfunctions;estimation;image reconstruction;shape;three-dimensional displays 
self-paced multiple-instance learning framework co-saliency detection emerging topic co-saliency detection aims simultaneously extracting common salient objects images traditional co-saliency detection approaches rely heavily human knowledge designing hand-crafted metrics explore intrinsic patterns underlying co-salient objects strategies suffer poor generalization capability flexibly adapt scenarios real applications lack insightful understanding biological mechanisms human visual co-attention alleviate propose framework task naturally reformulating multiple-instance learning mil integrating self-paced learning spl regime proposed framework hand capable fitting insightful metric measurements discovering common patterns co-salient regions self-learning mil hand promise learning reliability stability simulating human learning process spl experiments benchmark datasets demonstrated effectiveness proposed framework compared state-of-the-arts feature extraction;object detection;unsupervised learning;mil problem;spl;co-saliency detection;co-salient regions;human learning process;learning reliability;learning stability;metric measurements;patterns discovery;salient objects extraction;self-paced learning;self-paced multiple-instance learning framework;context;detectors;measurement;reliability;search problems;training;visualization 
external patch prior guided internal clustering image denoising natural image modeling plays key role vision image denoising image priors regularize denoising process ill-posed inverse category denoising methods exploit priors tv sparsity learned external clean images reconstruct noisy image category methods exploit internal prior self-similarity reconstruct latent image internal prior based methods achieved impressive denoising improvement visual quality difficult increase noise level paper propose exploit image external patch prior internal self-similarity prior jointly develop external patch prior guided internal clustering algorithm image denoising natural image patches form multiple subspaces utilizing gaussian mixture models gmms learning image patches clustered subspaces learned learned gmms clean images guide clustering noisy-patches input noisy images low-rank approximation process estimate latent subspace image recovery numerical experiments proposed method outperforms state-of-the-art denoising algorithms bm wnnm gaussian processes;image denoising;image reconstruction;learning artificial intelligence ;mixture models;pattern clustering;gmm learning;gaussian mixture model learning;external patch prior guided internal clustering;illposed inverse problem;image denoising;image recovery;latent image reconstruction;low-rank approximation process;natural image modeling;natural image patch;visual quality improvement;covariance matrices;euclidean distance;image denoising;image reconstruction;minimization;noise measurement;noise reduction 
self-calibration optical lenses high-quality lenses suffer optical aberrations aperture lens-to-lens deviations manufacturing tolerances rendering current software solutions dxo lightroom ptlens insufficient adapt generic lens blur models propose method enables self-calibration lenses natural image set images develop machine learning framework exploit recorded images distills accurate model considered lens aberrations;calibration;image processing;learning artificial intelligence ;lenses;optical processing;high-quality lenses;lens-to-lens deviations;machine learning;natural image;optical aberrations;optical lenses;self-calibration;software solutions;adaptive optics;apertures;deconvolution;estimation;lenses;optical imaging;optical variables measurement 
illumination robust color naming label propagation color composition property computer vision tasks image retrieval object classification paper address inferring color composition intrinsic reflectance objects shadows highlights change observed color dramatically achieve color label propagation recovering intrinsic reflectance color labels propagated regions sharing reflectance direction propagation promoted regions illumination normal view angles abnormal regions detect shadowed highlighted regions pairs regions reflectance joint inference process trim inconsistent identities connections evaluation collect datasets images noticeable highlights shadows experimental model effectively color composition real-world images computer vision;image colour analysis;inference mechanisms;object detection;color composition;color naming;computer vision;inference process;label propagation;object intrinsic reflectance;computer vision;green products;histograms;image color analysis;image edge detection;lighting;robustness 
unsupervised cross-modal synthesis subject-specific scans cross-modal synthesis subject-specific scans receiving attention medical imaging community synthesis approaches introduced tailored specific application proposed supervised setting assume availability training data set subjects source target modalities collecting multiple scans subject undesirable address issue propose unsupervised cross-modal medical image synthesis approach paired training data source modality image subject generate multiple target modality candidate values voxel independently cross-modal nearest neighbor search select candidate values jointly voxels simultaneously maximizing global mutual cost function local spatial consistency cost function finally coupled sparse representation refinement synthesized images experiments generating -mri brain scans -mri vice versa demonstrate synthesis capability proposed unsupervised approach comparable state-of-the-art supervised approaches literature medical image processing;t -mri brain scans;t -mri;coupled sparse representation;general unsupervised cross-modal medical image synthesis approach;global mutual cost function;local spatial consistency cost function;medical imaging community;subject-specific scans;biomedical imaging;image generation;image resolution;magnetic resonance imaging;nearest neighbor searches;training;training data 
learning boost filamentary structure segmentation challenging filamentary structure segmentation broad range applications biological medical fields critical challenging issue remains detect restore filamentary fragments backgrounds: fragments diverse shapes appearances backgrounds cluttered ambiguous focusing issue paper proposes iterative two-step learning-based approach boost performance based base segmenter arbitrarily chosen existing segmenters: start initial partial segmentation filamentary structure confidence based existing segmenter define scanning horizon epsilon balls centred partial segmentation result step approach centers data-driven latent classification tree model detect filamentary fragments model learned training process distinct local figure/background separation scenarios established geometrically organized tree structure step spatially restores isolated fragments current partial segmentation accomplished completion fields matting steps alternated growth partial segmentation result input image space explored approach generic easily augmented wide range existing supervised/unsupervised segmenters produce improved result empirically verified specific filamentary structure segmentation tasks: retinal blood vessel segmentation neuronal segmentations noticeable improvement original state-of-the-arts blood vessels;eye;image classification;image restoration;image segmentation;learning artificial intelligence ;medical image processing;object detection;trees mathematics ;base segmenter;data-driven latent classification tree model;epsilon balls;filamentary fragment detection;filamentary fragment restoration;filamentary structure segmentation boosting;image space;initial partial segmentation;iterative two-step learning-based approach;local figure/background separation scenarios;neuronal segmentation;retinal blood vessel segmentation;scanning horizon;supervised/unsupervised segmenter;training process;biomedical imaging;context;image restoration;image segmentation;retina;shape;vegetation 
weakly-supervised structured output learning flexible latent graphs high-order loss functions introduce structured output models latent graph flexible terms nodes structure training process minimises high-order loss function weakly annotated training set models developed context microscopy imaging malignant tumours estimation proportion classes microcirculatory supply units mcsu assessment efficacy common cancer treatments mcsu region tumour tissue supplied microvessel proposed methodologies input multimodal microscopy images tumour estimate proportion mcsu classes estimation facilitated underlying latent graph manual annotations mcsu represented node graph labelled mcsu class image location training process manual weak annotations consisting mcsu classes training image training objective minimisation high-order loss function based norm error manual estimated annotations models proposed based flexible latent structure support vector machine flssvm based deep convolutional neural network dcnn model dataset weakly annotated pairs multimodal images tumours quantitative dcnn superior qualitative flssvm display correlation values proportion mcsu classes compared manual annotations cancer;graph theory;medical image processing;neural nets;support vector machines;tumours;dcnn model;flssvm;cancer treatment;deep convolutional neural network;flexible graph;flexible latent structure support vector machine;high-order loss function;latent graph;malignant tumour;microcirculatory supply units;microscopy imaging;weakly-supervised structured output learning;cancer;computer vision;manuals;microscopy;support vector machines;training;tumors 
efficient classifier training minimize false merges electron microscopy segmentation prospect neural reconstruction electron microscopy em images elucidated automatic segmentation algorithms segmentation algorithms eliminate necessity tracing neurons hand manual effort essential correcting mistakes considerable amount human labor required annotating groundtruth volumes training classifiers segmentation framework critically diminish dependence human interaction reconstruction system study proposes classifier training algorithm em segmentation aimed reduce amount manual effort demanded groundtruth annotation error refinement tasks exhaustive pixel level groundtruth active learning algorithm proposed sparse labeling pixel boundaries superpixels over-segmentation errors tolerable easier correct under-segmentation errors algorithm designed prioritize minimization false-merges false-split mistakes experiments data proposed method yields segmentation outputs amenable neural reconstruction existing methods electron microscopy;image reconstruction;image segmentation;medical image processing;em images;em segmentation;active learning algorithm;automatic segmentation algorithm;efficient classifier training;electron microscopy segmentation;error refinement tasks;false-merge minimization;false-split mistakes;groundtruth annotation;groundtruth volumes;neural reconstruction;over-segmentation errors;pixel level groundtruth;pixel sparse labeling;segmentation framework;superpixel boundary;under-segmentation errors;algorithm design analysis;image reconstruction;image segmentation;labeling;neurons;three-dimensional displays;training 
statistical analysis neuroimages imperfect registration variety studies neuroscience/neuroimaging seek perform statistical inference acquired brain image scans diagnosis understanding pathological manifestation diseases step register co-register image data common coordinate system permits meaningful comparison intensities voxel diseased versus healthy evaluate effects disease and/or machine learning algorithms subsequent step errors underlying registration problematic decrease statistical power follow-up inference tasks effective/accurate paper derive algorithm offers immunity local errors underlying deformation field registration procedures deriving deformation invariant representation image downstream analysis robust access hypothetical superior registration procedure algorithm based scattering coefficients starting harmonic analysis non-euclidean wavelets yields strategies designing deformation additive noise invariant representations -d brain image volumes set synthetic real brain images achieve robust statistical analysis presence substantial deformation errors standard analysis procedures under-perform fail identify true signal harmonic analysis;image registration;medical image processing;statistical analysis; brain image volumes;additive noise invariant representations;deformation errors;downstream analysis;harmonic analysis;imperfect registration;neuroimages;scattering coefficients;statistical analysis;statistical inference;algorithm design analysis;brain;diseases;robustness;scattering;wavelet transforms 
convex optimization abstract linear operators introduce convex optimization modeling framework transforms convex optimization expressed form natural convenient user equivalent cone program preserves fast linear transforms original representing linear functions transformation process matrices graphs encode composition abstract linear operators arrive matrix-free cone program data matrix represented abstract linear operator adjoint cone program solved matrix-free cone solver combining matrix-free modeling framework cone solver method efficiently solving convex optimization involving fast linear transforms computational complexity;convex programming;graph theory;matrix algebra;transforms;abstract linear operator;abstract linear operators;convex optimization modeling framework;convex optimization problem;convex optimization problems;data matrix;equivalent cone program;fast linear transforms;linear functions;matrix-free cone program;matrix-free cone solver;matrix-free modeling framework;transformation process;convex functions;convolution;matrix converters;signal processing algorithms;sparse matrices;standards;transforms 
hand pose estimation randomized decision forest segmentation paper propose real-time hand pose estimation algorithm randomized decision forest framework algorithm takes depth image input generates set skeletal joints output previous decision forest-based methods labels cloud stage vote joint locations contrast algorithm tracks set flexible virtual landmark named segmentation sips reaching final decision leaf node roughly speaking sip represents centroid subset skeletal joints located leaves branch expanded sip inspired latent regression forest-based hand pose estimation framework tang al integrate sip framework improvements: devise forest growing strategy decision randomized feature guided sips speed-up training procedure sips skeletal joints estimated non-leaf nodes experimental public benchmark datasets advantage proposed algorithm previous state-of-the-art methods algorithm runs fps normal cpu parallelism decision trees;feature extraction;image segmentation;palmprint recognition;pose estimation; hand pose estimation;sip;randomized decision forest;segmentation point;skeleton detection;indexes;joints;resource description framework;three-dimensional displays;training;vegetation 
accurate camera calibration robust defocus smartphone propose camera calibration method defocused images smartphone assumption defocus blur modeled convolution sharp image gaussian spread function psf contrast existing calibration approaches require well-focused images proposed method achieves accurate camera calibration severely defocused images robustness defocus proposed set unidirectional binary patterns simplifies gaussian deconvolution gaussian deconvolution multiple observations capturing set patterns consecutively displayed smartphone formulate feature extraction deconvolution estimate feature locations sub-pixel accuracy blur kernel location compensate error camera parameters refraction glass panel display device evaluate performance proposed method synthetic real data severe defocus method accurate camera calibration result gaussian processes;calibration;cameras;convolution;deconvolution;estimation theory;feature extraction;optical transfer function;smart phones;gaussian spread function;psf;binary pattern;camera calibration method;deconvolution problem;feature extraction;feature location estimation;image convolution;image defocus;smart phone;calibration;cameras;deconvolution;feature extraction;image edge detection;kernel;shape 
quality structure motion rolling shutter cameras practical reconstruction method high-quality dense depth map narrow-baseline image sequences captured commercial digital cameras dslrs mobile phones depth estimation motion gained photographic editing limitations form depth uncertainty narrow baseline rolling shutter address introduce reconstruction method narrow-baseline image sequences effectively handles effects rolling shutter occur commercial digital cameras additionally depth propagation method fill holes unknown pixels based geometric guidance model qualitative quantitative experimental algorithm consistently generates depth maps state-of-the-art method image reconstruction;image sequences; depth maps; reconstruction method;dslrs;dense depth map;depth estimation;depth propagation method;depth uncertainty;digital cameras;geometric guidance model;high quality structure;mobile phones;narrow-baseline image sequences;photographic editing;qualitative experimental result;quantitative experimental result;rolling shutter cameras;cameras;feature extraction;image sequences;interpolation;solid modeling;three-dimensional displays 
photogeometric scene flow high-detail dynamic reconstruction photometric stereo ps established technique high-detail reconstruction geometry appearance correct surface integration errors ps combined multiview stereo mvs dynamic objects ps reconstruction computing optical flow image alignment rapid illumination current ps methods typically compute optical flow mvs independent stages limitations errors introduced regularization contrast scene flow methods estimate geometry motion lack fine detail ps paper proposes photogeometric scene flow pgsf high-quality dynamic reconstruction pgsf performs ps mvs simultaneously based key observations: image alignment improves ps ps surfaces relit improve alignment ii ps surface gradients render smoothness term mvs unnecessary leading data-driven continuous depth estimates synergy demonstrated quality rgb appearance geometry motion computational geometry;image reconstruction;image resolution;image sequences;rendering computer graphics geometry;mvs;rgb appearance;geometry estimation;high-detail dynamic reconstruction;high-quality dynamic reconstruction;multiview stereo;optical flow;photogeometric scene flow;photometric stereo reconstruction;rendering;scene flow method;surface integration error;synergy;cameras;geometry;image color analysis;image reconstruction;lighting;surface reconstruction;three-dimensional displays 
blur-aware disparity estimation defocus stereo images defocus blur performance degradation establishing visual correspondence stereo images propose blur-aware disparity estimation method robust mismatch focus stereo images relative blur mismatch focus stereo images approximated difference square diameters blur kernels based defocus stereo model propose relative blur versus disparity rbd model characterizes relative blur second-order polynomial function disparity method alternates rbd model update disparity update iteration rbd model return refines disparity estimation updating matching cost aggregation weight compensate mismatch focus experiments synthesized real datasets demonstrate effectiveness proposed algorithm estimation theory;image matching;polynomials;stereo image processing;rbd model update;blur-aware disparity estimation;defocus blur;defocus stereo images;disparity update;focus mismatch;relative blur versus disparity model;second-order polynomial function disparity;visual correspondence;apertures;cameras;computational modeling;estimation;kernel;lenses;visualization 
global structure-from-motion similarity averaging global structure-from-motion sfm methods solve cameras simultaneously relative motions potential reconstruction accuracy computation efficiency incremental methods global sfm challenging reasons firstly translation averaging difficult essential matrix tells direction relative translation hard filter bad essential matrices feature matching failures propose compute sparse depth image camera solve depth images upgrade essential matrix similarity transformation determine scale relative translation camera registration formulated well-posed similarity averaging depth images filtering essential matrices simple effective translation averaging solved robustly convex optimization reach global optimum rapidly demonstrate method examples including sequential data internet data ambiguous data repetitive scene structures cameras;convex programming;feature extraction;image matching;image reconstruction;image registration;matrix algebra;sfm method;camera registration;computation efficiency;convex optimization problem;essential matrix;feature matching failure;global structure-from-motion;reconstruction accuracy;relative translation scale;similarity averaging;similarity transformation;sparse depth image;translation averaging;cameras;image edge detection;image reconstruction;internet;optimization;robustness;sparse matrices 
massively parallel multiview stereopsis surface normal diffusion massively parallel method high-quality multiview matching builds patchmatch idea: starting randomly generated planes scene space best-fitting planes iteratively propagated refined depth normal field view robust photo-consistency measure images maximized main novelties hand formulate patchmatch scene space aggregate image similarity multiple views accurate depth maps hand modified diffusion-like propagation scheme massively parallelized delivers dense multiview correspondence ten -megapixel images consumer-grade gpu method slanted support window fronto-parallel bias completely local parallel computation time scales linearly image size inversely proportional parallel threads low memory footprint values pixel independent depth range scales exceptionally handle multiple images depth resolution experiments dtu middlebury multiview datasets oblique aerial images method achieves competitive accuracy completeness range scenarios image matching;image reconstruction;image resolution;stereo image processing;visual perception; -megapixel images;dtu multiview datasets;middlebury multiview datasets;patchmatch idea;best-fitting planes;consumer-grade gpu;depth maps;high depth resolution;high-quality multiview matching;image similarity;massively parallel multiview stereopsis;modified diffusion-like propagation scheme;oblique aerial images;randomly generated planes;robust photo-consistency measure;surface normal diffusion;benchmark testing;image reconstruction;image resolution;robustness;standards;surface reconstruction;three-dimensional displays 
variational patchmatch multiview reconstruction refinement propose approach multi-view stereo reconstruction building proposed patchmatch stereo pm-huber algorithm introduce extension multi-view scenario employs iterative refinement scheme proposed approach extended robustified volumetric truncated signed distance function representation advantageous fusion refined depth maps raycasting current reconstruction estimation estimated depth normals arbitrary camera views formulate combined multi-view stereo reconstruction refinement variational optimization newly introduced plane based smoothing term energy formulation guided current reconstruction confidence image contents propose extension patchmatch scheme additional klt step avoid unnecessary sampling iterations improper camera poses corrected direct image aligment step performs robust outlier compensation proposed kernel lifting framework speed optimization variational formulation adapted scheme faster convergence image fusion;image reconstruction;iterative methods;optimisation;ray tracing;smoothing methods;stereo image processing;variational techniques;klt step;patchmatch stereo;convergence;depth map fusion;direct image alignment;energy formulation;iterative refinement scheme;kernel lifting framework;multiview stereo reconstruction;multiview stereo refinement;outlier compensation;plane based smoothing term;raycasting;variational optimization problem;volumetric truncated signed distance function representation;cameras;estimation;image reconstruction;kernel;optimization;robustness;visualization 
as-rigid-as-possible volumetric shape-from-template objective shape-from-template sft infer object's shape single image object template existing methods called thin-shell sft represent object outer surface surface thin objects piece paper closed surface thicker objects ball propose volumetric sft handles objects volumetric sft object's volume express deformation constraints reconstructs object's surface interior deformation challenging opaque objects outer surface visible image inspired mesh-editing techniques as-rigid-as-possible arap deformation model softly imposes local rigidity formalise arap isometric sft constrained variational optimisation solve iterative optimisation strategies initial solution based thin-shell sft volume propagation experiments synthetic real data method typical maximum relative error reconstructing deformation entire object including interior visual data image reconstruction;image representation;iterative methods;optimisation;solid modelling; object template;arap deformation model;as-rigid-as-possible deformation model;iterative optimisation;mesh-editing technique;object reconstruction;object representation;shape-from-template;thin-shell sft;deformable models;image reconstruction;optimization;shape;surface reconstruction;three-dimensional displays 
dynamic scene reconstruction multiple view video paper introduces approach dynamic scene reconstruction multiple moving cameras prior knowledge limiting constraints scene structure appearance illumination existing techniques dynamic scene reconstruction multiple wide-baseline camera views focus accurate reconstruction controlled environments cameras fixed calibrated background approaches robust dynamic scenes captured sparse moving cameras previous approaches outdoor dynamic scene reconstruction assume prior knowledge static background appearance structure primary contributions paper twofold: automatic method initial coarse dynamic scene segmentation reconstruction prior knowledge background appearance structure robust approach joint segmentation refinement dense reconstruction dynamic scenes multiple wide-baseline static moving cameras evaluation performed variety indoor outdoor scenes cluttered backgrounds multiple dynamic non-rigid objects people comparison state-of-the-art approaches demonstrates improved accuracy multiple view segmentation dense reconstruction proposed approach eliminates requirement prior knowledge scene structure appearance image motion analysis;image reconstruction;image segmentation;image sensors;lighting;video signal processing;coarse dynamic scene reconstruction;coarse dynamic scene segmentation;dynamic nonrigid objects;general dynamic scene reconstruction;multiple moving cameras;multiple view segmentation;multiple view video;multiple wide-baseline camera views;scene appearance;scene illumination;scene structure;sparse moving cameras;wide-baseline static cameras;cameras;image color analysis;image reconstruction;lighting;optical sensors;robustness;three-dimensional displays 
joint image handbook multiple perspective photographs correspondences form joint image\ effectively replica dimensional space distributed two-dimensional projections set characterized multilinear equations image coordinates epipolar trifocal constraints revisit paper geometric algebraic properties joint image address fundamental questions multilinearities and/or sufficient determine camera geometry and/or image correspondences theoretical paper answer questions setting intended serve handbook\ reference multilinearities practitioners algebra;cameras;geometry;image processing;photography;algebraic properties;camera geometry;epipolar constraints;geometric properties;image coordinates;image correspondences;joint image handbook;multilinear equations;multilinearities;perspective photographs;point correspondences;three dimensional space;trifocal constraints;two-dimensional projections;algebra;cameras;computer vision;conferences;geometry;manganese;tensile stress 
direct dense deformable: template-based non-rigid reconstruction rgb video paper tackle capturing dense detailed geometry generic complex non-rigid meshes single rgb-only commodity video camera direct approach robust real-time solutions exist observed scene static non-rigid dense shape capture current systems typically restricted complex multi-camera rigs advantage additional depth channel rgb-d cameras deal specific shapes planar surfaces contrast method single rgb video input capture deformations generic shapes depth estimation dense per-pixel direct compute dense template shape object short rigid sequence subsequently perform online reconstruction non-rigid mesh evolves time energy optimization approach minimizes robust photometric cost simultaneously estimates temporal correspondences deformations respect template mesh experimental evaluation range qualitative datasets compare existing method requires multi-frame optical flow perform quantitative evaluation template-based approaches ground truth dataset image reconstruction;image sequences;real-time systems;video signal processing; geometry;complex nonrigid meshes;depth estimation;direct approach;energy optimization approach;generic shapes;ground truth dataset;multiframe optical flow;planar surfaces;quantitative evaluation;real-time solutions;single rgb-only commodity video camera;template-based approaches;template-based nonrigid reconstruction;cameras;image reconstruction;robustness;shape;streaming media;surface reconstruction;three-dimensional displays 
single image pop-up discriminatively learned introduce approach estimating fine grained shape continuous pose object single image training set view exemplars learn select appearance-based discriminative mapped model facility location optimization training set models summarized set basis shapes generalize linear combination test image detect hypotheses main challenge select hypotheses compute pose shape coefficients time achieve optimize function considers simultaneously appearance matching geometric reprojection error apply alternating direction method multipliers admm minimize convex function main contribution simultaneous solution localization detailed geometry estimation maximizing appearance geometric compatibility convex relaxation computational geometry;computer vision;image matching;optimisation;pose estimation;solid modelling; geometry estimation; model; pose;admm;alternating direction method multipliers;appearance matching;appearance-based discriminative parts;convex function;convex relaxation;facility location optimization;fine grained shape;geometric compatibility;geometric reprojection error;linear combination;single image pop-up;detectors;geometry;shape;solid modeling;three-dimensional displays;training 
learning informative edge maps indoor scene layout prediction paper introduce edge-based features task recovering layout indoor scene single image indoor scenes edges informative spatial layout edges formed pairwise intersections walls wall ceiling wall floor contrast previous approaches rely area-based features geometric context orientation maps method attempts directly detect informative edges learn predict 'informative edge' probability maps methods exploit local global context respectively: structured edge detection forests convolutional network pixelwise labeling convolutional network successful predicting informative edges lack contrast occluded accuracy improved training network jointly predict edges geometric context features derived 'informative edge' maps learn maximum margin structured classifier achieves state-of-the-art performance layout prediction convolution;edge detection;learning artificial intelligence layout;area-based features;convolutional network;edge-based features;geometric context;global context;indoor scene layout prediction;informative edge maps learning;local context;maximum margin structured classifier;orientation maps;pixelwise labeling;structured edge detection forests;clutter;context;feature extraction;image edge detection;layout;three-dimensional displays;training 
multi-view convolutional neural networks shape recognition longstanding question computer vision concerns representation shapes recognition: shapes represented descriptors operating native formats voxel grid polygon mesh effectively represented view-based descriptors address question context learning recognize shapes collection rendered views images standard cnn architecture trained recognize shapes' rendered views independently shape recognized single view accuracy state-of-the-art shape descriptors recognition rates increase multiple views shapes provided addition cnn architecture combines multiple views shape single compact shape descriptor offering recognition performance architecture applied accurately recognize human hand-drawn sketches shapes conclude collection views highly informative shape recognition amenable emerging cnn architectures derivatives computer vision;convolution;image representation;learning artificial intelligence ;mesh generation;neural nets;rendering computer graphics ;shape recognition; images; shape descriptors; shape recognition; shape representation;computer vision;hand-drawn shape sketch recognition;learning;multiview convolutional neural networks;polygon mesh;shape rendered views;view-based descriptors;voxel grid;cameras;computer architecture;computer vision;image recognition;shape;solid modeling;three-dimensional displays 
learning analysis-by-synthesis pose estimation rgb-d images analysis-by-synthesis successful approach tasks computer vision pose estimation object rgb-d image topic idea compare observation output forward process rendered image object pose occlusion complicated sensor noise difficult perform comparison meaningful propose approach learns compare\ difficulties account describing posterior density object pose convolutional neural network cnn compares observed rendered images network trained maximum likelihood paradigm observe empirically cnn specialize geometry appearance specific objects objects vastly shapes appearances backgrounds compared state-of-the-art demonstrate improvement datasets total eleven objects cluttered background heavy occlusion computer vision;geometry;image colour analysis;learning artificial intelligence ;maximum likelihood estimation;neural nets;pose estimation;rendering computer graphics pose estimation;cnn;rgb-d images;analysis-by-synthesis;computer vision;convolutional neural network;geometry;learning;maximum likelihood paradigm;rendered image;computer vision;feature extraction;image segmentation;maximum likelihood estimation;probabilistic logic;three-dimensional displays 
surface profilometry phase shifting de bruijn pattern structured light method color surface profilometry proposed proposed method require color calibration camera-projector pair reconstruction dynamic static scenes method structured light pattern combination de bruijn color sequence sinusoidal fringe dynamic scenes hessian ridge detector gaussian mixture model combined extract stripe centers identify color stripes uniquely identified dynamic programming based smith-waterman algorithm de bruijn window property static scenes phase-shifting de bruijn window property combined accuracy reconstruction tested proposed method multiple objects challenging surfaces albedos demonstrate usability robustness method gaussian processes;hessian matrices;calibration;cameras;dynamic programming;image colour analysis;image reconstruction;image sequences;mixture models; surface profilometry;de bruijn color sequence;de bruijn pattern;de bruijn window property;gaussian mixture model;hessian ridge detector;smith-waterman algorithm;camera-projector pair;color surface profilometry;color calibration;dynamic programming;dynamic scenes;sinusoidal fringe;static scene phase-shifting;structured light method;calibration;decoding;dynamic programming;image color analysis;image reconstruction;surface reconstruction;three-dimensional displays 
deep visual correspondence embedding model stereo matching costs paper data-driven matching cost stereo matching deep visual correspondence embedding model trained convolutional neural network set stereo images ground truth disparities deep embedding model leverages appearance data learn visual similarity relationships image patches explicitly maps intensity values embedding feature space measure pixel dissimilarities experimental kitti middlebury data sets demonstrate effectiveness model prove measure pixel dissimilarity outperforms traditional matching costs integrated global stereo framework method ranks top two-frame algorithms kitti benchmark finally cross-validation model correct predictions unseen data labeled training set image matching;neural nets;stereo image processing;kitti benchmark;middlebury data sets;convolutional neural network;cross-validation results;data-driven matching cost;deep visual correspondence embedding model;embedding feature space;global stereo framework;ground truth disparities;image patches;labeled training set;pixel dissimilarity measurement;stereo images;stereo matching costs;visual similarity relationships;computational modeling;data models;feature extraction;machine learning;neural networks;training;visualization 
learning concept embeddings combined human-machine expertise paper snack low-dimensional concept embedding algorithm combines human expertise automatic machine similarity kernels complimentary: human insight capture relationships apparent object's visual similarity machine relieve human exhaustively constraints snack embeddings tasks: distinguishing prime nonprime mnist discovering labeling mistakes caltech ucsd birds cub dataset deep-learned features creating training datasets bird classifiers capturing subjective human taste dataset foods qualitatively exploring unstructured set pictographic characters comparisons state-of-the-art tasks snack produces concept embeddings require human supervision leading methods feature extraction;image classification;learning artificial intelligence ;cub dataset;caltech ucsd birds;mnist;snack embeddings;automatic machine similarity kernels;bird classifiers;deep-learned features;human insight;human-machine expertise;labeling mistakes discovery;low-dimensional concept embedding algorithm;nonprime numbers;object visual similarity;pictographic characters;supervised learning;birds;computer vision;crowdsourcing;kernel;labeling;machine learning;visualization 
deep multi-patch aggregation network image style aesthetics quality estimation paper investigates image style aesthetics quality estimation require fine-grained details high-resolution images utilizing deep neural network training approach existing deep convolutional neural networks extracted patch down-sized crop image training patch represent entire image ambiguity training propose deep multi-patch aggregation network training approach train models multiple patches generated image achieve constructing multiple shared columns neural network feeding multiple patches columns importantly propose network layers statistics sorting support aggregation patches proposed deep multi-patch aggregation network integrates shared feature learning aggregation function learning unified framework demonstrate effectiveness deep multi-patch aggregation network image style recognition aesthetic quality categorization image quality estimation models trained proposed networks outperformed art applications image recognition;image resolution;learning artificial intelligence ;neural nets;aesthetic quality categorization;aggregation function learning;deep convolutional neural networks;deep multipatch aggregation network;deep multipatch aggregation network training approach;deep neural network training approach;down-sized crop;feature learning;high-resolution images;image aesthetics;image quality estimation;image style recognition;estimation;feature extraction;image resolution;neural networks;object detection;sorting;training 
computational baby learning: weakly-supervised approach object detection intuitive observations baby inherently possess capability recognizing visual concept chair dog learning positive instances taught parent recognition capability gradually improved exploring and/or interacting real instances physical inspired observations propose computational model weakly-supervised object detection based prior knowledge modelling exemplar learning learning video contexts prior knowledge modeled pre-trained convolutional neural network cnn instances concept initial concept detector built exemplar learning deep features pre-trained cnn well-designed tracking solution discover diverse instances massive online weakly labeled videos positive instance detected/identified score video instances view-angles and/or distances tracked accumulated concept detector fine-tuned based instances process repeated till mature concept detector extensive experiments pascal voc- object detection datasets demonstrate effectiveness framework beat state-of-the-art full-training based performances learning samples object category weakly labeled videos feature extraction;image recognition;learning artificial intelligence ;neural nets;object detection;video signal processing;pascal voc- object detection datasets;computational baby learning;deep features;exemplar learning;initial concept detector;object category;online weakly labeled videos;pre-trained cnn;pre-trained convolutional neural network;prior knowledge modelling;recognition capability improvement;video contexts;visual concept recognition;weakly-supervised object detection;computational modeling;context;detectors;feature extraction;object detection;pediatrics;training 
improving image classification location context widespread availability cellphones cameras gps capabilities common images uploaded internet gps coordinates addition predict gps coordinates visual features door conditioned availability gps coordinates tackle performing image classification location context gps coordinates images train test phases explore encoding extracting features gps coordinates naturally incorporate features convolutional neural network cnn current state-of-the-art image classification recognition simultaneously learn optimal pooling radii subset features cnn framework evaluate model promote identify set location-sensitive concepts annotate subset yahoo flickr creative commons dataset gps coordinates concepts publicly leveraging location context achieve gain average precision global positioning system;feature extraction;image classification;image coding;mobile computing;neural nets;set theory;smart phones;cnn framework;gps capabilities;gps coordinates;internet;yahoo flickr creative commons dataset;cameras;cellphones;convolutional neural network;feature encoding;feature extraction;image classification;image recognition problems;location context;location-sensitive concepts;optimal pooling radii;visual features;context;global positioning system;image recognition;internet;snow;tagging;twitter 
hico: benchmark recognizing human-object interactions images introduce benchmark humans interacting common objects\ hico recognizing human-object interactions hoi demonstrate key features hico: diverse set interactions common object categories list well-defined sense-based hoi categories exhaustive labeling co-occurring interactions object category image perform in-depth analysis representative current approaches dnns enjoy edge addition semantic knowledge improve hoi recognition uncommon categories image recognition;dnns;hico;hoi recognition;humans interacting common objects;cooccurring interaction exhaustive labeling;human-object interaction recognition;images;sense-based hoi categories;benchmark testing;bicycles;image recognition;maintenance engineering;pipelines;semantics 
delving deep rectifiers: surpassing human-level performance imagenet classification rectified activation units rectifiers essential state-of-the-art neural networks study rectifier neural networks image classification aspects propose parametric rectified linear unit prelu generalizes traditional rectified unit prelu improves model fitting extra computational cost overfitting risk derive robust initialization method considers rectifier nonlinearities method enables train extremely deep rectified models directly scratch investigate deeper wider network architectures based learnable activation advanced initialization achieve top- test error imagenet classification dataset relative improvement ilsvrc winner googlenet knowledge result surpass reported human-level performance dataset image classification;neural nets;ilsvrc winner;imagenet classification dataset;imagenet classification;prelu;human-level performance;model fitting;network architectures;overfitting risk;parametric rectified linear unit;rectified activation units;rectifier neural networks;rectifier nonlinearities;robust initialization method;state-of-the-art neural networks;adaptation models;biological neural networks;computational modeling;gaussian distribution;testing;training 
continuous pose estimation spatial ensemble fisher regressors paper treat continuous pose estimation object categories regression basis training regression natural framework continuous regression methods achieved inferior respect d-based d-based classification-and-refinement approaches attributed weakness intra-class variability noisy matching procedures lack geometrical constraints propose apply regression fisher-encoded vectors computed cells learning array fisher regressors fisher encoding algorithm flexible variations class appearance array structure permits indirectly introduce spatial context approach formulate map inference likelihood function composed generative term based prediction error generated ensemble fisher regressors discriminative term based svm classifiers test algorithm publicly datasets envisage difficulties intra-class variability truncations occlusions motion blur obtaining state-of-the-art image classification;image matching;maximum likelihood estimation;pose estimation;prediction theory;regression analysis;support vector machines; training information; d-based classification-and-refinement approaches; d-based classification-and-refinement approaches;fisher encoding;fisher regressors;fisher-encoded vectors;map inference problem;svm classifiers;class appearance;continuous pose estimation;geometrical constraints;intra-class variability;likelihood function;noisy matching procedures;object categories;prediction error;regression problem;spatial context information;spatial ensemble;support vector machine;design automation;encoding;feature extraction;solid modeling;three-dimensional displays;training 
adaptive hashing fast similarity search staggering growth image video datasets algorithms provide fast similarity search compact storage crucial hashing methods map data hamming space promise methods employ batch-learning strategy computational cost memory requirements intractable infeasible larger larger datasets overcome challenges propose online learning algorithm based stochastic gradient descent hash functions updated iteratively streaming data experiments image retrieval benchmarks online algorithm attains retrieval accuracy comparable competing state-of-the-art batch-learning solutions formulation magnitude faster online adaptable variations data formulation yields improved retrieval performance reported online hashing technique online kernel hashing gradient methods;image retrieval;learning artificial intelligence ;stochastic processes;hamming space;adaptive hashing;batch-learning solution;batch-learning strategy;compact storage;computational cost;fast similarity search;hash function;hashing method;image dataset;image retrieval benchmark;memory requirement;online algorithm;online hashing technique;online kernel hashing;online learning algorithm;retrieval accuracy;retrieval performance;staggering growth;stochastic gradient descent;streaming data;video dataset;benchmark testing;binary codes;computational efficiency;computer vision;kernel;search problems;streaming media 
single image single image labels learn predict paper learn mapping appearance properties single explicit label explicit supervision regularity indoor scenes learn mapping completely unsupervised manner demonstrate standard scene understanding dataset internet images unavailable precluding supervised learning label method produces competitive image processing;learning artificial intelligence properties;internet images;indoor scenes;single image d;standard scene;supervised learning;detectors;dictionaries;face;solid modeling;standards;three-dimensional displays;training 
cross-domain image retrieval dual attribute-aware ranking network address cross-domain image retrieval practical application: user photo depicting clothing image goal retrieve attribute-similar clothing items online shopping stores challenging discrepancy online shopping images ideal lighting/pose/background conditions user photos captured uncontrolled conditions address propose dual attribute-aware ranking network darn retrieval feature learning darn consists sub-networks domain retrieval feature representations driven semantic attribute learning attribute-guided learning key factor retrieval accuracy improvement addition align nature retrieval impose triplet visual similarity constraint learning rank subnetworks contribution large-scale dataset network learning feasible exploit customer review websites crawl set online shopping images offline user photos fine-grained clothing attributes online shopping images exact offline counterpart images online images collected real-world consumer websites reflecting diversity data modality dataset unique rare academic community extensively evaluate retrieval performance networks configurations top- retrieval accuracy doubled proposed darn current popular solution pre-trained cnn features web sites;clothing;consumer behaviour;feature extraction;image representation;image retrieval;learning artificial intelligence ;neural nets;retail data processing;darn;attribute-guided learning;attribute-similar clothing items;background conditions;clothing image;cross-domain image retrieval;customer review web sites;data modality;dual attribute-aware ranking network;fine-grained clothing attributes;lighting conditions;network learning;online shopping images;online shopping stores;pose conditions;pretrained cnn features;real-world consumer web sites;retrieval feature learning;semantic attribute learning;cameras;clothing;image color analysis;image retrieval;semantics;shape;visualization 
attribute-graph: graph based approach image ranking propose image representation termed attribute-graph rank images semantic similarity query image attribute-graph undirected connected graph incorporating local global image characteristics graph nodes characterise objects scene context mid-level semantic attributes edges capture object topology demonstrate effectiveness attribute-graphs applying image ranking benchmark performance algorithm 'rpascal' 'rimagenet' datasets created evaluate ranking performance complex queries multiple objects experimental evaluation modelling images attribute-graphs improved ranking performance existing techniques graph theory;image representation;image retrieval;attribute-graph;graph based approach;image ranking;image representation;query image;undirected connected graph;context;feature extraction;image edge detection;image representation;image retrieval;proposals;semantics 
contextual action recognition r*cnn multiple cues image reveal action person performing jogger pose characteristic jogging scene road trail presence joggers additional source exploit simple observation actions accompanied contextual cues build strong action recognition system adapt rcnn region classification maintaining ability localize action call system r*cnn action-specific models feature maps trained jointly allowing action specific representations emerge r*cnn achieves ap pasal voc action dataset outperforming approaches field margin r*cnn limited action recognition r*cnn tackle fine-grained tasks attribute classification validate claim reporting state-of-the-art performance berkeley attributes people dataset image classification;image motion analysis;neural nets;object recognition;berkeley attributes people dataset;pasal voc action dataset;r*cnn;action specific representations;action-specific models;attribute classification;contextual action recognition;feature maps;computer architecture;computer vision;feature extraction;image recognition;object detection;proposals;training 
object memorable studies image memorability light distinguishes memorability images intrinsic extrinsic properties images memorable understanding memorability specific objects inside image remains elusive paper provide attempt answer question: remembered image augment images object segmentations pascal-s dataset ground truth memorability scores light factors properties object memorable forgettable humans analyze visual factors influence object memorability color visual saliency object categories study correlation object image memorability image memorability greatly memorability memorable object lastly explore effectiveness deep learning computational approaches predicting object memorability images efforts offer deeper understanding memorability avenues wide variety applications image colour analysis;image segmentation;pascal-s dataset;color;ground truth memorability scores;image memorability;image segmentation;object categories;object memorability;object segmentation;visual factors;visual saliency;computer vision;correlation;games;image color analysis;image segmentation;presses;visualization 
knn hashing factorized neighborhood representation hashing effective tasks reducing processing time compressing massive databases lots approaches developed learn data-dependent hash functions learn hash functions yield performance acceptable computational memory cost challenging based observation retrieval precision highly knn classification accuracy paper proposes knn-based supervised hashing method learns hash functions directly maximizing knn accuracy hamming-embedded training data scalable propose factorized neighborhood representation parsimoniously model neighborhood relationships inherent training data real-world data linearly inseparable kernelize basic model improve performance result proposed method learn accurate hashing functions tolerable computation storage cost experiments benchmarks demonstrate method outperforms state-of-the-arts embedded systems;file organisation;pattern classification;data-dependent hash function;factorized neighborhood representation;hamming-embedded training data;knn classification accuracy;knn-based supervised hashing method;storage cost;binary codes;computational modeling;data models;linear programming;stochastic processes;training;training data 
multi-view complementary hash tables nearest neighbor search witnessed success hashing techniques fast nearest neighbor search practice applications visual search object detection image matching enjoyed benefits complementary hash tables fusion multiple views prior focused compact hash code cleaning rare studies build multiple complementary hash tables adaptively integrate stemming multiple views paper multi-view complementary hash table method learns complementarity hash tables data multiple views single multi-view table exemplar based feature fusion approximate inherent data similarities low-rank matrix learn discriminative hash functions efficient build complementary tables maintain scalable training fast out-of-sample extension exemplar reweighting scheme introduced update induced low-rank similarity sequential table construction framework brings mutual benefits tables placing exemplars shared mis-separated neighbors extensive experiments large-scale image datasets demonstrate proposed method outperforms naive solutions state-of-the-art multi-table methods data;feature extraction;file organisation;image fusion;image retrieval;matrix algebra;search problems;compact hash code cleaning;data similarities;discriminative hash functions;exemplar based feature fusion;exemplar reweighting scheme;hashing techniques;image matching;information fusion;information stemming;large-scale image datasets;low-rank matrix;multiview complementary hash table method;nearest neighbor search;object detection;visual search;binary codes;eigenvalues eigenfunctions;image matching;matrix decomposition;nearest neighbor searches;training;visualization 
scalable person re-identification: benchmark paper contributes quality dataset person re-identification named market- current datasets: limited scale consist hand-drawn bboxes unavailable realistic settings ground truth query image identity close environment tackle proposed market- dataset featured aspects annotated bboxes distractor set images largest person re-id dataset images market- dataset produced deformable model dpm pedestrian detector dataset collected system identity multiple images camera minor contribution inspired advances large-scale image search paper proposes unsupervised bag-of-words descriptor view person re-identification special task image search experiment proposed descriptor yields competitive accuracy viper cuhk market- datasets scalable large-scale dataset feature extraction;image matching;image retrieval;open systems;pedestrians;dpm;market- dataset;camera;deformable model;ground truth;hand-drawn bbox;large-scale image search;open system;pedestrian detector;query im- age;scalable person reidentification;unsupervised bag-of-words descriptor;benchmark testing;boosting;cameras;detectors;measurement;open systems;visualization 
mmss: multi-modal sharable specific feature learning rgb-d object recognition feature-learning methods rgb-d object recognition learn features color depth modalities separately simply treat rgb-d undifferentiated four-channel data adequately exploit relationship modalities motivated intuition modalities modal-specific patterns shared common patterns propose multi-modal feature learning framework rgb-d object recognition construct deep cnn layers color depth separately connect carefully designed multi-modal layers fuse color depth enforcing common shared features modalities features reflecting shared properties modal-specific properties modalities multi-modal learning frameworks back-propagated cnn layers experimental proposed multi-modal feature learning method outperforms state-of-the-art approaches rgb-d object benchmark datasets backpropagation;image colour analysis;object recognition;mmss;rgb-d object recognition;backpropagation;color modalities;convolutional neural network;deep cnn layers;depth modalities;modal-specific patterns;modal-specific properties;multimodal sharable specific feature learning;computer vision;feature extraction;image color analysis;labeling;learning systems;object recognition;sparse matrices 
object detection multi-region semantic segmentation-aware cnn model propose object detection system relies multi-region deep convolutional neural network cnn encodes semantic segmentation-aware features cnn-based representation aims capturing diverse set discriminative appearance factors exhibits localization sensitivity essential accurate object localization exploit properties recognition module integrating iterative localization mechanism alternates scoring box proposal refining location deep cnn regression model efficient modules detect objects localization accuracy detection challenges pascal voc pascal voc achieve map correspondingly surpassing published margin image segmentation;neural nets;object detection;object recognition;regression analysis;cnn-based representation;pascal voc ;pascal voc ;accurate object localization;box proposal;deep cnn regression model;discriminative appearance factors;high localization accuracy;iterative localization mechanism;localization sensitivity;multiregion deep convolutional neural network;object detection system;recognition module;semantic segmentation-aware cnn model;semantic segmentation-aware features;biological system modeling;computer architecture;context;feature extraction;object detection;semantics;shape 
neural activation constellations: unsupervised model discovery convolutional networks models object categories essential challenging recognition tasks differences categories subtle reflected appearances object approach learn models completely unsupervised manner annotations bounding boxes learning key idea constellations neural activation patterns computed convolutional neural networks experiments outperform existing approaches fine-grained recognition cub oxford pets oxford flowers dataset bounding box annotations achieve state-of-the-art performance stanford dog dataset benefits neural constellation models data augmentation technique fine-tuning paper unites generic fine-grained classification approach suitable scenarios learning artificial intelligence ;neural nets;object recognition;cub ;oxford flowers dataset;oxford pets;stanford dog dataset;convolutional networks;data augmentation technique;fine-grained recognition;neural activation constellations;neural activation patterns;unsupervised model discovery;birds;computational modeling;deformable models;detectors;feature extraction;proposals;training 
cascaded sparse spatial bins efficient effective generic object detection efficient method extraction object proposals introduced objectness\ function exploits deep spatial pyramid features fast-to-compute hog-based edge statistic edgeboxes score efficiency achieved spatial bins combination sparsity-inducing normalized svm state-of-the-art recall performance achieved pascal voc outperforming methods comparable speed interestingly proposals image considered method attains recall voc method improves map rcnn class-specific detector increasing proposals image system trained twenty classes performs class ilsvrc set confirming generalization capability edge detection;feature extraction;object detection;recurrent neural nets;support vector machines;edgeboxes score;ilsvrc set;pascal voc ;rcnn state-of-the-art class-specific detector;cascaded sparse spatial bins;deep spatial pyramid features;fast-to-compute hog-based edge statistics;generalization capability;generic object detection;map improvement;object proposal extraction;objectness function;sparsity-inducing normalized svm;state-of-the-art recall performance;detectors;feature extraction;image edge detection;image segmentation;pipelines;proposals;support vector machines 
probabilistic label relation graphs ising models classification label space structure common hierarchical label spaces label subsumes animal subsumes dog labels mutually exclusive dog cat unrelated furry carnivore jointly model hierarchy exclusion relations notion hex hierarchy exclusion graph introduced combined conditional random field crf deep neural network dnn art applied visual object classification training labels drawn levels imagenet hierarchy image labeled basic level category dog\ specific label husky\ paper extend hex model soft probabilistic relations labels uncertainty relationship labels antelope sort of\ furry degree grizzly bear call model phex probabilistic hex phex graph converted ising model existing off-the-shelf inference methods contrast hex method specialized inference algorithms experimental improvements large-scale visual object classification tasks outperforming previous hex model ising model;graph theory;image classification;inference mechanisms;hex model;imagenet hierarchy;ising model;conditional random field;deep neural network;hierarchical label spaces;hierarchy exclusion graph;inference methods;phex graph;probabilistic hex;probabilistic label relation graphs;visual object classification problems;animals;computational modeling;law;mathematical model;probabilistic logic;standards 
predicting features image geo-localization per-bundle vlad address recognizing depicted query image database geo-tagged images city-scale discover features recognizing data-driven manner knowledge predict features query image prior geo-localization process achieve performance reducing features learning predict features retrieving geo-tagged images database propose per-bundle vector locally aggregated descriptors pbvlad maximally stable region vector locally aggregated descriptors vlad multiple scale-invariant features detected region experimental proposed approach achieves improvement baseline methods feature extraction;geography;image retrieval;learning artificial intelligence ;vectors;geotagged image retrieval;image geolocalization;learning;per-bundle vlad;per-bundle vector locally aggregated descriptors;place recognition;query image;scale-invariant feature detection;feature extraction;image retrieval;robustness;support vector machines;training;visualization 
task-driven feature pooling image classification feature pooling strategy achieve performance image classification pooling methods unsupervised heuristic paper propose task-driven pooling tdp model directly learn pooled representation data discriminative manner traditional methods average max pooling tdp implicit pooling method elegantly integrates learning representations classification task optimization tdp equalize similarities descriptors learned representation maximize classification accuracy tdp combined traditional bow models coding vectors state-of-the-art cnn models feature maps achieve pooled representation self-training mechanism generate tdp representation test image multi-task extension tdp proposed improve performance experiments databases flower- indoor- caltech- validate effectiveness models feature extraction;image classification;image representation;learning artificial intelligence ;visual databases;bow model;cnn model;caltech- database;flower- database;indoor- database;tdp model;coding vector;feature map;image classification;implicit pooling method;pooled representation learning;self-training mechanism;task-driven feature pooling;test image;encoding;feature extraction;image coding;image color analysis;optimization;tensile stress;training 
cutting edge: soft correspondences multimodal scene parsing exploiting multiple modalities semantic scene parsing improve accuracy single modality scenario existing methods assume regions modalities label paper address data misalignment label inconsistencies moving objects semantic labeling violate assumption existing techniques formulate multimodal semantic labeling inference crf introduce latent nodes explicitly model inconsistencies domains latent nodes leverage domains improve labeling cut edges inconsistent regions eliminate hand tuning parameters model propose learn intra-domain inter-domain potential functions training data demonstrate benefits approach publicly datasets imagery clouds latent nodes learning strategy method outperforms state-of-the-art image processing; imagery; clouds;crf;conditional random field;data misalignment;inference;inter-domain potential functions;intra-domain potential functions;label inconsistencies;latent nodes;multimodal scene parsing;multimodal semantic labeling;feature extraction;image analysis;labeling;laser radar;semantics;sensors;three-dimensional displays 
shot learning compositions meaningful patches task discriminating object trivial human task computationally taxing modern machine learning methods perform task ease examples learning proposed quick grasp concept shared knowledge examples learned key one-shot learning sharing common holds immense amounts visual concept constructed propose unsupervised method learning compact dictionary image patches representing meaningful components objects patches features build compositional model outperforms popular algorithms one-shot learning task demonstrate effectiveness approach hand-written digits model generalizes multiple datasets data analysis;image processing;learning artificial intelligence ;compact dictionary;hand-written digits;image patches;machine learning methods;multiple datasets;one-shot learning task;unsupervised method;visual concept;dictionaries;feature extraction;image recognition;image reconstruction;skeleton;training;visualization 
fastext: efficient unconstrained scene text detector propose easy-to-implement stroke detector based efficient pixel intensity comparison surrounding pixels stroke-specific keypoints efficiently detected text fragments subsequently extracted local thresholding guided keypoint properties classification based effectively calculated features eliminates non-text regions stroke-specific keypoints produce times region segmentations detect characters commonly exploited mser detector process times faster efficient classification step regions reduced times standard method times faster stages proposed pipeline scale-and rotation-invariant support wide variety scripts latin hebrew chinese fonts proposed detector plugged scene text localization recognition pipeline state-of-the-art text localization accuracy maintained whilst processing time reduced character recognition;feature extraction;image classification;image segmentation;fastext;classification;local thresholding;pixel intensity comparison;recognition pipeline;scene text localization;stroke detector;stroke-specific keypoint detection;text fragment extraction;text localization accuracy;unconstrained scene text detector;detectors;image edge detection;lead;pipelines;robustness;standards;text recognition 
multi-scale recognition dag-cnns explore multi-scale convolutional neural nets cnns image classification contemporary approaches extract features single output layer extracting features multiple layers simultaneously reason mid low-level features classification multi-scale architecture feed-forward model structured directed acyclic graph dag-cnns dag-cnns learn set multi-scale features effectively shared coarse fine-grained classification tasks fine-tuning models helps performance off-the-self\ multi-scale features perform extensive analysis demonstrate state-of-the-art classification performance standard scene benchmarks sun mit scene terms heavily benchmarked mit scene datasets reduce lowest previously-reported error directed graphs;feature extraction;image recognition;neural nets;cnn;dag-cnn;directed acyclic graph;feature extraction;feed-forward model;fine grained classification;image classification;multiscale architecture;multiscale convolutional neural nets;multiscale recognition;benchmark testing;computational modeling;computer architecture;feature extraction;image recognition;neural networks;training 
relaxed multiple-instance svm application object discovery multiple-instance learning mil served tool wide range vision applications instance image classification object detection visual tracking paper propose method solve classical mil named relaxed multiple-instance svm rmi-svm treat positiveness instance continuous variable noisy-or model enforce mil constraints optimize jointly unified framework optimization efficiently solved stochastic gradient decent extensive experiments demonstrate rmi-svm consistently achieves superior performance benchmarks mil simply applied rmi-svm challenging vision task common object discovery state-of-the arts object discovery pascal voc datasets confirm advantages proposed method computer vision;gradient methods;learning artificial intelligence ;object detection;optimisation;stochastic processes;support vector machines;mil;pascal voc datasets;rmi-svm;common object discovery;multiple-instance learning;noisy-or model;optimization problem;relaxed multiple-instance svm;stochastic gradient decent;vision applications;image edge detection;noise measurement;object detection;optimization;proposals;support vector machines;visualization 
calories: automated mobile vision food diary system recognize contents meal single image predict nutritional contents calories simplest version assumes user eating restaurant menu collect images offline train multi-label classifier time apply classifier running phone predict foods meal lookup nutritional apply method dataset images restaurants cnn-based classifier outperforming previous challenging setting restaurants estimate size foods labels requires solving segmentation depth volume estimation single image cnn-based approaches promising preliminary computer vision;image classification;image segmentation;medical computing;mobile computing;neural nets;personal computing;prediction theory;cnn-based classifier;im calories;automated mobile vision food diary;content recognition;depth estimation;food prediction;food size estimation;multilabel classifier;nutritional content prediction;nutritional facts;restaurant;volume estimation;cameras;image recognition;image segmentation;machine learning;mobile communication;visualization 
lewis: latent embeddings word images semantics goal bring semantics tasks text recognition retrieval natural images text recognition retrieval received lot attention previous focused recognizing retrieving word query semantics consideration paper question: predict semantic concepts directly word image explicitly transcribe word image characters goal propose convolutional neural network cnn weighted ranking loss objective ensures concepts relevant query image ranked ahead relevant interpreted learning euclidean space word images concepts jointly embedded model learned end-to-end manner image pixels semantic concepts dataset synthetically generated word images concepts mined lexical database wordnet complexity task word images concepts degree accuracy computational complexity;image retrieval;natural scenes;neural nets;text detection;cnn;euclidean space;lewis;wordnet;convolutional neural network;image pixel;latent embeddings word images semantics;lexical database;natural image;query image;semantic concept;task complexity;text recognition;text retrieval;weighted ranking loss objective;computer vision;databases;image recognition;image representation;neural networks;semantics;text recognition 
per-sample kernel adaptation visual recognition object action scene representations corrupted noise impair performance visual recognition typically partial occlusion clutter excessive articulation subset feature dimensions importantly dimensions corrupted samples common approach feature selection kernel methods down-weight eliminate entire training samples dimensions samples valuable signal lost suboptimal classification goal adjust contribution individual feature dimensions comparing samples computing similarity per-sample selection informative dimensions directly integrated kernel computation interrelated learning parameters kernel classifier determining informative components sample addressed joint objective function approach integrated learning stage kernel-based visual recognition affect computational performance retrieval phase experiments diverse challenges action recognition videos indoor scene classification applicability approach ability improve learning visual representations feature selection;gesture recognition;image classification;image representation;learning artificial intelligence ;object recognition;action recognition;action representation;clutter;computational performance;feature dimension;feature selection;indoor scene classification;informative dimension;joint objective function;kernel classifier;kernel computation;kernel method;kernel-based visual recognition problem;learning stage;object representation;partial occlusion;per-sample kernel adaptation;per-sample selection;retrieval phase;scene representation;suboptimal classification;visual grouping;visual representation;image recognition;kernel;noise measurement;reliability;support vector machines;training;visualization 
fine-grained change detection misaligned scenes varied illuminations detecting fine-grained subtle scene critically practice previous change detection methods focusing detecting large-scale paper proposes feasible end-to-end approach challenging start active camera relocation relocates camera pose position time observation guarantee detection sensitivity accuracy minute observation capture images multiple illuminations roughly aligned time lighting conditions times observations formulate fine-grained change detection joint optimization factors normal-aware lighting difference camera geometry correction flow real scene change mask solve factors coarse-to-fine manner achieve reliable change decision rank minimization build real-world datasets benchmark fine-grained change detection misaligned scenes varied multiple lighting conditions extensive experiments superior performance approach state-of-the-art change detection methods ability distinguish real scene false caused lighting variations computational geometry;image sensors;lighting;object detection;optimisation;active camera relocation;camera geometry correction flow;detection sensitivity;fine-grained subtle change detection;joint optimization problem;misaligned scenes;normal-aware lighting difference;real scene change mask;varied illuminations;varied multiple lighting conditions;cameras;dsl;geometry;image color analysis;lighting;reliability;three-dimensional displays 
aggregating local deep features image retrieval image descriptors produced deep convolutional neural networks provide state-of-the-art performance image classification retrieval activations convolutional layers interpreted local features describing image regions local features aggregated aggregating methods developed local features fisher vectors providing powerful global descriptor paper investigate aggregate local deep features produce compact descriptors image retrieval deep features traditional hand-engineered features distributions pairwise similarities existing aggregation methods carefully re-evaluated re-evaluation reveals contrast shallow features simple aggregation method based sum pooling performance deep convolutional features method efficient parameters bears risk overfitting learning pca matrix addition simple efficient query expansion scheme suitable proposed aggregation method compact global descriptor improves state-of-the-art common benchmarks considerably image classification;image retrieval;matrix algebra;neural nets;principal component analysis;vectors;fisher vector;pca matrix;aggregating method;aggregation method;compact descriptor;compact global descriptor;convolutional layer;deep convolutional feature;deep convolutional neural network;hand-engineered feature;image classification;image descriptor;image region;image retrieval problem;local deep feature;local feature;pairwise similarity;query expansion scheme;shallow feature;sum pooling;aggregates;buildings;feature extraction;image retrieval;neural networks;principal component analysis;reliability 
learning deep object detectors models crowdsourced cad models easily accessible online generate infinite training images object category augmenting training data contemporary deep convolutional neural net dcnn models synthetic data effective real training data limited matched target domain freely cad models capture shape missing low level cues realistic object texture pose background detailed analysis synthetic cad images probe ability dcnn learn cues surprising findings dcnn fine-tuned target detection task exhibits degree invariance missing low-level cues pretrained generic imagenet classification learns low-level cues simulated synthetic dcnn training approach outperforms previous methods benchmark pascal voc dataset learning few-shot scenario improves performance domain shift scenario office benchmark cad;convolution;learning artificial intelligence ;neural nets;object detection;solid modelling; cad model;cad image;dcnn model;deep convolutional neural net;deep object detector learning;target detection;data models;design automation;detectors;image color analysis;solid modeling;three-dimensional displays;training 
harvesting discriminative meta objects deep cnn features scene classification scene classification generic cnn features rudimentary manner paper pipeline built deep cnn features harvest discriminative visual objects scene classification region proposal technique generate set high-quality patches objects apply pre-trained cnn extract generic deep features patches perform unsupervised weakly supervised learning screen patches discover discriminative representing category-specific objects apply discriminative clustering enhanced local cnn fine-tuning aggregate objects called meta objects scene image representation constructed pooling feature response maps learned meta objects multiple spatial scales confirmed scene image representation pipeline capable delivering state-of-the-art performance popular scene benchmark datasets mit indoor sun convolution;feature extraction;image classification;image representation;neural nets;unsupervised learning;convolutional neural network;deep cnn;feature extraction;feature response map;metaobject learning;scene classification;scene image representation;unsupervised learning;aggregates;computers;feature extraction;pipelines;proposals;supervised learning;visualization 
scalable nonlinear embeddings semantic category-based image retrieval propose algorithm task supervised discriminative distance learning nonlinearly embedding vectors low dimensional euclidean space challenging setting supervision constraints dissimilar pairs training proposed method derived approximate kernelization linear mahalanobis-like distance metric learning algorithm kernel neural network model parameters test time evaluation complexity proposed method dd dimensionality input features dimension projection space -- contrast usual kernelization methods complexity scale linearly training examples propose stochastic gradient based learning algorithm method scalable training examples nonlinear train method half training pairs dimensional cnn features empirical comparisons relevant baselines challenging datasets task low dimensional semantic category based image retrieval computational complexity;computational geometry;computer vision;distance learning;gradient methods;image retrieval;learning artificial intelligence ;neural nets;stochastic processes;kernel neural network;linear mahalanobis-like distance metric learning algorithm;low dimensional euclidean space;nonlinearly embedding vectors;scalable nonlinear embeddings;semantic category-based image retrieval;stochastic gradient based learning algorithm;supervised discriminative distance learning;approximation algorithms;complexity theory;computer vision;kernel;measurement;neural networks;training 
person re-identification ranking optimisation discriminant context analysis person re-identification challenging computer vision existing re-identification approaches focus optimal methods features matching metric learning approaches study inter-camera transformations features methods pay attention visual ambiguities shared ranks paper focus introduce unsupervised ranking optimization approach based discriminant context analysis proposed approach refines initial ranking removing visual ambiguities common ranks achieved analyzing content context extensive experiments publicly benchmark datasets baseline methods conducted demonstrate remarkable improvement positions ranking selected dataset state-of-the-art methods outperformed method computer vision;image matching;learning artificial intelligence ;optimisation;computer vision;discriminant context analysis;person reidentification ranking optimisation;unsupervised ranking optimization approach;context;feature extraction;information analysis;measurement;probes;training;visualization 
unsupervised generation view annotated car dataset videos object recognition approaches extended yield object class output viewpoint pose training approaches typically requires additional viewpoint keypoint annotation training data alternatively synthetic cad models paper approach creates dataset images annotated bounding boxes viewpoint labels automated manner videos assume scene static reconstruct surfaces structure motion automatically detect reconstruction fails normalize viewpoint models aligning reconstructed clouds exemplarily cars expand dataset annotated single images improved performance training viewpoint regressor joined dataset automobiles;computer graphics;image classification;image motion analysis;image reconstruction;video signal processing; surface reconstruction;annotated single images;bounding boxes;point clouds;structure motion;videos;viewpoint annotated car dataset unsupervised generation;viewpoint labels;viewpoint regressor;cameras;histograms;image reconstruction;solid modeling;three-dimensional displays;training;videos 
structured indoor modeling paper modeling framework reconstructs indoor scene structured model panorama rgbd images scene geometry represented graph nodes correspond structural elements walls objects approach devises structure grammar defines scene graph manipulated grammar drives principled reconstruction algorithm grammar rules sequentially applied recover structured model paper proposes segmentation algorithm offset-map reconstruction algorithm framework enforce architectural shape priors existing state-of-the-art structured scene representation enables variety applications ranging indoor scene visualization automated floorplan generation inverse-cad tested framework algorithms synthetic real datasets qualitative quantitative evaluations source code data project website geometry;graph theory;image colour analysis;image reconstruction;image segmentation;modelling; modelling framework;rgb-d image;indoor scene reconstruction;offset-map reconstruction algorithm;room segmentation algorithm;scene geometry;structured graph;computational modeling;geometry;grammar;image reconstruction;reconstruction algorithms;solid modeling;three-dimensional displays 
time-lapse reconstruction internet photos internet photo collection landmark compute time-lapse video sequence virtual camera moves continuously time space previous assumed static camera addition camera motion time-lapse creates compelling impression parallax achieving goal requires addressing multiple technical challenges including solving time-varying depth maps regularizing color profiles time reconstructing quality hole-free images frame projected profiles photorealistic time-lapses skylines natural scenes dramatic parallax effects image reconstruction;image sequences;video signal processing; color profiles; time-lapse reconstruction;internet photo collection;camera motion;multiple technical challenges;natural scenes;parallax effects;photorealistic time-lapses;skylines;static camera;time-varying depth maps;video sequence;cameras;geometry;image color analysis;image reconstruction;internet;three-dimensional displays;video sequences 
global dense multiscale reconstruction billion variational approach surface reconstruction set oriented scale focus scenarios non-uniform densities images distances contrast previous methods integrate scale objective globally optimize signed distance function surface balanced octree grid finite element discretization dual structure octree minimizing variables tetrahedral mesh generated efficiently dual structure memory efficiency optimized robust data terms scenes surface normals explicitly optimized surface extraction improve reconstruction edges corners computational geometry;finite element analysis;image reconstruction;octrees;surface reconstruction;balanced octree grid;dense multiscale reconstruction;dual structure;finite element discretization;global multiscale reconstruction;nonuniform density;scale information;signed distance function;surface extraction;surface reconstruction;tetrahedral mesh;cost function;image reconstruction;octrees;robustness;surface reconstruction;three-dimensional displays 
visibility clouds determine visible subset directly cloud interestingly occlude task performed surface reconstruction normal estimation operator simple transforms domain constructs convex hull domain lie convex hull transformed set images visible operator numerous applications computer vision including reconstruction keypoint detection finding viewpoints reduction current paper addresses fundamental question: properties transformation function satisfy utilized operator properties sufficient: sign function monotonicity condition function's parameter correctness algorithm satisfies properties proved finally application operator assignment visibility-confidence score feature missing previous approaches binary yes/no visibility determined score utilized applications illustrate view-dependent curvature estimation computer vision;convex programming;estimation theory;face recognition;image reconstruction;object detection;computer vision;convex hull;keypoint detection;point clouds;surface reconstruction;transformation function;view-dependent curvature estimation;visibility-confidence score;computer vision;estimation;face;image reconstruction;kernel;surface reconstruction;three-dimensional displays 
weakly supervised graph based semantic segmentation learning communities image-parts weakly-supervised approach semantic segmentation goal assign pixel-level labels partial image-level labels application scenarios difficult accurate segmentation feasible detailed annotations proposed approach starts initial coarse segmentation spectral clustering approach image communities community-driven graph constructed captures spatial feature relationships communities label graph captures correlations image labels finally mapping image level labels communities formulated convex optimization proposed approach require location image level labels trained partially labeled datasets compared state-of-the-art weakly supervised approaches achieve performance improvement msrc- dataset labelme dataset times faster convex programming;graph theory;image segmentation;pattern clustering;community-driven graph;convex optimization problem;image-level label;image-parts;label graph;pixel-level label;semantic segmentation;spectral clustering;weakly supervised graph;correlation;databases;image segmentation;partitioning algorithms;semantics;training;visualization 
piecewise flat embedding image segmentation image segmentation critical step computer vision tasks including high-level visual recognition scene understanding low-level photo video processing paper propose nonlinear embedding called piecewise flat embedding image segmentation based theory sparse signal recovery piecewise flat embedding attempts identify segment boundaries suppressing variations segments adopt -regularized energy term formulation promote sparse solutions devise effective two-stage numerical algorithm based bregman iterations solve proposed embedding piecewise flat embedding easily integrated existing image segmentation frameworks including segmentation based spectral clustering hierarchical segmentation based contour detection experiments bsds segmentation algorithms incorporating embedding achieve improved frameworks computer vision;edge detection;image segmentation;iterative methods;optimisation;pattern clustering;bsds ;bregman iterations;l -regularized energy term;computer vision tasks;contour detection;high-level visual recognition;image segmentation frameworks;low-level photo processing;low-level video processing;nonlinear embedding;piecewise flat embedding;scene understanding;segment boundary identification;sparse signal recovery;spectral clustering;two-stage numerical algorithm;clustering algorithms;context;image segmentation;laplace equations;linear programming;manifolds;optimization 
semantic image segmentation deep parsing network paper addresses semantic image segmentation incorporating rich markov random field mrf including high-order relations mixture label contexts previous optimized mrfs iterative algorithm solve mrf proposing convolutional neural network cnn deep parsing network dpn enables deterministic end-to-end computation single forward pass dpn extends contemporary cnn architecture model unary terms additional layers carefully devised approximate field algorithm mf pairwise terms appealing properties combined cnn mrf iterations mf required training image back-propagation dpn achieve performance approximating iteration mf dpn represents types pairwise terms existing special dpn mf easier parallelized speeded graphical processing unit gpu dpn evaluated pascal voc dataset single dpn model yields state-of-the-art segmentation accuracy markov processes;approximation theory;convolution;graphics processing units;image segmentation;iterative methods;learning artificial intelligence ;neural nets;semantic networks;cnn;dpn;gpu;mf approximation;mrf;markov random field;convolutional neural network;deep parsing network;graphical processing unit;high-order relation;iterative algorithm;label context mixture;mean field algorithm;semantic image segmentation;computational efficiency;computational modeling;context;graphics processing units;image segmentation;labeling;semantics 
human parsing contextualized convolutional neural network address human parsing task contextualized convolutional neural network co-cnn architecture integrates cross-layer context global image-level context within-super-pixel context cross-super-pixel neighborhood context unified network input human image co-cnn produces pixel-wise categorization end-to-end cross-layer context captured basic local-to-global-to-local structure hierarchically combines global semantic structure local fine details cross-layers global image-level label prediction auxiliary objective intermediate layer co-cnn outputs guiding feature learning subsequent convolutional layers leverage global image-level context finally utilize local super-pixel contexts within-super-pixel smoothing cross-super-pixel neighbourhood voting formulated natural sub-components co-cnn achieve local label consistency training testing process comprehensive evaluations public datasets demonstrate superiority co-cnn architecture state-of-the-arts human parsing f- score dataset reaches co-cnn state-of-the-art algorithms m-cnn atr feature extraction;learning artificial intelligence ;neural nets;smoothing methods;co-cnn;contextualized convolutional neural network architecture;convolutional layers;cross-layer context;cross-super-pixel neighborhood context;cross-super-pixel neighbourhood voting;feature learning;global image-level context;global image-level label prediction;global semantic structure;human parsing;local fine details;local label consistency;local super-pixel context;local-to-global-to-local structure;pixel-wise categorization;within-super-pixel context;within-super-pixel smoothing;context;labeling;neural networks;semantics;smoothing methods;spatial resolution;training 
holistically-nested edge detection develop edge detection algorithm addresses critical issues long-standing vision problem: holistic image training multi-scale feature learning proposed method holistically-nested edge detection pixel-wise edge classification image-to-image prediction deep learning model leverages convolutional neural networks deeply-supervised nets automatically learns rich hierarchical representations guided deep supervision responses crucially approach human ability resolve challenging ambiguity edge object boundary detection advance state-of-the-art bsd dataset ods f-score nyu depth dataset ods f-score improved speed image magnitude faster cnn-based edge detection algorithms edge detection;feature extraction;feedforward neural nets;image classification;image representation;image resolution;learning artificial intelligence ;bsd dataset;nyu depth dataset;convolutional neural networks;deep learning model;deeply-supervised nets;hierarchical representations;holistic image training;holistically-nested edge detection algorithm;image-to-image prediction;multiscale feature learning;object boundary detection;pixel-wise edge classification;detectors;feature extraction;image edge detection;machine learning;neural networks;predictive models;training 
minimum barrier salient object detection fps propose highly efficient powerful salient object detection method based minimum barrier distance mbd transform mbd transform robust pixel-value fluctuation effectively applied raw pixels region abstraction approximate mbd transform algorithm speedup exact algorithm error bound analysis provided powered fast mbd transform algorithm proposed salient object detection method runs fps outperforms previous methods speed benchmark datasets achieves comparable performance state-of-the-art methods technique based color whitening proposed extend method leverage appearance-based backgroundness cue extended version improves performance magnitude faster leading methods error analysis;object detection;mbd transform algorithm;appearance-based backgroundness;color whitening;error bound analysis;exact algorithm;minimum barrier distance transform;minimum barrier salient object detection;pixel-value fluctuation;region abstraction;algorithm design analysis;approximation algorithms;computer vision;cost function;image color analysis;object detection;transforms 
learning image representations tied ego-motion understanding images objects scenes behave response specific ego-motions crucial aspect proper visual development existing visual learning methods conspicuously disconnected physical source images propose exploit proprioceptive motor signals provide unsupervised regularization convolutional neural networks learn visual representations egocentric video enforce learned features exhibit equivariance respond predictably transformations distinct ego-motions datasets unsupervised feature learning approach outperforms previous approaches visual recognition next-best-view prediction tasks challenging test features learned video captured autonomous driving platform improve large-scale scene recognition static images disjoint domain image representation;motion estimation;unsupervised learning;convolutional neural networks;ego-motion;egocentric video;learning image representations;motor signals;object images;unsupervised feature learning approach;visual development;visual learning methods;visual recognition;cameras;convolution;image recognition;observers;robot sensing systems;training data;visualization 
unsupervised visual representation learning context prediction explores spatial context source free plentiful supervisory signal training rich visual representation unlabeled image collection extract random pairs patches image train convolutional neural net predict position patch relative argue task requires model learn recognize objects demonstrate feature representation learned within-image context captures visual similarity images representation perform unsupervised visual discovery objects cats people birds pascal voc detection dataset learned convnet r-cnn framework boost randomly-initialized convnet state-of-the-art performance algorithms pascal-provided training set annotations feature extraction;image representation;object recognition;prediction theory;unsupervised learning;convnet;pascal voc detection dataset;r-cnn framework;context prediction;convolutional neural net;feature representation;image patches extraction;objects recognition;position prediction;spatial context;supervisory signal;unlabeled image collection;unsupervised visual discovery;unsupervised visual representation learning;visual similarity;within-image context;context;data mining;image representation;predictive models;semantics;training;visualization 
webly supervised learning convolutional networks approach utilize amounts web data learning cnns inspired curriculum learning two-step approach cnn training easy images train initial visual representation initial cnn adapt harder realistic images leveraging structure data categories demonstrate two-stage cnn outperforms fine-tuned cnn trained imagenet pascal voc demonstrate strength webly supervised learning localizing objects web images training r-cnn style detector achieves performance voc voc training data finally approach robust noise performs comparably image search march pre-cnn image search era internet;data structures;image retrieval;learning artificial intelligence ;neural nets;cnn training;imagenet;pascal voc ;r-cnn style detector;voc ;voc training data;web data;web image;webly supervised learning;data structure;image search;initial visual representation;two-stage cnn;data models;google;noise measurement;search engines;training;visualization 
fast r-cnn paper proposes fast region-based convolutional network method fast r-cnn object detection fast r-cnn builds previous efficiently classify object proposals deep convolutional networks compared previous fast r-cnn employs innovations improve training testing speed increasing detection accuracy fast r-cnn trains deep vgg network faster r-cnn faster test-time achieves map pascal voc compared sppnet fast r-cnn trains vgg faster tests faster accurate fast r-cnn implemented python c++ caffe open-source mit license https://github com/rbgirshick/fast-rcnn feedforward neural nets;object detection;c++;caffe;python;vgg network;fast r-cnn;fast region-based convolutional network method;object detection;open-source mit license;computer architecture;feature extraction;object detection;open source software;pipelines;proposals;training 
bilinear cnn models fine-grained visual recognition propose bilinear models recognition architecture consists feature extractors outputs multiplied outer product location image pooled image descriptor architecture model local pairwise feature interactions translationally invariant manner fine-grained categorization generalizes orderless texture descriptors fisher vector vlad experiments bilinear models feature extractors based convolutional neural networks bilinear form simplifies gradient computation end-to-end training networks image labels networks initialized imagenet dataset domain specific fine-tuning accuracy cub- dataset requiring category labels training time experiments visualizations analyze effects fine-tuning choice networks speed accuracy models architecture compares favorably existing art fine-grained datasets simpler easier train accurate model fairly efficient running frames/sec nvidia tesla gpu source code complete system http://vis-www cs umass edu/bcnn feature extraction;neural nets;source code software ;imagenet dataset;nvidia tesla gpu;bilinear cnn model;convolutional neural network;feature extraction;fine-grained visual recognition architecture;image descriptor;image label;local pairwise feature interaction;orderless texture descriptor;source code;atmospheric modeling;computational modeling;computer architecture;feature extraction;image recognition;training;visualization 
discovering spatial extent relative attributes weakly-supervised approach discovers spatial extent relative attributes pairs images contrast traditional approaches global appearance features rely keypoint detectors goal automatically discover image regions relevant attribute attribute's appearance drastically attribute spectrum accomplish develop formulation combines detector local smoothness discover set coherent visual chains image collection introduce efficient generate additional chains anchored initial discovered finally automatically identify relevant visual chains create ensemble image representation model attribute extensive experiments demonstrate method's promise relative baselines modeling relative attributes feature extraction;image representation;coherent visual chains;ensemble image representation;global appearance features;image collection;keypoint detectors;relative attributes;weakly-supervised approach;computational modeling;computer vision;detectors;footwear;image representation;scalability;visualization 
deep neural decision forests deep neural decision forests approach unifies classification trees representation learning functionality deep convolutional networks training end-to-end manner combine worlds introduce stochastic differentiable decision tree model steers representation learning conducted initial layers deep convolutional network model differs conventional deep networks decision forest final predictions differs conventional decision forests propose principled joint global optimization split leaf node parameters experimental benchmark machine learning datasets mnist imagenet on-par superior compared state-of-the-art deep models remarkably top -errors %/ imagenet validation data integrating forests single-crop single/seven model googlenet architecture form training data set augmentation improving error googlenet architecture models crops computer vision;convolution;decision trees;image representation;learning artificial intelligence ;neural nets;optimisation;classification tree;computer vision;deep convolutional network;deep neural decision forest;machine learning;parameter optimization;representation learning functionality;computer vision;decision trees;optimization;routing;stochastic processes;training;vegetation 
deep fried convnets fully-connected layers deep convolutional neural networks typically network parameters reducing parameters preserving predictive performance critically training models distributed systems deployment embedded devices paper introduce adaptive fastfood transform reparameterize matrix-vector multiplication connected layers reparameterizing connected layer inputs outputs adaptive fastfood transform reduces storage computational costs costs log adaptive fastfood transform convolutional networks call deep fried convnet convnets end-to-end trainable enable attain substantial reductions parameters prediction accuracy mnist imagenet datasets computational complexity;matrix algebra;neural nets;adaptive fastfood transform;deep convolutional neural networks;network parameters;adaptive systems;computational efficiency;kernel;neural networks;sparse matrices;training;transforms 
semantic component analysis unsupervised weakly-supervised visual learning image collections critical avoid time-consuming error-prone process manual labeling standard approaches rely methods multiple-instance learning graphical models computationally intensive sensitive initialization hand simpler component analysis clustering methods achieve meaningful invariances semantic interpretability address issues previous simple effective method called semantic component analysis sca decomposition images semantic components unsupervised sca decomposes additive image representations spatially-meaningful visual components naturally correspond object categories overcomplete representation rich instance-level constraints spatial priors sca improved interpretable components comparison traditional matrix factorization techniques weakly-supervised form image-level tags sca factorizes set images semantic superpixels provide qualitative connections traditional methods component analysis grassmann averages pca nmf effectiveness approach validated synthetic data msrc sift flow datasets demonstrating competitive unsupervised weakly-supervised semantic segmentation image representation;principal component analysis;unsupervised learning;additive image representation;image-level tags;instance-level constraint;overcomplete representation;semantic component analysis;spatially-meaningful visual component;unsupervised sca;unsupervised visual learning;weakly-supervised visual learning;feature extraction;histograms;image segmentation;matrix decomposition;principal component analysis;semantics;visualization 
low-rank matrix factorization mixture noise distributions computer vision posed learning low-dimensional subspace dimensional data low rank matrix factorization lrmf represents commonly utilized subspace learning strategy current lrmf techniques constructed optimization l_ norm l_ norm deal laplacian gaussian noise lrmf capable adapting complex noise paper proposes lrmf model assuming noise mixture exponential power moep distributions proposes penalized moep model combining penalized likelihood method moep distributions setting facilitates learned lrmf model capable automatically fitting real noise moep distributions component mixture adapted series preliminary super-or sub-gaussian candidates expectation maximization em algorithm designed infer parameters involved proposed pmoep model advantage method demonstrated extensive experiments synthetic data modeling hyperspectral image restoration gaussian processes;computer vision;exponential distribution;image restoration;matrix decomposition;mixture models;em algorithm;lrmf;moep distributions;computer vision;expectation maximization algorithm;general mixture noise distributions;hyperspectral image restoration;low-rank matrix factorization;mixture exponential power;penalized likelihood method;subspace learning strategy;adaptation models;algorithm design analysis;computer vision;data mining;data models;gaussian noise;robustness 
web-scale image clustering revisited scale duplicate detection clustering mining documents images conventionally treated seed detection hashing seed growing heuristics fast search principled clustering methods kernelized spectral complexity difficult scale millions assumption documents images embedded euclidean space revisit advances approximate k-means variants borrow ingredients introduce inverted-quantized k-means iq-means key underlying concepts quantization data multi-index based inverted search centroids cells quantization form hashing analogous seed detection updates analogous seed growing principled sense distortion minimization design dynamic variant determine clusters single additional cost combined powerful deep learned representations achieve clustering image collection single machine hour approximation theory;data mining;document handling;image processing;minimisation;pattern clustering;search problems;iq-means;web-scale image clustering;approximate k-means variants;data quantization;distortion minimization;document clustering;document mining;duplicate detection;dynamic variant design;hashing;inverted-quantized k-means;multiindex based inverted search;principled clustering methods;seed detection;seed growing heuristics;artificial neural networks;distortion;metadata;probabilistic logic;quantization signal ;search problems;visualization 
learning discriminative reconstructions unsupervised outlier removal study automatically removing outliers noisy data application removing outlier images image collection address utilizing reconstruction errors autoencoder observe data reconstructed low-dimensional representations inliers outliers separated reconstruction errors based basic observation gradually inject discriminative learning process autoencoder inliers outliers separable experiments variety image datasets validate approach image reconstruction;learning artificial intelligence ;visual databases;autoencoder;discriminative reconstructions learning;image collection;image datasets;low-dimensional representations;reconstruction errors;unsupervised outlier removal;computer vision;image reconstruction;neurons;noise measurement;principal component analysis;training;training data 
learning deconvolution network semantic segmentation propose semantic segmentation algorithm learning deep deconvolution network learn network top convolutional layers vgg -layer net deconvolution network composed deconvolution unpooling layers identify pixelwise class labels predict segmentation masks apply trained network proposal input image construct final semantic segmentation map combining proposals simple manner proposed algorithm mitigates limitations existing methods based convolutional networks integrating deep deconvolution network proposal-wise prediction segmentation method typically identifies detailed structures handles objects multiple scales naturally network demonstrates outstanding performance pascal voc dataset achieve accuracy methods trained microsoft coco dataset ensemble convolutional network convolution;deconvolution;image segmentation;learning artificial intelligence ;neural nets;prediction theory;semantic networks;cnn;convolutional neural network;deconvolution network learning;proposal-wise prediction;semantic segmentation algorithm;deconvolution;feature extraction;image reconstruction;image segmentation;semantics;shape;visualization 
conditional random fields recurrent neural networks pixel-level labelling tasks semantic segmentation play central role image understanding approaches attempted harness capabilities deep learning techniques image recognition tackle pixel-level labelling tasks central issue methodology limited capacity deep learning techniques delineate visual objects solve introduce form convolutional neural network combines strengths convolutional neural networks cnns conditional random fields crfs -based probabilistic graphical modelling formulate conditional random fields gaussian pairwise potentials mean-field approximate inference recurrent neural networks network called crf-rnn plugged cnn deep network desirable properties cnns crfs importantly system integrates crf modelling cnns train deep network end-to-end usual back-propagation algorithm avoiding offline post-processing methods object delineation apply proposed method semantic image segmentation obtaining top challenging pascal voc segmentation benchmark gaussian processes;backpropagation;image segmentation;neural nets;probability;random processes;cnn;crf;gaussian pairwise potential;back-propagation algorithm;conditional random field;convolutional neural network;deep learning technique;image understanding;mean-field approximate inference;pixel-level labelling task;probabilistic graphical modelling;recurrent neural network;semantic image segmentation;computer vision;graphical models;image segmentation;labeling;machine learning;semantics;training 
triangle parallelograms sampling strategy application shape regression purpose paper threefold firstly paper introduces triangle parallelograms ottp sampling strategy viewed pixels shape image framework cascaded shape regression including ottp sampling short framework involves binary pixel tests appearance features combined shape features linear system regression stage cascade proposed solution produce state-of-the-art task facial landmark estimation thirdly dependence accuracy landmark predictions placement shape detection box discussed method visualize data visualisation;feature extraction;image sampling;regression analysis;shape recognition;ottp sampling strategy;binary pixel tests;cascaded shape regression;detection box;facial landmark estimation;landmark predictions;linear system;one triangle parallelogram sampling strategy;regression stage;shape features;shape regression;computer vision;estimation;feature extraction;indexes;shape;training;visualization 
boosting object proposals: pascal coco computer vision object proposals nowadays influenced databases researchers evaluate performance algorithms paper studies transition pascal visual object challenge dataset benchmark reference updated bigger challenging microsoft common objects context review deeply analyze challenges opportunities database survey current art object proposals evaluate focusing generalizes dataset sight propose lines advantage benchmark improve techniques explore lines leads improvement art computer vision;visual databases;coco;microsoft common objects context;pascal;pascal visual object challenge dataset;computer vision;databases;object proposals;computer vision;databases;image segmentation;object segmentation;proposals;training;visualization 
secrets grabcut kernel k-means log-likelihood energy term popular model-fitting segmentation methods generalized probabilistic k-means\ energy color space clustering interpretation reveals limitations over-fitting propose alternative approach color clustering kernel k-means energy well-known properties non-linear separation scalability higher-dimensional feature spaces bound formulation kernel k-means combine pair-wise feature clustering methods image grid regularization graph cuts standard color model fitting techniques segmentation histogram gmm fitting approach closely average association normalized cut contrast previous pairwise clustering algorithms approach incorporate standard geometric regularization image domain analyze extreme kernel bandwidth gini bias demonstrate effectiveness knn-based adaptive bandwidth strategies kernel k-means approach segmentation benefits higher-dimensional features standard model fitting fails feature extraction;graph theory;image colour analysis;image segmentation;pattern clustering;probability;grabcut;knn-based adaptive bandwidth strategies;average association;bound formulation;color space clustering;generalized probabilistic k-means energy;graph cuts;higher-dimensional feature spaces;image grid regularization;kernel k-means energy;kernel bandwidth;log-likelihood energy term;model-fitting segmentation methods;normalized cut;pair-wise feature clustering methods;standard geometric regularization;entropy;histograms;image color analysis;image segmentation;kernel;probabilistic logic;standards 
video matting sparse low-rank representation introduce method video matting sparse low-rank representation previous matting methods introduced nonlocal prior estimate alpha matte achieved impressive data hand searching inadequate excessive samples samples introduce noise hand difficult construct consistent nonlocal structures pixels features yielding spatially temporally inconsistent video mattes paper proposed video matting method achieve spatially temporally consistent matting result sparse low-rank representation model introduced pursue consistent nonlocal structures pixels features sparse representation adaptively select samples accurately construct nonlocal structures pixels low-rank representation globally ensure consistent nonlocal structures pixels features representations combined generate consistent video mattes experimental method achieved quality variety challenging examples featuring illumination feature ambiguity topology transparency variation dis-occlusion fast motion motion blur video signal processing;low-rank representation;sparse representation;spatially consistent video mattes;temporally consistent video mattes;video matting;dictionaries;geometrical optics;laplace equations;optical coupling;optical imaging;sparse matrices;topology 
joint object segmentation deep learned potentials segmenting semantic objects images parsing respective semantic fundamental steps detailed object understanding computer vision paper propose joint solution tackles semantic object segmentation simultaneously object-level context provided guide segmentation detailed part-level localization utilized refine object segmentation introduce concept semantic compositional scp semantic shared objects two-stream convolutional network fcn trained provide scp object potentials pixel time compact set segments scp predictions network potentials generated segments explore long-range context finally construct efficient connected conditional random field fcrf jointly predict final object labels extensive evaluation datasets approach mutually enhance performance object segmentation outperforms current state-of-the-art tasks computer vision;image segmentation;learning artificial intelligence ;prediction theory;random processes;fcn;fcrf;scp predictions;computer vision;deep learned potentials;fully connected conditional random field;object labels;object potentials;object understanding;object-level context;parsing;part labels;part segmentation;part-level localization;semantic compositional parts;semantic object segmentation;two-stream convolutional network;context;image segmentation;legged locomotion;object segmentation;proposals;semantics;training 
low-rank tensor constrained multiview subspace clustering paper explore multiview subspace clustering introduce low-rank tensor constraint explore complementary multiple views establish method called low-rank tensor constrained multiview subspace clustering lt-msc method subspace representation matrices views tensor captures dexterously correlations underlying multiview data tensor equipped low-rank constraint models elegantly cross views reduces effectually redundancy learned subspace representations improves accuracy clustering inference process affinity matrix clustering formulated tensor nuclear norm minimization constrained additional -norm regularizer linear equalities minimization convex solved efficiently augmented lagrangian alternating direction minimization al-adm method extensive experimental benchmark datasets effectiveness proposed lt-msc method computer vision;learning artificial intelligence ;pattern clustering;tensors;al-adm;augmented lagrangian alternating direction minimization;lt-msc;affinity matrix;complementary information;computer vision;convex programming;inference process;learned subspace representations;low rank tensor constrained multiview subspace clustering;machine learning;multiview data;subspace representation matrices;tensor nuclear norm minimization problem;aerospace electronics;clustering methods;correlation;kernel;minimization;optimization;tensile stress 
bodyprint: pose invariant shape matching human bodies human body shape matching potential real applications advances range sensing technology address proposing holistic human body shape descriptor called bodyprint compute bodyprint body scan fit deformable human body mesh project mesh parameters low-dimensional subspace improves discriminability persons experiments carried real-world human body datasets demonstrate bodyprint robust pose variation missing sensor noise improves matching accuracy compared conventional shape matching techniques local features facilitate practical applications shape database grow time extend learning framework handle online updates image matching;learning artificial intelligence ;mesh generation;shape recognition; range sensing technology;bodyprint;deformable human body mesh;holistic human body shape descriptor;learning framework;pose invariant human body shape matching;databases;measurement;principal component analysis;robustness;shape;three-dimensional displays;training 
middle child problem: revisiting parametric min-cut seeds object proposals object proposals fueled progress detection performance proposals aim provide category-agnostic localizations objects image generate proposals perform parametric min-cuts seed locations paper demonstrates standard parametric-cut models ineffective obtaining medium-sized objects refer middle child propose energy minimization framework incorporating geodesic distances segments solves addition introduce superpixel merging algorithm generate set seeds reliably cover objects sizes call method poise proposals objects improved seeds energies poise enables parametric min-cuts reach potential pascal voc generates segments average overlap closest competing methods require proposals reach accuracy detailed quantitative comparisons state-of-the-art methods pascal voc microsoft coco segmentation challenges category theory;differential geometry;minimisation;object detection;poise;category-agnostic localization;energy minimization framework;geodesic distance;middle child problem;object detection;object proposal;parametric min-cut model;proposals objects improved seeds energies;seed location;image segmentation;measurement;merging;minimization;proposals;reliability;standards 
contour guided hierarchical model shape matching simplicity effectiveness star model popular shape matching suffers loose geometric connections paper algorithm reconsiders connections reduces global matching set interrelated local matching purpose divide shape template overlapped model matching part-based layered structure latent variable constrain parts' deformation inference localizing candidates partial matching contour fragments partial matching solved modified dynamic programming overlapped regions template explored candidates meet shared process fulfilled refined procedure based iterative dynamic programming ethz shape inria horse datasets demonstrate benefits proposed algorithm dynamic programming;image matching;iterative methods;ethz shape dataset;inria horse datasets;candidate localization;contour fragments;contour guided hierarchical model;global matching;interrelated local matching;iterative dynamic programming;latent variable;overlapped parts;part-based layered structure;partial matching;shape matching;shape template;star model;computational modeling;deformable models;image edge detection;periodic structures;shape;shape measurement;transforms 
robust image segmentation contour-guided color palettes contour-guided color palette ccp proposed robust image segmentation efficiently integrates contour color cues image representative colors image color samples contours regions spirit machine learning methodology focus samples decision boundaries collected mean-shift ms algorithm sampled color space achieve image-dependent color palette color palette preliminary segmentation spatial domain fine-tuned post-processing techniques leakage avoidance fake boundary removal region mergence segmentation performances ccp ms compared analyzed ccp offers acceptable standalone segmentation result integrated framework layered spectral segmentation produce robust segmentation superior performance ccp-based segmentation algorithm demonstrated experiments berkeley segmentation dataset image colour analysis;image segmentation;learning artificial intelligence ;color cues;contour-guided color palette;fake boundary removal;image-dependent color palette;layered spectral segmentation;leakage avoidance;machine learning;mean-shift algorithm;robust image segmentation;small region mergence;bandwidth;complexity theory;image color analysis;image segmentation;quantization signal ;robustness;spectral analysis 
joint optimization segmentation color clustering binary energy optimization popular approach segmenting color image foreground/background regions model appearance regions color dimensional feature handled effectively color histogram sparse reliable approach explicitly reduce dimensionality clustering quantizing color space popular approach fit gmms soft implicit clustering color space approaches foreground/background distinct subtle difference appearance approaches reduce eliminate foreground/background distinction color clustering performed completely independently segmentation process preprocessing step clustering independently foreground independently background gmm propose clustering integral segmentation including clustering term energy function energy function clustering term favours clusterings foreground/background appearance distinct energy function jointly optimizes color clustering foreground/background models segmentation exact optimization feasible develop approximate algorithm advantage including color clustering term energy function camouflage images standard segmentation datasets approximation theory;image colour analysis;image segmentation;optimisation;approximate algorithm;binary energy optimization;color clustering;color image segmentation;full color histogram;clustering algorithms;computational modeling;histograms;image color analysis;image segmentation;optimization;standards 
boxsup: exploiting bounding boxes supervise convolutional networks semantic segmentation leading approaches semantic segmentation rely deep convolutional networks trained human-annotated pixel-level segmentation masks pixel-accurate supervision demands expensive labeling effort limits performance deep networks benefit training data paper propose method achieves competitive accuracy requires easily bounding box annotations basic idea iterate automatically generating region proposals training convolutional networks steps gradually recover segmentation masks improving networks vise versa method called boxsup\ produces competitive map validation supervised boxes par strong baselines map supervised masks setting leveraging amount bounding boxes boxsup yields state-of-the-art pascal voc pascal-context image segmentation;neural nets;boxsup;pascal voc ;pascal-context;bounding box annotations;bounding boxes;convolutional network training;convolutional networks;human-annotated mask;pixel-accurate supervision;pixel-level segmentation mask;semantic segmentation;erbium;image segmentation;labeling;linear programming;proposals;semantics;training 
detection segmentation curved reflection symmetric structures symmetry key components gestalt theory mid-level cue serves input visual processes segmentation propose complete approach links detection curved reflection symmetries produce symmetry-constrained segments structures/regions real images clutter curved reflection symmetry detection leverage patch-based symmetric features train structured random forest classifier detects multiscaled curved symmetries images curved symmetries modulate symmetry-constrained foreground-background segmentation symmetry scores enforce global symmetrical consistency final segmentation achieved imposing pairwise symmetry prior encourages symmetric pixels labels mrf-based representation input image edges final segmentation graph-cuts experimental publicly datasets annotated symmetric structures: symmax- bsd-parts weizmann horse ny-roads demonstrate approach's applicability environments state-of-the-art performance image segmentation;object detection; curved reflection symmetric structures; images;bsd-parts;gestalt theory;mrf-based representation;ny-roads;symmax- ;weizmann horse;annotated symmetric structures;global symmetrical consistency;graph-cuts;multiscaled curved symmetries;pairwise symmetry;patch-based symmetric features;real images;structured random forest classifier;symmetry-constrained foreground-background segmentation;visual processes;clutter;computer vision;feature extraction;image color analysis;image segmentation;robustness;visualization 
unsupervised tube extraction transductive learning dense trajectories address automatic extraction foreground objects videos goal provide method unsupervised collection samples object detection training human intervention selective search approach produce initial still-image based segmentation video frames initial set proposals pruned temporally extended optical flow transductive learning propose dense trajectories robustly match track candidate boxes frames box tracks collect samples unsupervised training track-specific detectors finally detectors videos extract final tubes combination appearance-based static objectness\ selective search motion dense trajectories transductive learning detectors forced overfit\ unsupervised data training proposed approach extremely robust outperform state-of-the-art systems margin common benchmarks tube proposal evaluation image sequences;object detection;unsupervised learning;dense trajectory;object detection training;optical flow;still-image based segmentation;track-specific detector;transductive learning;unsupervised data;unsupervised tube extraction;video frame;detectors;electron tubes;optical imaging;tracking;training;trajectory;videos 
compositional hierarchical representation shape manifolds classification non-manifold shapes address statistical learning shape models invariant translation rotation scale compositional hierarchies data spaces measurements shape spaces topological manifolds practice observed modeling shapes multiple disconnected components partially occluded shapes cluttered scenes resolve aforementioned reformulating relationship data shape spaces interaction receptive fields rfs shape manifolds sms compositional hierarchical shape vocabulary method model topological structure sms statistical learning geometric transformations shapes defined actions sms purpose design disjoint union topology indexing mechanism formation shape models sms vocabulary recursively represent topological relationship shape components graphs aggregated construct hierarchical graph structure shape vocabulary introduce framework implement indexing mechanisms employment vocabulary structural shape classification proposed approach construct invariant shape representations benchmark shape classification outperform state-of-the-art methods geometry;graph theory;image classification;image representation;indexing;learning artificial intelligence ;shape recognition;rf;sm;compositional hierarchical representation;geometric transformation;graph theory;indexing mechanism;nonmanifold shape classification;receptive field;shape manifold;shape model;shape representation;statistical learning;structural shape classification;topological relationship;computational modeling;manifolds;mathematical model;radio frequency;shape;shape measurement;statistical learning 
shell pca: statistical shape modelling shell space paper perform principal components analysis shell space\ thin shells physical model surfaces non-zero thickness deformation dissipates elastic energy thin shells discrete counterparts considered reside shell space notion distance elastic energy required deform shape setting perform statistical analysis set shapes meshes dense correspondence providing hybrid physical statistical shape modelling models capture non-linear deformations articulated motion training data sparse compared dimensionality observation space modelling;principal component analysis;shape recognition;elastic energy;principal component analysis;shell pca;statistical shape modelling;computational modeling;computer vision;data models;manifolds;principal component analysis;shape 
learning combine mid-level cues object proposal generation region proposals replaced sliding windows support object recognition offering discriminating shape appearance improved localization powerful approach generating region proposals based minimizing parametric energy functions parametric maxflow paper introduce parametric min-loss pml structured learning framework parametric energy functions pml applicable domains context region proposals learn combine set mid-level cues yield set object region proposals recall learning framework accounts multiple diverse outputs complemented diversification seeds based image location color approach casts perceptual cue combination structured learning framework yields baseline improvements voc coco learning artificial intelligence ;object recognition;pml;combine midlevel cues;learning framework;object proposal generation;object recognition;object region proposals;parametric energy functions;parametric maxflow;parametric min-loss;adaptation models;detectors;graphical models;image color analysis;image segmentation;proposals;shape 
enhancing road maps parsing aerial images contextual models exploit maps effective recognition localization tasks paper propose exploit aerial images enhance freely maps goal openstreetmap formulate inference markov random field parameterized terms location road-segment centerlines width parameterization enables efficient inference returns topologically correct roads segment osm roads single day cluster computers importantly approach generalizes trained aerial imagery produce accurate location globe demonstrate effectiveness approach outperforming state-of-the-art benchmarks collect enhanced maps beneficial semantic segmentation ground images markov processes;image segmentation;road traffic;traffic engineering computing;markov random field;openstreetmap;aerial imagery;enhancing road maps;ground images;parsing aerial images;road segment centerlines;semantic segmentation;computer vision;image segmentation;manuals;markov processes;roads;robustness;topology 
probabilistic appearance models segmentation classification statistical shape appearance models based accurate identification one-to-one correspondences training data set time determination landmarks challenging methods hufnagel etal developed alternative method correspondence probabilities statistical shape model propose probabilistic correspondences statistical appearance models incorporating appearance framework point-based representation employed representing image set vectors assembling position appearances probabilistic correspondences multi-dimensional feature vectors eliminates extensive preprocessing landmarks reduces dependence generated model landmark positions maximum a-posteriori approach derive single global optimization criterion respect model parameters observation dependent parameters directly shape appearance considered structures model generation fitting expressed optimizing criterion developed framework describes modeling process concise flexible mathematical additional constraints topological regularity modeling process eliminates demand costly correspondence determination apply model segmentation landmark identification hand x-ray images segmentation modeled features vectorial image representation demonstrate feasibility model reconstruct contours landmarks unseen test images apply model tissue classification model generated healthy brain tissue mri slices applying model images stroke patients probabilistic correspondences t- classify healthy pathological structures demonstrate ability probabilistic model recognize healthy pathological tissue automatically biomedical mri;feature extraction;image classification;image representation;image segmentation;maximum likelihood estimation;medical image processing;probability;shape recognition;statistical analysis;topology; mri slices;x-ray images;global optimization criterion;healthy brain tissue;landmark determination;landmark positions;maximum a-posteriori approach;model generation;multidimensional feature vectors;point-based representation;probabilistic appearance models;probabilistic correspondences;statistical appearance models;statistical shape models;tissue classification;topological regularity;vectorial image representation;adaptation models;brain models;computational modeling;data models;probabilistic logic;shape 
randomized ensemble approach industrial ct segmentation tuning models parameters common segmentation approaches challenging presence noise artifacts ensemble-based techniques attempt compensate randomly varying models and/or parameters create diverse set hypotheses subsequently ranked arrive solution methods restricted underlying models well-established natural images practice difficult determine suitable base-model amount randomization required multi-object scenes single hypothesis perform objects reducing quality paper ensemble-based segmentation framework industrial ct images demonstrating comparatively simple models randomization strategies improve result existing techniques introduce per-object based ranking consensus inference outperform scenario existing hypothesis ranking approaches demonstrate effectiveness approach set noise artifact rich ct images baggage security outperforms existing solutions computerised tomography;image segmentation;baggage security;base-model;consensus inference;ensemble-based segmentation framework;industrial ct images;industrial ct segmentation;per-object based ranking;randomized ensemble approach;computed tomography;image segmentation;metals;object recognition;security;semantics;training data 
semi-supervised normalized cuts image segmentation introduction powerful graph-based method image segmentation normalized cuts ncuts algorithm generalized incorporate expert knowledge pixels regions segmentation biased correlated priors previous approaches incorporate hard must-link constraints pixels hard cannot-link constraints pixels separated paper reformulate ncuts sets constraints handled soft manner enabling user tune degree constraints satisfied approximate spectral solution reformulated exists requiring explicit construction dense matrix computation time comparable unconstrained ncuts synthetic data real imagery soft handling constraints yields unconstrained ncuts enables robust clustering segmentation constraints strictly enforced computational complexity;constraint handling;expert systems;graph theory;image segmentation;matrix algebra;pattern clustering;realistic images;ncuts algorithm;approximate spectral solution;computation time;constraint handling;dense matrix;expert knowledge;graph-based method;image segmentation;must-link constraint;real imagery;robust clustering;semi-supervised normalized cut;synthetic data;unconstrained ncut;chlorine;clustering algorithms;eigenvalues eigenfunctions;image segmentation;linear programming;minimization;partitioning algorithms 
stereosnakes: contour based consistent object extraction stereo images consistent object extraction plays essential role stereo image editing population stereoscopic media previous methods perform segmentation entire images views dense stereo correspondence constraints methods computation highly redundant views near-duplicate consistency violated imperfectness current stereo matching algorithms paper propose contour based method searches consistent object contours regions integrates stereo correspondence object boundary constraints energy minimization framework proposed method advantages compared previous searching space restricted object boundaries efficiency improved discriminative power object contours consistent segmentation proposed method effortlessly extend existing single-image segmentation methods stereo scenarios experiment adobe benchmark superior extraction accuracy improvement efficiency method state-of-the-art demonstrate applications method basic tool stereo image editing image matching;image segmentation;search problems;stereo image processing;adobe benchmark;stereosnakes;contour based consistent object extraction;contour based method;dense stereo correspondence constraint;discriminative power;energy minimization framework;extraction accuracy;object boundary constraint;object contour;searching space;single-image segmentation method;stereo image editing;stereo matching algorithm;stereo scenario;stereoscopic media;computational modeling;histograms;image color analysis;image segmentation;media;stereo image processing;three-dimensional displays 
semantic segmentation rgbd images mutex constraints paper address semantic scene segmentation rgb-d images indoor scenes propose image region labeling method augments crf formulation hard mutual exclusion mutex constraints approach rich accurate geometric structure coming kinect principled manner final labeling result satisfy mutex constraints eliminate configurations violate common sense physics laws placing floor night stand classes mutex constraints proposed: global object co-occurrence constraint relative height relationship constraint local support relationship constraint evaluate approach nyu-depth dataset consists cluttered indoor scenes test generalization model trained nyu-depth dataset directly sun dataset training experimental outperform state-of-the-art methods scene labeling datasets image colour analysis;image segmentation;image sensors; geometric structure;crf formulation;kinect;mutex constraints;nyu-depth dataset;sun dataset;cluttered indoor scenes;global object cooccurrence constraint;image region labeling method;local support relationship constraint;mutex constraints;semantic rgb-d image segmentation;semantic scene segmentation;test generalization;computational modeling;feature extraction;image segmentation;labeling;semantics;three-dimensional displays;visualization 
weakly-and semi-supervised learning deep convolutional network semantic image segmentation deep convolutional neural networks dcnns trained images strong pixel-level annotations pushed state-of-art semantic image segmentation study challenging learning dcnns semantic image segmentation weakly annotated training data bounding boxes image-level labels combination labeled weakly labeled images sourced multiple datasets develop expectation-maximization em methods semantic image segmentation model training weakly supervised semi-supervised settings extensive experimental evaluation proposed techniques learn models delivering competitive challenging pascal voc image segmentation benchmark requiring annotation effort share source code implementing proposed system https://bitbucket org/deeplab/deeplab-public expectation-maximisation algorithm;feedforward neural nets;image segmentation;learning artificial intelligence ;dcnn;em method;pascal voc image segmentation benchmark;deep convolutional neural network;expectation-maximization method;pixel-level annotation;semantic image segmentation model training;semisupervised learning;strongly labeled image;weakly annotated training data;weakly labeled image;weakly-supervised learning;benchmark testing;convolutional codes;google;image segmentation;semantics;training;training data 
efficient decomposition image mesh graphs lifted multicuts formulations image decomposition multicut mp superpixel graph received considerable attention contrast instances mp pixel grid graph received attention firstly mp np-hard instances pixel grid graph hard solve practice lack long-range terms objective function mp propose generalization mp long-range terms lmp design implement efficient algorithms primal feasible heuristics mp lmp study instances pixel grid graphs images bsds- benchmark decompositions art suggesting lmp competitive formulation image decomposition demonstrate generality lmp apply mesh decomposition posed princeton benchmark obtaining state-of-the-art decompositions image segmentation;mesh generation;bsds- benchmark;lmp;mp;image decomposition problem;image segmentation;lifted multicut problem;long range terms;mesh decomposition problem;mesh graphs;multicut problem;pixel grid graphs;algorithm design analysis;bayes methods;benchmark testing;image decomposition;image edge detection;linear programming;optimization 
parsimonious labeling propose family discrete energy minimization call parsimonious labeling energy function consists unary potentials high-order clique potentials unary potentials arbitrary clique potentials proportional diversity set unique labels assigned clique intuitively energy function encourages labeling parsimonious labels capture cues computer vision applications stereo correspondence image denoising propose efficient graph-cuts based algorithm parsimonious labeling strong theoretical guarantees quality solution algorithm consists steps approximate diversity mixture hierarchical pn potts model divide-and-conquer approach mixture component subproblem solved efficient alpha-expansion algorithm putative labelings mixture component choose putative labeling terms energy synthetic standard real datasets algorithm outperforms graph-cuts based approaches computer vision;divide conquer methods;graph theory;image denoising;stereo image processing;potts model;computer vision applications;discrete energy minimization problems;divide-and-conquer approach;energy function;graph-cuts based algorithm;high-order clique potentials;image denoising;parsimonious labeling;stereo correspondence;unary potentials;algorithm design analysis;approximation algorithms;computer vision;labeling;measurement;random variables;robustness 
volumetric bias segmentation reconstruction: secrets solutions standard optimization methods segmentation reconstruction compute model estimates appearance geometry segments zhu-yuille torr chan-vese grabcut delong al observe standard likelihood term formu-lations corresponds generalized probabilistic k-means energy learning energy strong bias clusters equal size express penalty kl divergence uniform distribution cardinalities volumetric bias computer vision demonstrate signif- icant artifacts standard segmentation reconstruction methods bias propose binary multi-label optimization techniques remove bias replace kl divergence term target volume distribution ideas apply continuous discrete energy formulations segmenta- tion stereo reconstruction computer vision;image reconstruction;image segmentation;maximum likelihood estimation;optimisation;probability;kl divergence;ml model estimates;binary optimization technique;computer vision;multilabel optimization technique;probabilistic k-means energy;reconstruction method;segmentation method;standard likelihood term;volumetric bias;computational modeling;computer vision;entropy;optimization methods;probabilistic logic;standards 
entropy minimization convex relaxation approaches enormous success solving hard combinatorial convex relaxation approaches suffer computed solutions binary subsequent heuristic binarization degrade quality computed solutions paper propose relaxation technique incorporates entropy objective variable measure relaxation tightness theoretically experimentally augmenting objective function entropy term rise binary solutions solutions tighter optimality gap difference convex function dc programming efficient provably convergent solver arising convex-concave minimization evaluate approach prominent non-convex computer vision challenges: multi-label inpainting image segmentation spatio-temporal multi-view reconstruction experiments approach consistently yields solutions respect original integral optimization combinatorial mathematics;computer vision;concave programming;convex programming;image restoration;image segmentation;minimisation;convex function programming;convex relaxation;convex-concave minimization;entropy minimization;hard combinatorial problem;heuristic binarization;image segmentation;multilabel inpainting;nonconvex computer vision;spatio-temporal multiview reconstruction;computer vision;convex functions;entropy;image segmentation;labeling;minimization;programming 
adaptively unified semi-supervised dictionary learning active semi-supervised dictionary learning aims construct dictionary utilizing labeled unlabeled data enhance discriminative capability learned dictionary numerous discriminative terms proposed evaluating prediction loss class separation criterion coding vectors labeled data rare consideration power coding vectors unlabeled data paper semi-supervised dictionary learning method informative coding vectors labeled unlabeled data adaptively emphasizes confidence coding vectors unlabeled data enhance dictionary discriminative capability simultaneously integrate discrimination dictionary induction classifier testing data transduction labels unlabeled data unified framework solve proposed effective iterative algorithm designed experimental series benchmark databases method outperforms state-of-the-art dictionary learning methods adaptive signal processing;encoding;iterative methods;prediction theory;vectors;active points;adaptively unified semisupervised dictionary learning;class separation criterion;dictionary discrimination;dictionary discriminative capability;discriminative terms;informative coding vectors;iterative algorithm;prediction loss;data models;dictionaries;encoding;learning systems;loss measurement;solids;testing 
constrained convolutional neural networks weakly supervised segmentation approach learn dense pixel-wise labeling image-level tags image-level tag imposes constraints output labeling convolutional neural network cnn classifier propose constrained cnn ccnn method loss function optimize set linear constraints output space predicted label distribution cnn loss formulation easy optimize incorporated directly standard stochastic gradient descent optimization key idea phrase training objective biconvex optimization linear models relax nonlinear deep networks extensive experiments demonstrate generality learning framework constrained loss yields state-of-the-art weakly supervised semantic image segmentation demonstrate adding supervision greatly improve performance learning algorithm image segmentation;learning artificial intelligence ;neural nets;cnn classifier;constrained convolutional neural networks;dense pixel-wise labeling;image-level tags;learning framework;loss formulation;stochastic gradient descent optimization;weakly supervised semantic image segmentation;convolutional codes;image segmentation;labeling;neural networks;optimization;semantics;standards 
multiscale variable-grouping framework mrf energy minimization multiscale approach minimizing energy markov random fields mrfs energy functions arbitrary pairwise potentials mrf represented hierarchy successively coarser scales scale mrf suitably defined potentials representations construct efficient multiscale algorithm seeks minimal-energy solution original algorithm iterative features bidirectional crosstalk fine coarse representations consistency criteria guarantee energy nonincreasing iterative process algorithm evaluated real-world datasets achieving competitive performance short run-times image representation;mrf;mrf energy minimization;markov random field;consistency criteria;energy function;fine-coarse representation bidirectional crosstalk;iterative process;minimal-energy solution;multiscale algorithm;multiscale variable-grouping framework;pairwise potentials;crosstalk;data models;graphical models;inference algorithms;interpolation;labeling;topology 
inferring m-best diverse labelings single task finding m-best diverse solutions graphical model previous batra al algorithmic approach finding solutions proposed numerous applications contrary previous propose formulation form single energy minimization specially constructed graphical model method batra al considered greedy approximate algorithm model introduce efficient specialized optimization technique based alpha-expansion evaluate method application scenarios interactive semantic image segmentation binary multiple labels achieve considerably error rates state-of-the art diversity methods empirically discover binary label reach global optimality test instances greedy algorithms;image segmentation;optimisation;m-best diverse labelings;alpha-expansion;application scenarios;binary labels;global optimality;graphical model;greedy approximate algorithm;interactive image segmentation;multiple labels;semantic image segmentation;single energy minimization problem;specialized optimization technique;computational modeling;computer vision;diversity methods;graphical models;labeling;minimization;optimization 
convolutional sparse coding image super-resolution previous sparse coding sc based super resolution sr methods partition image overlapped patches process patch separately methods ignore consistency pixels overlapped patches strong constraint image reconstruction paper propose convolutional sparse coding csc based sr csc-sr method address consistency issue csc-sr involves parameters learned: set filters decompose low resolution lr image lr sparse feature maps ii mapping function predict resolution hr feature maps lr iii set filters reconstruct hr images predicted hr feature maps simple convolution operations directly image proposed csc-sr algorithm divide image overlapped patches exploit image global correlation produce robust reconstruction image local structures experimental validate advantages csc patch based sc sr application compared state-of-the-art sr methods proposed csc-sr method achieves highly competitive psnr demonstrating edge texture preservation performance filtering theory;image coding;image reconstruction;image resolution;image texture;csc;hr feature maps;lr sparse feature maps;sr methods;convolution operations;convolutional sparse coding;edge preservation performance;high resolution feature maps;image global correlation;image local structure reconstruction;image super-resolution;low resolution image;mapping function;overlapped patches;texture preservation performance;convolution;convolutional codes;dictionaries;encoding;image coding;image reconstruction;image resolution 
wavefront marching method solving eikonal equation cartesian grids paper wavefront propagation method dealing classic eikonal equation classic dijkstra-like graph-based techniques achieve solution log approximate unique physically relevant solution fast marching methods fmm created efficiently solve continuous proposed approximation maintain complexity algorithm wide range contexts key idea method creation 'mini wave-fronts' combined propagate solution experimental improvement accuracy respect art average computational speed maintained log fmm techniques computational complexity;computer vision;graph theory;cartesian grids;dijkstra-like graph-based technique;eikonal equation;fmm;computer vision;fast marching method;new wavefront propagation method;wavefront marching method;complexity theory;computational modeling;computer vision;estimation;mathematical model;tv;trajectory 
projection free method generalized eigenvalue nonsmooth regularizer eigenvalue ubiquitous computer vision covering broad spectrum applications ranging estimation multi-view geometry image segmentation linear algebra mature set numerical routines computer vision libraries leverage tools extensively ability call underlying solver black box\ restrictive 'human loop' settings vision frequently exploit supervision expert extent user considered subroutine system additional domain knowledge partial incorporate formulation regularizing generalized eigenvalue remains difficult motivated paper optimization scheme solve generalized eigenvalue gep involving nonsmooth regularizer start alternative formulation gep feasibility set model involves stiefel manifold core paper stochastic optimization scheme resultant algorithm enables improved statistical analysis brain imaging data regularizer derived 'views' disease pathology involving clinical measurements image-derived representations diseases;eigenvalues eigenfunctions;medical image processing;statistical analysis;stochastic programming;gep;stiefel manifold;brain imaging data;clinical measurements;computer vision;disease pathology;domain knowledge;generalized eigenvalue problem;human loop settings;image segmentation;image-derived representations;linear algebra problems;multiview geometry;nonsmooth regularizer;partial information;projection free method;statistical analysis;stochastic optimization;computer vision;computers;eigenvalues eigenfunctions;kernel;manifolds;optimization;shape 
optimizing expected intersection-over-union candidate-constrained crfs study question loss-aware predictions image segmentation settings evaluation function intersection-over-union iou measure evaluating image segmentation systems dominant approaches: approximates expected-iou eiou score expected-intersection-over-expected-union eioeu approach compute exact eiou set high-quality candidate solutions approach favor typical image segmentation tasks studying question leads methods draw ideas existing approaches methods eioeu approximation paired quality candidate solutions experimentally approaches lead improved performance image segmentation tasks approximation theory;image segmentation;optimisation;statistical analysis;crf;eiou approximation;eiou optimization;conditional random field;expected intersection-over-union;image segmentation;bayes methods;decision making;image segmentation;indexes;loss measurement;semantics 
higher-order inference multi-class log-supermodular models higher-order models plethora computer vision tasks existing techniques focused map inference paper efficient approach approximate bayesian marginal inference class high-order multi-label attractive models previous techniques slow exponentially clique size formalize task performing inference log-supermodular models partition constraints efficient variational inference technique optimization convex yield bounds partition function factorized approximation posterior lieu true complicated distribution empirically demonstrate performance approach comparing traditional inference methods challenging high-fidelity multi-label image segmentation dataset state-of-the-art classification accuracy map inference improved roc curves approximate marginals bayes methods;computer vision;image segmentation;inference mechanisms;optimisation;map inference;roc curve;approximate bayesian marginal inference;computer vision;high-fidelity multilabel image segmentation dataset;higher-order inference;multiclass log-supermodular model;multilabel attractive model;variational inference technique;computational modeling;computer vision;image segmentation;inference algorithms;optimization;probabilistic logic;uncertainty 
depth-based hand pose estimation: data methods challenges hand pose estimation matured rapidly introduction commodity depth sensors multitude practical applications spurred advances provide extensive analysis state-of-the-art focusing hand pose estimation single depth frame implemented considerable systems release software evaluation code summarize conclusions here: pose estimation appears roughly solved scenes isolated hands methods struggle analyze cluttered scenes hands interacting nearby objects surfaces spur progress introduce challenging dataset diverse cluttered scenes methods evaluate disparate criteria comparisons difficult define consistent evaluation criteria rigorously motivated human experiments introduce simple nearest-neighbor baseline outperforms existing systems implies systems generalize training sets reinforces under-appreciated training data model conclude directions future progress image sensors;pose estimation;cluttered scene analysis;commodity depth sensors;depth-based hand pose estimation;nearest-neighbor baseline;single depth frame;training data;underappreciated point;benchmark testing;cameras;clutter;data models;training;training data 
adaptive dither voting robust spatial verification hough voting geometric transformation space realize spatial verification remains sensitive feature detection errors inflexible quantization single feature correspondences handle propose method called adaptive dither voting robust spatial verification correspondence hard-mapping single transformation method augments description multiple dithered transformations deterministically generated correspondences method reduces probability losing correspondences transformation quantization robustness mismatches imposing geometric constraints dithering process propose exploiting non-uniformity hough histogram spatial similarity handle multiple matching surfaces extensive experiments conducted datasets superiority method method outperforms state-of-the-art counterparts accuracy scalability retrieval rotated objects hough transforms;feature extraction;image matching;image retrieval;probability;quantisation signal ;hough histogram nonuniformity;hough voting;adaptive dither voting;feature detection errors;geometric constraints;geometric transformation space;losing correspondence probability reduction;multiple dithered transformations;multiple matching surface handling;robust spatial verification;single feature correspondences;small rotated object retrieval;spatial similarity;transformation quantization;feature extraction;gaussian distribution;histograms;quantization signal ;robustness;scalability;visualization 
alternating co-quantization cross-modal hashing paper addresses unsupervised learning binary hash codes efficient cross-modal retrieval unimodal hashing studies proven similarity preservation data maintenance quantization quality essential improving retrieval performance binary hash codes existing cross-modal hashing methods focused remains untouched propose method minimize binary quantization errors tailored cross-modal hashing approach named alternating co-quantization acq alternately seeks binary quantizers modality space connections modality data minimal quantization errors preserving data similarities acq coupled existing cross-modal dimension reduction methods canonical correlation analysis cca boosts retrieval performance hamming space extensive experiments demonstrate acq outperform state-of-the-art methods combined simple cca hamming codes;file organisation;information retrieval;unsupervised learning;acq;cca;alternating co-quantization;binary hash code;binary quantization error;canonical correlation analysis;cross-modal dimension reduction method;cross-modal hashing method;cross-modal retrieval;hamming space;quantization quality maintenance;similarity data preservation;unimodal hashing;unsupervised learning;binary codes;computer vision;correlation;databases;encoding;quantization signal ;symmetric matrices 
learning deep representation large-scale attributes learning strong feature representations scale supervision achieved remarkable success computer vision emergence deep learning techniques driven visual data rich annotations paper contributes large-scale object attribute database rich attribute annotations attributes samples object classes based imagenet object detection dataset annotates rotation viewpoint object location occlusion existence common attributes class-specific attributes dataset train deep representations extensively evaluate attributes object detection task attribute annotations deep learning scheme proposed modeling relationship attributes hierarchically clustering semantically meaningful mixture types experimental attributes helpful learning features improving object detection accuracy map ilsvrc object detection dataset map pascal voc object detection dataset improvement generalized datasets computer vision;image representation;learning artificial intelligence ;object detection;object recognition;imagenet object detection;class-specific attribute annotation;common attribute annotation;computer vision;deep learning technique;deep representation;feature representation;large scale supervision;large-scale object attribute database;object location;part existence;part occlusion;rich attribute annotation;rotation annotation;viewpoint annotation;computer vision;databases;feature extraction;machine learning;object detection;semantics;visualization 
deep learning strong pedestrian detection advances pedestrian detection attained transferring learned features convolutional neural network convnet pedestrians convnet typically pre-trained massive object categories imagenet features handle variations poses viewpoints lightings fail pedestrian images complex occlusions occlusion handling pedestrian detection previous deep models directly learned single detector pedestrian detection propose deepparts consists extensive detectors deepparts appealing properties deepparts trained weakly labeled data pedestrian bounding boxes annotations provided deepparts handle low iou positive proposals shift ground truth detector deepparts strong detector detect pedestrian observing proposal extensive experiments caltech dataset demonstrate effectiveness deepparts yields state-of-the-art rate outperforming method convolution;learning artificial intelligence ;neural nets;object detection;pedestrians;convnet;deepparts;imagenet;iou positive proposals;convolutional neural network;extensive detectors;massive object categories;occlusion handling;pedestrian bounding boxes;pedestrian detection;pedestrian images;detectors;feature extraction;proposals;prototypes;semantics;training;training data 
flowing convnets human pose estimation videos objective human pose estimation videos multiple frames investigate convnet architecture benefit temporal context combining multiple frames optical flow propose network architecture novelties: deeper network investigated regressing heatmaps ii spatial fusion layers learn implicit spatial model iii optical flow align heatmap predictions neighbouring frames iv final parametric pooling layer learns combine aligned heatmaps pooled confidence map architecture outperforms including optical flow solely input layers regresses joint coordinates directly predicts heatmaps spatial fusion architecture outperforms art margin video pose estimation datasets including challenging poses wild dataset outperforms deep methods graphical model single-image flic benchmark precision region image sequences;neural nets;pose estimation;video signal processing;convnets;heatmap prediction;human pose estimation;optical flow;parametric pooling layer;pooled confidence map;spatial fusion layer;temporal context;video pose estimation;adaptive optics;computer architecture;heating;optical imaging;training;videos 
top rank supervised binary coding visual search binary coding techniques increasingly popular efficiency handling large-scale computer vision applications demonstrated supervised binary coding techniques leverage supervised enhance coding quality greatly benefit visual search tasks typically modern binary coding method seeks learn coding functions compress data samples binary codes methods pursued coding functions precision top ranking list hamming distances generated binary codes optimized paper propose supervised binary coding approach top rank supervised binary coding top-rsbc explicitly focuses optimizing precision top positions hamming-distance ranking list preserving supervision core idea train disciplined coding functions mistakes top hamming-distance ranking list penalized bottom solve coding functions relax original discrete optimization objective continuous surrogate derive stochastic gradient descent optimize surrogate objective reduce training time cost design online learning algorithm optimize surrogate objective efficiently empirical studies based benchmark image datasets demonstrate proposed binary coding approach achieves superior image search accuracy state-of-the-arts hamming codes;computer vision;gradient methods;image coding;image retrieval;learning artificial intelligence ;stochastic programming;hamming-distance ranking list;top-rsbc;coding functions;coding quality enhancement;data sample compression;discrete optimization;image datasets;image search accuracy;large-scale computer vision applications;online learning algorithm;precision optimization;stochastic gradient descent method;supervised information;surrogate objective optimization;top rank supervised binary coding;training time cost reduction;visual search;binary codes;computer vision;encoding;hamming distance;image coding;optimization;visualization 
bubblenet: foveated imaging visual discovery propose method internet-scale corpus categorized images set human-interpretable discriminative visual elements powerful tools based deep learning key challenge deep learning methods generating human-interpretable models address propose technique bubble images -- images content obscured -- identify spatially localized discriminative content image modifying model training procedure source imagery bubble images arrive final models retain original classification performance amenable identifying interpretable visual elements apply algorithm wide variety datasets including internet-scale datasets people applications visual mining discovery method simple scalable produces visual elements highly representative compared prior internet;computer vision;learning artificial intelligence ;bubblenet;internet scale corpus;internet scale datasets;bubble images;computer vision;deep learning methods;foveated imaging;human interpretable discriminative visual elements;identify spatially localized;training procedure;visual discovery;visual elements;visual mining;computer vision;machine learning;market research;neurons;training;training data;visualization 
pqtable: fast exact asymmetric distance neighbor search product quantization hash tables propose product quantization table pqtable product quantization-based hash table fast requires parameter tuning training steps pqtable produces linear pq search times faster tested sift data addition state-of-the-art performance achieved previous inverted-indexing-based approaches methods require manually designed parameter setting training method free pqtable offers practical solution real-world database indexing;quantisation signal ;query processing;search problems;pqtable;fast exact asymmetric distance neighbor search;hash tables;inverted-indexing-based approaches;linear pq search;product quantization-based hash table;query vector;artificial neural networks;data structures;indexing;quantization signal ;training;tuning 
lending hand: detecting hands recognizing activities complex egocentric interactions hands egocentric video appearance pose cues people paying attention existing hand detection strong assumptions simple scenarios limited interaction people lab settings develop methods locate distinguish hands egocentric video strong appearance models convolutional neural networks introduce simple candidate region generation approach outperforms existing techniques fraction computational cost high-quality bounding boxes create accurate pixelwise hand regions application investigate extent hand segmentation distinguish activities evaluate techniques dataset first-person videos people interacting realistic environments pixel-level ground truth hand instances image segmentation;neural nets;object detection;video signal processing;activity recognition;candidate region generation approach;complex egocentric interactions;convolutional neural networks;egocentric video;hand detection;hand segmentation;pixel-level ground truth;strong appearance models;cameras;computational modeling;computer vision;context;object detection;proposals;skin 
fast accurate head pose estimation random projection forests paper estimating gaze direction person low-resolution image condition reliably extracting facial features difficult propose head pose estimation algorithm based compressive sensing head image patches mapped feature space proposed extensive efficient filter bank filter bank designed generate sparse responses color gradient compressed random projection classified random forest extensive experiments challenging datasets proposed algorithm performs favorably state-of-the-art methods head pose estimation low-resolution images degraded noise occlusion blurring decision trees;face recognition;feature extraction;image resolution;pose estimation;facial features extraction;filter bank;gaze direction estimation;head image patches;head pose estimation;head pose estimation algorithm;low-resolution image;random forest;random projection;random projection forests;compressed sensing;feature extraction;head;image coding;image color analysis;vegetation 
mrf-poselets model detecting highly articulated humans detecting highly articulated objects humans challenging paper proposes part-based model built poselets notion markov random field mrf modelling human body structure variation human poses viewpoints human detection formulated maximum posteriori map estimation mrf model variational field method robust statistical inference approximate map estimation proposed method evaluated compared existing methods test sets including pascal voc experimental favourbly robustness proposed method comparison state-of-the-art markov processes;approximation theory;inference mechanisms;maximum likelihood estimation;object detection;pose estimation;h d;map estimation;mrf-poselets model;markov random field;pascal voc ;highly articulated human detection;human body structure modelling;human pose variation;human viewpoint variation;maximum-a-posteriori estimation;part-based model;robust statistical inference;biological system modeling;computational modeling;deformable models;detectors;feature extraction;legged locomotion;robustness 
tree structure models: occlusion aware graphical model human pose estimation occlusion main challenge human pose estimation popular tree structure models tree structure model simple convenient exact inference short modeling occlusion coherence self-occlusion propose occlusion aware graphical model model self-occlusion occlusion objects simultaneously proposed model structure encodes interactions human body objects enables learn occlusion coherence data discriminatively evaluate model public benchmarks human pose estimation including challenging subsets featuring occlusion experimental method obtains comparable accuracy state-of-the-arts robust occlusion human pose estimation graph theory;pose estimation;human pose estimation;new occlusion aware graphical model;occlusion aware graphical model;public benchmarks;tree structure models;brain modeling;cognition;graphical models;image edge detection;kinetic theory;message passing 
relaxing vocabulary: robust weakly-supervised deep learning vocabulary-free image tagging development deep learning empowered machines comparable capability recognizing limited image categories human existing approaches heavily rely human-curated training data hinders scalability unlabeled vocabularies image tagging paper propose weakly-supervised deep learning model trained web images relax dependence human labors scale arbitrary tags categories based assumption features true samples category tend noises tend variant embed feature map deep layer affinity representation minimize discrepancy affinity representation low-rank approximation discrepancy finally transformed objective function relevance feedback propagation experiments achieve performance gain terms semantic-based relevance metric image tagging tags wordnet typical deep model trained imagenet vocabulary set approximation theory;backpropagation;feature extraction;image representation;image retrieval;relevance feedback;imagenet;web images;wordnet;affinity representation;back propagation;feature map;human labors;image categories;low-rank approximation;objective function;relevance feedback;semantic-based relevance metric;vocabulary-free image tagging;weakly-supervised deep learning model;machine learning;noise measurement;robustness;tagging;training;training data;vocabulary 
visual phrases exemplar detection exemplar based approaches applied detection wild contrary traditional approaches model variations diverse set training examples exemplar-based approaches collection discriminatively trained exemplars detection paradigm exemplar casts vote retrieval framework generalized hough voting locate target image advantage approach database covers variations challenging conditions detected learn explicit models variations current schemes assumption independence visual ignoring relations process ignore spatial consistency visual exemplar word contributes equally voting location paper propose approach incorporates voting process discover visual phrases semantically visual exploit detection visual spatial consistency estimate spatial distribution visual phrases entire database weigh occurrence exemplars ensures visual word phrase exemplar major contribution occurs semantic location suppressing noise perform extensive experiments standard fddb afw g-album datasets improvement previous exemplar approaches hough transforms;face recognition;interference suppression;afw;g-album dataset;hough voting;discriminatively trained exemplar;exemplar detection;face variation;noise suppression;retrieval framework;semantic location;spatial consistency;spatial distribution;standard fddb;target image;visual phrase;visual word;voting process;databases;detectors;face;face detection;feature extraction;training;visualization 
spatial semantic regularisation scale object detection scale object detection thousands classes introduces contradicting false positive detections suppressed class-independent non-maximum suppression traditionally step scale classes grows traditional non-maximum suppression label-and instance-level relationships exploitation spatial layout detection proposals propose multi-class spatial semantic regularisation method based affinity propagation clustering simultaneously optimises categories proposed locations image improve localisation categorisation selected detection proposals constraints shared labels semantic wordnet hierarchy approach proves scale settings thousands classes spatial semantic interactions frequent weakly supervised detectors built lack bounding box annotations detection experiments conducted imagenet coco dataset settings thousands detected categories method precision improvement reducing false positives simultaneously improving recall image classification;object detection;coco dataset;imagenet dataset;affinity propagation clustering;bounding box annotations;class-independent nonmaximum suppression;false positive detections;instance-level relationships;label-level relationships;large scale object detection;multiclass spatial semantic regularisation method;semantic wordnet hierarchy;semantic interactions;spatial semantic regularisation;supervised detectors;clustering algorithms;detectors;message passing;object detection;proposals;semantics;visualization 
human pose estimation videos paper method estimate sequence human poses unconstrained videos contrast commonly employed graph optimization framework np-hard approximate solutions formulate unified stage tree-based optimization efficient exact solution exists proposed method exact solution sacrifice ability model spatial temporal constraints body video frames models symmetric existing methods proposed method based main ideas: 'abstraction' 'association' enforce intra-and inter-frame body constraints inducing extra computational complexity polynomial time solution idea 'abstraction' concept 'abstract body part' introduced model tree based body structure existing methods extra constraints symmetric idea 'association' optimal tracklets generated abstract body enforce spatiotemporal constraints body adjacent frames finally sequence poses inferred abstract body tracklets tree-based optimization evaluated proposed method publicly video based human pose estimation datasets dramatically improved performance compared state-of-the-art methods computational complexity;image sequences;optimisation;performance evaluation;pose estimation;spatiotemporal phenomena;trees mathematics ;video signal processing;abstract body part;abstraction;association;computational complexity;human pose sequence estimation;inter-frame body constraints;intra-frame body constraints;optimal tracklet generation;performance improvement;polynomial time solution;spatial constraints;spatiotemporal constraints;temporal constraints;tree based body structure;unconstrained videos;unified stage tree-based optimization problem;video frames;computational complexity;computational modeling;computer vision;optimization;spatiotemporal phenomena;videos 
contour box: rejecting object proposals explicit closed contours closed contour objectness indicator propose measure subject completeness tightness constraints optimized closed contour tightly bounded object proposal closed contour measure defined closed path integral solve optimization efficiently polar coordinate system global optimum guaranteed extensive experiments method reject false proposals achieve improvement object recall challenging overlap threshold voc test dataset object detection;closed contour measure;closed path integral;contour box;object detection;object proposal;optimization problem;overlap threshold;polar coordinate system;dynamic programming;image edge detection;image segmentation;object detection;optimization;proposals;search 
registering images untextured geometry average shading gradients existing approaches image-to-geometry registration assume textured model initial guess pose bootstrap registration process paper registration photographs models texture challenging rely texture gradients shading gradients hard estimate lighting conditions unknown propose average shading gradients rendering technique estimates average gradient magnitude lighting directions lambertian shading gradient representation building block registration pipeline based matching sparse features cope inevitable false matches missing texture increase robustness pose model estimated stages coarse pose hypotheses single correct match subsequently refined sift flow finally verified apply algorithm registering images real-world objects untextured meshes limited accuracy feature extraction;geometry;image matching;image registration;image representation;image texture;mesh generation;pose estimation;rendering computer graphics ;solid modelling;transforms; pose;lambertian shading;sift flow;average shading gradients;gradient representation;image registration;image-to-geometry registration;lighting directions;photograph registration;pose estimation;registration pipeline;registration process bootstrapping;rendering technique;sparse feature matching;textured model;untextured mesh;untextured geometry;cameras;computational modeling;geometry;lighting;rendering computer graphics ;solid modeling;three-dimensional displays 
robust nonrigid registration convex optimization approach nonrigid registration surfaces cast isometric embedding mrf optimization apply efficient global optimization algorithms based linear programming relaxations markov random field perspective suggests natural connection robust statistics motivates robust forms intrinsic distortion functional approach outperforms body prior margin increasing registration precision real data factor markov processes;computer graphics;convex programming;image registration;linear programming; surfaces;mrf optimization;markov random field;convex optimization;global optimization algorithms;intrinsic distortion functional;isometric embedding;linear programming relaxations;registration precision;robust nonrigid registration;computational modeling;distortion;labeling;markov processes;optimization;robustness;shape 
robust optimal sum-of-squares-based point-to-plane registration image sets structured scenes paper deals registering structured scene metric structure-from-motion sfm counterpart proposed relies prior plane segmentation scene aligns data modalities solving point-to-plane assignment inliers-maximization approach branch-and-bound bnb search scheme time paper sum-of-squares optimization theory framework employed identifying point-to-plane mismatches outliers certainty iteratively build potential inliers sets converge solution satisfied largest point-to-plane assignments approach boosted plane visibility conditions introduced paper framework solve registration cases: set putative point-to-plane correspondences overwhelmingly outliers input ii initial correspondences approach yields outstanding terms robustness optimality image motion analysis;image registration;image segmentation;optimisation;tree searching;bnb search scheme;sfm counterpart;branch-and-bound search scheme;image set;inliers-maximization approach;metric structure-from-motion counterpart;optimal sum-of-squares-based point-to-plane registration;plane visibility condition;point-to-plane assignment problem;point-to-plane mismatch;potential inliers set;prior plane segmentation;putative point-to-plane correspondence;robust sum-of-squares-based point-to-plane registration;structured scene;structured scene;sum-of-squares optimization theory framework;cameras;image sensors;iterative closest algorithm;measurement;sensors;symmetric matrices;three-dimensional displays 
meshstereo: global stereo model mesh alignment regularization view interpolation global stereo model designed view interpolation existing stereo models output disparity map model output triangular mesh directly view interpolation aim partition input stereo images triangles shared vertices lifting triangulation naturally generates mesh technical difficulty properly split vertices multiple copies depth discontinuous boundaries deal formulate objective two-layer mrf upper layer modeling splitting properties vertices lower layer optimizing region-based stereo matching experiments middlebury herodion datasets demonstrate model synthesize visually coherent view angles psnr outputting quality disparity maps rank challenging resolution middlebury benchmark image matching;image resolution;interpolation;mesh generation;stereo image processing; triangle; triangular mesh;herodion;meshstereo;middlebury benchmark;psnr;global stereo model;mesh alignment regularization;mesh generation;region-based stereo matching;stereo image;two-layer mrf;computational modeling;interpolation;mathematical model;rendering computer graphics ;solid modeling;three-dimensional displays;visualization 
cv-hazop: introducing test data validation computer vision test data plays role computer vision cv plagued questions: situations covered test data tested reach conclusion paper propose solution answering questions standard procedure devised safety community validate complex systems: hazard operability analysis hazop designed systematically search identify difficult performance-decreasing situations aspects introduce generic cv model creates basis hazard analysis time apply extensive hazop cv domain result publicly checklist identified individual hazards checklist evaluate existing test datasets quantifying amount covered hazards evaluate approach analyzing annotating popular stereo vision test datasets middlebury kitti compare performance popular stereo matching algorithms identified hazards checklist average performance expected negative influence hazards approach tool evaluate improve test datasets creates common basis future dataset designs computer vision;image matching;search problems;stereo image processing;cv-hazop;kitti;middlebury;complex systems;computer vision;generic cv model;hazard operability analysis;safety community;stereo matching algorithms;stereo vision test datasets;test data validation;benchmark testing;computer vision;context;hazards;mathematical model;robustness 
structure motion structure-less resection paper proposes incremental structure motion sfm algorithm based structure-less camera resection technique traditional methods rely d- correspondences compute pose candidate cameras pnp collection reconstructed cameras generalized camera determine absolute pose candidate pinhole camera pure correspondences call semi-generalized camera pose minimal solvers calibrated partially calibrated unknown focal length pinhole cameras integrating algorithms incremental sfm system state-of-art methods capability reconstructing cameras d- correspondences large-scale real image experiments sfm system improves completeness reconstruction standard approach calibration;cameras;computer vision;image motion analysis;image reconstruction;calibration;camera reconstruction;incremental sfm system;structure motion;structure-less camera resection technique;cameras;conferences;image reconstruction;matrix decomposition;standards;three-dimensional displays 
joint camera clustering surface segmentation large-scale multi-view stereo paper propose optimal decomposition approach large-scale multi-view stereo initial sparse reconstruction success approach depends introduction surface-segmentation-based camera clustering sparse-point-based camera clustering suffers non-uniform reconstruction coverage ratio redundancy details introduce criteria camera clustering surface segmentation reconstruction formulate criteria energy minimization constraints solve propose joint optimization hierarchical framework final surface segments optimal camera clusters level hierarchical framework camera clustering formulated parameter estimation probability model solved expectation-maximization algorithm surface segmentation formulated markov random field model based probability estimated previous camera clustering process experiments internet datasets aerial photo datasets demonstrate proposed approach method generates uniform complete dense reconstruction redundancy efficient multi-view stereo algorithm markov processes;cameras;expectation-maximisation algorithm;image reconstruction;image segmentation;minimisation;parameter estimation;pattern clustering;random processes;stereo image processing;internet datasets;markov random field model;aerial photo datasets;energy minimization problem;general expectation-maximization algorithm;hierarchical framework;image reconstruction;joint optimization;large-scale multiview stereo;nonuniform reconstruction coverage ratio;optimal camera clusters;optimal decomposition approach;parameter estimation;probability estimation;probability model;sparse reconstruction;sparse-point-based camera clustering;surface-segmentation-based camera clustering;cameras;clustering algorithms;image reconstruction;minimization;redundancy;surface reconstruction;three-dimensional displays 
higher-order crf structural segmentation reconstructed surfaces paper propose structural segmentation algorithm partition multi-view stereo reconstructed surfaces large-scale urban environments structural segments segment corresponds structural component describable surface primitive segmentation subsequent urban object modeling vectorization recognition overcome geometrical topological noise levels reconstructed urban surfaces formulate structural segmentation higher-order conditional random field crf labeling incorporates classical lower-order local cues encodes contextual geometric regularities disambiguate noisy local cues higher-order crf difficult solve develop bottom-up progressive approach patch-based surface representation iteratively evolves initial mesh triangles final segmentation iteration alternates performing prior discovery step contextual regularities patch-based representation inference step leverages regularities higher-order priors construct stable regular segmentation efficiency robustness proposed method extensively demonstrated real reconstruction models yielding performance classical mesh segmentation methods image reconstruction;image segmentation;stereo image processing; reconstructed surfaces;conditional random field;contextual geometric regularities;higher crf structural segmentation;initial mesh triangles;large-scale urban environments;multiview stereo reconstructed surfaces;patch based representation;structural component;structural segmentation algorithm;topological noise levels;face;image edge detection;image reconstruction;image segmentation;labeling;surface reconstruction;three-dimensional displays 
hyperpoints fine vocabularies large-scale location recognition structure-based localization task finding absolute pose query image pre-computed model trivial scale special care size model grows straight-forward descriptor matching ineffective memory footprint model strictness ratio test authors overcome smart compression model clever sampling strategies geometric verification explore orthogonal strategy standard sampling performs feature matching implicitly quantization fine vocabulary matching ambiguous rise hyperpoints matching query feature isolation simple voting strategy enforces selected co-visible reliably locally unique d- assignment experiments large-scale datasets demonstrate method achieves state-of-the-art performance memory footprint greatly reduced visual word labels descriptors stored data compression;feature extraction;image matching;image retrieval;image sampling;object recognition; query feature matching; d- assignment; hyperpoints; model smart compression; points;descriptor matching;fine vocabularies;geometric verification;large-scale location recognition;orthogonal strategy;query image;sampling strategies;standard sampling;structure-based localization;visual word labels;voting strategy;cameras;image reconstruction;solid modeling;three-dimensional displays;visualization;vocabulary 
globally optimal d- registration lines correspondences approach d- registration lines correspondences exist established solutions correspondences situations reliably extract correspondences modalities requiring correspondence-free registration algorithm existing correspondence-free methods rely local search strategies guarantee finding optimal solution contrast globally optimal approach d- registration correspondences achieved branch-and-bound algorithm deterministic annealing procedure proposed speed nested branch-and-bound algorithm theoretical practical advantages brings demonstrated range synthetic real data observed proposed approach robust proportions outliers compared existing approaches deterministic algorithms;image registration;tree searching;branch-and-bound algorithm;correspondence-free registration algorithm;deterministic annealing procedure;globally optimal d- registration;local search strategies;annealing;cameras;feature extraction;robustness;solid modeling;three-dimensional displays;upper bound 
hci stereo metrics: geometry-aware performance analysis stereo algorithms performance characterization stereo methods mandatory decide algorithm application prevalent benchmarks root squared error rms respect ground truth disparity maps quantify algorithm performance rms limited expressiveness algorithm selection introduce hci stereo metrics metrics assess stereo harnessing semantic cues: depth discontinuities planar surfaces fine geometric structures cue extract relevant set pixels existing ground truth apply evaluation functions quantify characteristics edge fattening surface smoothness demonstrate approach supports practitioners selecting suitable algorithm application middlebury dataset rankings based metrics reveal specific algorithm strengths weaknesses quantified existing metrics finally stacked bar charts radar charts visually support multidimensional performance evaluation interactive stereo benchmark based proposed metrics visualizations at: http://hci iwr uni-heidelberg de/stereometrics computational geometry;mean square error methods;performance evaluation;stereo image processing;hci stereo metrics;middlebury dataset;depth discontinuities;edge fattening;fine geometric structures;geometry-aware performance analysis;ground truth disparity maps;multidimensional performance evaluation;planar surfaces;radar charts;root squared error;semantic cues;stacked bar charts;stereo algorithms;surface smoothness;algorithm design analysis;benchmark testing;human computer interaction;object detection;performance evaluation;surface reconstruction 
merging unmatchable: stitching visually disconnected sfm models advances structure-from-motion enable reconstruction scale scenes detect ambiguous structures caused repeating elements result incorrect reconstructions reconstruct scene images required merge sub-models missing impossible acquire images occlusions structure scene aligning multiple reconstructions visual overlap impossible solve variant individual building reconstructed joined missing visual overlap paper combinatorial approach solving variant automatically stitching multiple building approach exploits symmetries semantic reason geometric relations individual models approach reconstruct complete building models traditional sfm disconnected building image motion analysis;image reconstruction;sfm models;combinatorial approach;feature matching;image reconstruction;structure-from-motion;buildings;computational modeling;image reconstruction;merging;solid modeling;three-dimensional displays;visualization 
fragment reassembly integrated template guidance fracture-region matching paper studies matching fragmented objects recompose original geometry solving geometric reassembly direct applications archaeology forensic investigation computer-aided restoration damaged artifacts evidence develop algorithm effectively integrate guidance template matching adjacent pieces' fracture-regions compute partial matchings fragments template pairwise matchings fragments potential matches selected/refined multi-piece matching stage maximize global groupwise matching consistency pipeline effective composing fragmented thin-shell objects pieces pairwise matching unreliable ambiguous reassembly remains challenging existing algorithms computational geometry;computer graphics;image matching; fragment reassembly;adjacent pieces fracture regions;computer aided restoration;fracture region matching;fragmented object matching;geometric reassembly problem;integrated template guidance;partial matchings;thin shell objects;feature extraction;forensics;geometry;pipelines;shape;surface cracks;three-dimensional displays 
procedural editing building clouds advances computational photography remote sensing clouds buildings increasingly processing poses challenges tackle cloud completion editing approach inverse procedural modeling contrary previous approach operates directly cloud intermediate triangulation approach consists semi-automatic segmentation input cloud segment comparison template matching detect repeating structures consensus-based voting schema pattern extraction algorithm discover completed terminal geometry patterns usage encoded context-free grammar interactive editing tool user create clouds procedural copy paste operations smart resizing demonstrate approach editing building models implementation preprocessing takes minutes single editing operation minute depending model size operation type computational geometry;feature extraction;human computer interaction;image matching;image segmentation;interactive systems;object detection; building clouds;completed terminal geometry discovery;computational photography;consensus-based voting schema;context-free grammar;interactive editing tool;inverse procedural modeling;pattern extraction algorithm;point cloud completion problem;point cloud editing problem;procedural copy operation;procedural editing;procedural paste operation;remote sensing;repeating structure detection;segment comparison;semi automatic input cloud segmentation;smart resizing;template matching;buildings;computational modeling;geometry;grammar;solid modeling;surface reconstruction;three-dimensional displays 
semantically-aware aerial reconstruction multi-modal data methodology integrating multiple sensors semantic enhance scene representations propose probabilistic generative model inferring semantically-informed aerial reconstructions multi-modal data consistent mathematical framework approach called semantically-aware aerial reconstruction saar exploits inferred scene geometry appearance semantic observations meaningful categorization data extends proposed methods imposing structure prior geometry appearance semantic labels leads accurate reconstructions ability fill missing contextual labels joint sensor semantic introduce multi-modal synthetic dataset provide quantitative performance analysis additionally apply model real-world data exploit openstreetmap source semantic observations quantitative improvements reconstruction accuracy large-scale urban scenes combination lidar aerial photography semantic data demonstrate model's ability fill missing sensed data leading interpretable reconstructions geographic systems;image reconstruction;image representation;optical radar;photography;sensor fusion;lidar;openstreetmap;saar;aerial photography;consistent mathematical framework;contextual label;data categorization;joint sensor;large-scale urban scene;multimodal data;multimodal synthetic dataset;multiple sensor;performance analysis;probabilistic generative model;quantitative improvement;real-world data;reconstruction accuracy;scene geometry;scene representation;semantic data;semantic information;semantic observation;semantically-aware aerial reconstruction;semantically-informed aerial reconstruction;geometry;image reconstruction;laser radar;probabilistic logic;semantics;solid modeling;three-dimensional displays 
guaranteed outlier removal rotation search rotation search core routine solving computer vision aim rotationally align input sets correspondences developing globally optimal rotation search algorithms notable weakness global algorithms computational cost sizes data proportion outliers paper propose outlier removal technique rotation search method guarantees correspondence discards outlier exist inlier set globally optimal rotation original data based simple geometric operations algorithm deterministic fast preprocessor prune portion outliers input data method enables substantial speed-up rotation search algorithms compromising global optimality demonstrate efficacy method synthetic real data experiments computer vision;geometry;search problems;set theory;computer vision problems;geometric operations;globally optimal rotation search algorithms;guaranteed outlier removal;computational efficiency;computer vision;runtime;search problems;three-dimensional displays;uncertainty;upper bound 
peeking template matching depth extension propose method extends depth image regions visible view camera algorithm detects repeated structures visible scene suggests set extension hypotheses combined global mrf discrete optimization recovered global surface consistent input depth map hypotheses key component template matcher detect repeated structure scene hypotheses unique property matcher handle depth uncertainty crucial matcher required peek corner\ operates boundaries visible scene depth missing proposed matcher fast guaranteed approximation globally optimal solution demonstrate real-world data algorithm capable completing scene single depth image synthesize depth map viewpoint scene addition report extensive synthetic set shapes evaluate method qualitatively quantitatively markov processes;computer graphics;image matching; extension hypotheses; template matcher; vision;depth extension;depth image;global mrf discrete optimization;global surface recovery;input depth map;peeking template matching;repeated structure detection;cameras;geometry;optimization;shape;surface reconstruction;three-dimensional displays;uncertainty 
deformable fusion: partial dynamic observations complete models capturing motion dynamic non-rigid objects attracted attention computer vision existing methods typically require complete volumetric observations shape template paper introduce template-less reconstruction method incrementally fuses highly-incomplete observations deforming object generates complete temporally-coherent shape representation object design online algorithm alternatively registers observations current model estimate updates model demonstrate effectiveness approach reconstructing non-rigidly moving objects highly-incomplete measurements sequences partial clouds kinect videos computer vision;image capture;image fusion;image motion analysis;image sensors;shape recognition; models;kinect videos;computer vision;deformable fusion;dynamic nonrigid object motion capturing;nonrigidly moving object reconstruction;object deformation;partial clouds;partial dynamic observations;shape template;template-less reconstruction method;temporally-coherent shape representation;deformable models;dynamics;laplace equations;shape;solid modeling;surface reconstruction;three-dimensional displays 
non-parametric structure-based calibration radially symmetric cameras propose two-step method estimating intrinsic extrinsic calibration radially symmetric camera including non-central systems step consists estimating camera pose structure motion sfm model translation optical axis step calibration finding translation camera center constraint method radial camera model effectively handle radially symmetric camera including non-central constraint calibrate central non-central wide field view wfov cameras including fisheye hyper-catadioptric spherical catadioptric cameras pinhole cameras single image jointly solving views calibration;cameras;motion estimation; radial camera model;sfm model;wfov cameras;camera center translation;camera pose estimation;extrinsic calibration;fisheye cameras;hypercatadioptric cameras;intrinsic calibration;noncentral systems;nonparametric structure-based calibration;ordering constraint;pinhole cameras;radially symmetric cameras;spherical catadioptric cameras;structure motion model;wide field view cameras;calibration;cameras;distortion;distortion measurement;optical distortion;optical imaging;three-dimensional displays 
exploiting object similarity reconstruction progress reconstructing outdoor scenes movable platforms remains highly difficult endeavour challenges low frame rates occlusions distortions difficult lighting conditions paper leverage larger reconstructed objects type shape occur scene true outdoor scenes buildings vehicles suffer missing texture reflections share similarity shape advantage shape similarity localizing objects detectors jointly reconstructing learning volumetric model shape reduce noise completing missing surfaces objects shape benefit observations respective category evaluate approach respect lidar ground truth challenging suburban dataset advantages state-of-the-art image reconstruction;learning artificial intelligence ;optical radar;radar imaging; reconstruction; shape;lidar ground truth;movable platforms;noise reduction;object localization;object similarity;outdoor scene reconstruction;shape similarity;suburban dataset;volumetric model learning;buildings;image reconstruction;proposals;shape;solid modeling;surface reconstruction;three-dimensional displays 
here: mimicking human thinking process reading floor-plans human easily unfamiliar building walking reading floor-plan mimic automate human thinking process precisely introduce task locating user floor-plan camera floor-plan prior address matching-localization algorithm inspired human logic demonstrate experiments method outperforms state-of-the-art floor-plan-based localization methods margin highly efficient real-time applications biomimetics;buildings structures ;floors;structural engineering computing;floor-plan-based localization methods;floor-plans;human logic;human thinking process;matching-localization algorithm;unfamiliar building;buildings;cameras;computer vision;image reconstruction;reliability;sensors;three-dimensional displays 
map disparity estimation hidden markov trees method introduced stereo matching operates minimum spanning trees msts generated images disparity maps represented collection hidden msts mst modeled hidden markov tree efficient recursive message-passing scheme designed operate hidden markov trees upward-downward algorithm compute maximum posteriori map disparity estimate pixel messages processed upward-downward algorithm involve types probabilities: probability pixel disparity set per-pixel matching costs probability disparity transition pair connected pixels similarity distributions probabilities modeled collection images ground truth disparities performance evaluation middlebury stereo benchmark version demonstrates proposed method ranks terms accuracy evaluated training test image sets hidden markov models;image matching;message passing;stereo image processing;trees mathematics ;map disparity estimation;middlebury stereo benchmark version ;hidden markov trees;maximum posteriori disparity;minimum spanning trees;recursive message-passing scheme;stereo matching;upward-downward algorithm;algorithm design analysis;computational modeling;estimation;hidden markov models;image color analysis;image edge detection;message passing 
wide baseline stereo matching convex bounded distortion constraints finding correspondences wide baseline setups challenging existing approaches focused developing feature descriptors correspondence accurate recovery epipolar constraints paper focuses challenging finding correspondences approximate epipolar constraints introduce method integrates deformation model formulate finding largest bounded distortion map obeys epipolar constraints set bounded distortion maps convex subset maps obey epipolar constraints convex allowing introduce efficient algorithm matching utilize robust cost function matching employ majorization-minimization optimization experiments method accurate maps existing approaches convex programming;feature extraction;image matching;minimisation;stereo image processing;approximate epipolar constraints;bounded distortion map;convex bounded distortion constraints;cost function;deformation model;feature descriptors;majorization-minimization;optimization;point correspondences;wide baseline setups correspondences;wide baseline stereo matching;cameras;deformable models;distortion;image analysis;optimization;robustness;transmission matrix methods 
interactive visual hull refinement specular transparent object surface reconstruction paper method standard multi-view images surface reconstruction non-lambertian objects extend original visual hull concept incorporate cues internal occluding contours occluding contours inside object's silhouettes discovered internal contours convex object's surface lead tighter fit original visual hull formulated visual hull refinement scheme -- locally convex carving completely reconstruct concavity caused intersecting convex surfaces addition develop approach contour tracking labeled contours sparse key frames designed highly specular transparent objects assumptions traditional contour detection/tracking methods gradient stationary texture edges valid formulated energy minimization function terms developed increase robustness based core algorithms developed interactive system modeling validated system quantitatively qualitatively datasets object materials generate visually pleasing models challenging image reconstruction;image texture;surface reconstruction; cues; surface reconstruction;contour tracking;convex carving;energy minimization function;gradient edges;interactive visual hull refinement;internal occluding contours;non-lambertian objects;object materials;sparse key frames;specular object surface reconstruction;standard multiview images;stationary texture edges;transparent object surface reconstruction;image edge detection;image reconstruction;image segmentation;shape;surface reconstruction;three-dimensional displays 
hierarchical higher-order regression forest fields: application indoor scene labelling paper addresses semantic segmentation indoor scenes reconstructed rgb-d images traditionally label prediction tackled employing graphical models capture scene features complex relations class labels existing restricted pairwise conditional random fields insufficient encoding rich scene context propose models higher-order potentials complex relational scenes relax labelling regression generalize higher-order associative potts model family arbitrary higher-order models based regression forests models robust models decomposed sum pairwise terms introducing auxiliary variables proposed higher-order models permit extension hierarchical random fields integration scene context features computed scales potential functions constructed based regression forests encoding gaussian densities admit efficient inference parameters model learned training data structured learning approach datasets improvements current state-of-the-art methods gaussian processes;potts model;image colour analysis;image reconstruction;image segmentation;learning artificial intelligence ;regression analysis; indoor scene labelling; indoor scenes reconstruction; points;gaussian densities;potts model;rgb-d images;graphical models;hierarchical higher-order regression forest fields;label prediction;pairwise conditional random fields;rich scene context;semantic segmentation;structured learning;computational modeling;context modeling;labeling;robustness;semantics;solid modeling;three-dimensional displays 
classical scaling revisited multidimensional-scaling mds analysis tool involves evaluation distances data quadratic space-time mds procedures embedding low dimensional euclidean flat domain optimizing similarity inter-points distances efficient solver classical scaling specific mds model extending distances measured subset rest exploiting smoothness property distance functions smoothness measured norm laplace-beltrami operator applied unknown distance function laplace beltrami reflects local differential relations computed linear time classical-scaling reformulated quasi-linear space-time complexities procedure computational complexity;information analysis;laplace beltrami;laplace-beltrami operator;mds procedure;dimensional euclidean domain;distance function;information analysis tool;multidimensional-scaling;quadratic space-time problem;quasi-linear space-time complexity procedure;smoothness property;complexity theory;interpolation;manifolds;matrix decomposition;principal component analysis;sparse matrices;spectral analysis 
dense continuous-time tracking mapping rolling shutter rgb-d cameras propose dense continuous-time tracking mapping method rgb-d cameras parametrize camera trajectory continuous b-splines optimize trajectory dense direct image alignment method directly models rolling shutter rgb depth images optimization improves tracking reconstruction quality low-cost cmos sensors continuous trajectory representation advantages discrete-time representation camera poses frame interval splines variables optimized discrete representation trajectory represented fewer control frames splines naturally smoothness constraints derivatives trajectory estimate finally continuous trajectory representation compensate rolling shutter effects pose estimate exposure time image approach demonstrates superior quality tracking reconstruction compared approaches discrete-time global shutter assumptions slam robots ;estimation theory;image representation;optimisation;pose estimation;splines mathematics ;b-spline;slam;camera trajectory representation;continuous trajectory representation;dense continuous-time tracking;image alignment;pose estimation;rolling shutter rgb-d camera;simultaneous localization mapping;trajectory estimation;trajectory optimization;cameras;image reconstruction;simultaneous localization mapping;splines mathematics ;three-dimensional displays;trajectory 
dense image registration deformable surface reconstruction presence occlusions minimal texture deformable surface tracking monocular images well-known under-constrained occlusions task challenging result failure surface textured explicitly address reconstruction textured occluded surfaces proposing framework based template-matching approach scales dense robust features relevancy score approach extensively compared current methods employing local feature matching dense template alignment test standard datasets dataset publicly sparsely textured occluded surface framework achieves state-of-the-art textured occluded surfaces image matching;image reconstruction;image registration;image texture; reconstruction;deformable surface reconstruction;deformable surface tracking;dense image registration;dense template alignment;local feature matching;minimal texture;monocular images;occluded surface;sparsely textured surface;template-matching approach;cameras;image reconstruction;robustness;shape;surface reconstruction;surface texture;three-dimensional displays 
likelihood-ratio test efficient robust estimation robust estimation model parameters presence outliers key computer vision ransac inspired techniques context application limited priori knowledge inlier noise level propose approach jointly optimizing model parameters inlier noise level based likelihood ratio test control type error incurred propose bailout strategy efficiency tests synthetic real data method outperforms state-of-the-art fraction time computational geometry;computer vision;ransac inspired techniques;computer vision;likelihood-ratio test;robust estimation;computational modeling;computer vision;data models;maximum likelihood estimation;noise level;robustness 
reflection modeling passive stereo stereo reconstruction presence reality challenges addressed paper considers reflections introduce incorrect matches observation violating diffuse-world assumption underlying majority stereo techniques existing employ regularization robust data terms suppress errors derive squares models principles generalize diffuse stereo explicitly reflections account models parametrized depth orientation material properties total parameters pixel estimated additionally non-local interactions viewed reflected surface account properties inference model prohibitive evidence inference variant patch match stereo image matching;image reconstruction;least squares methods;optimisation;stereo image processing;least square model;material property;optimization;patch match stereo matching;robust data;stereo reconstruction;cameras;image color analysis;mirrors;optimization;robustness;rough surfaces;surface roughness 
detailed full-body reconstructions moving people monocular rgb-d sequences accurately estimate geometry appearance human body monocular rgb-d sequence user moving freely front sensor range data frame brought alignment multi-resolution body model coarse-to-fine process method geometry image texture time accurate shape pose appearance unconstrained motion partial views varying resolution occlusion soft tissue deformation body model variable shape detail allowing capture high-resolution deformable head model body shape lower-resolution finally combine range data entire sequence estimate high-resolution displacement map captures fine shape details compare recovered models high-resolution scans professional system avatars created commercial product extract accurate avatars challenging motion sequences capture soft tissue dynamics recognition;geometry;image colour analysis;image motion analysis;image reconstruction;image resolution;image sequences;image texture; geometry;coarse-to-fine process;detailed full-body reconstructions;image texture;monocular rgb-d sequences;motion sequences;moving people;multiresolution body model;occlusion;soft tissue deformation;avatars;data models;deformable models;image reconstruction;sensors;shape;three-dimensional displays 
efficient solution epipolar geometry radially distorted cameras estimation epipolar geometry cameras image matches fundamental computer vision applications closely estimating relative pose uncalibrated cameras radial distortion published methods suitable practical applications solutions numerically unstable sensitive noise based correspondences simply slow real-time applications paper efficient solution image correspondences manipulating ten input polynomial equations derive degree polynomial equation variable solutions equation efficiently sturm sequences method experiments proposed solution stable noise resistant fast efficiently usable practical structure-from-motion pipeline cameras;computer vision;image matching;image sequences;polynomials;stereo image processing;sturm sequence method;computer vision;epipolar geometry;image matching;polynomial equation;radially distorted camera;structure-from-motion pipeline;cameras;distortion;eigenvalues eigenfunctions;estimation;geometry;mathematical model;real-time systems 
learning descriptor-specific keypoint detector keypoint detection represents stage majority modern computer vision pipelines based automatically established correspondences local descriptors standard solution emerged data clouds meshes exhibit variability level detail noise importantly existing proposals keypoint detection rely geometric saliency functions attempt maximize repeatability distinctiveness selected regions lead sub-optimal performance pipeline overcome shortcomings cast keypoint detection binary classification support correctly matched predefined descriptor learning descriptor-specific detector adapts seamlessly scenarios experiments public datasets approach design keypoint detector represents flexible solution provide state-of-the-art descriptor matching performance computer vision;geometry;image classification;image matching;learning artificial intelligence keypoint detection;binary classification;computer vision pipelines;descriptor matching performance;descriptor-specific keypoint detector learning;geometric saliency functions;local descriptors;public datasets;suboptimal performance;computer vision;detectors;feature extraction;robustness;standards;three-dimensional displays;training 
component-wise modeling articulated objects introduce framework modeling articulated objects based aspects components decomposing object components divide modeling tasks obtaining models component aspect employing shape deformation paradigm merge forming object components final model assembling components optimization scheme fits respective models apparent contours reference pose approach produce realistic models articulated objects reasonable time computer vision;optimisation;solid modelling; models;articulated objects;component-wise modeling;optimization scheme;shape deformation paradigm;computational modeling;deformable models;feature extraction;shape;solid modeling;surface treatment;three-dimensional displays 
collaborative filtering approach real-time hand pose estimation collaborative filtering aims predict unknown user ratings recommender system collectively assessing user preferences paper draw analogies collaborative filtering pose estimation recast hand pose estimation cold-start user unknown item ratings recommender system inspired fast accurate matrix factorization techniques collaborative filtering develop real-time algorithm estimating hand pose rgb-d data commercial depth camera efficiently identify nearest neighbors local shape descriptors rgb-d domain library hand poses pose parameter values evaluate unknown pose parameters joint matrix factorization completion jmfc approach quantitative qualitative approach robust variation hand configurations achieving real time performance fps standard computer cameras;collaborative filtering;image colour analysis;image retrieval;matrix decomposition;pose estimation;real-time systems;recommender systems;shape recognition;jmfc approach;rgb-d data;cold-start problem;collaborative filtering;depth camera;item ratings;joint matrix factorization completion approach;local shape descriptors;nearest neighbors;pose parameter values;real-time algorithm;real-time hand pose estimation;recommender system;user preferences;user ratings;cameras;libraries;real-time systems;recommender systems;shape;three-dimensional displays 
equivalence moving entrance pupil radial distortion camera calibration radial distortion ordinary non-fisheye camera lenses traditionally modeled infinite series function radial location image pixel image center empirical evidence model accurate sufficient radial distortion calibration analysis geometric/physical understanding radial distortion camera calibration perspective paper thick-lens imaging model variation entrance pupil location function incident image ray angle directly responsible radial distortion captured images proposed current state-of-the-art camera calibration radial distortion entrance pupil movement equivalent modeled modeling entrance pupil motion radial distortion achieve main benefits comparable pixel re-projection error traditional methods importantly directly back-project radially distorted image pixel true image ray formed thick-lens setting back-projection accurate two-step method undistorting image pixel back-projecting applied calibration method generative depth-from-focus focal stack accurate depth estimates calibration;image processing;image sensors;camera calibration;camera calibration perspective;entrance pupil location;image center;image pixel;image ray angle;infinite series function;moving entrance pupil;ordinary non-fisheye camera lenses;radial distortion calibration;radial location;thick lens imaging model;thick-lens setting;analytical models;apertures;calibration;cameras;distortion;lenses 
linear generalized camera calibration intersecting reference planes paper generalized ray-pixel raxel camera calibration algorithm camera systems involving distortions unknown refraction reflection processes key idea intersections calibration planes conventional methods utilized collinearity constraints planes intersections calibration planes realize simple linear algorithm method applied ray-distributions conventional methods require knowing ray-distribution class advance evaluations synthesized real datasets demonstrate performance method quantitatively qualitatively calibration;cameras;computer vision;calibration plane intersections;collinearity constraints;intersecting reference planes;linear generalized camera calibration;raxel camera calibration algorithm;ray-distributions;ray-pixel camera calibration algorithm;calibration;cameras;distortion;geometry;linear systems;solid modeling;three-dimensional displays 
pointless structure motion: reconstruction camera parameters curves modern structure motion sfm remains dependent features recover camera positions meaning reconstruction severely hampered low-texture environments scanning plain coffee cup uncluttered table curves refine camera position estimation challenging low-texture scenes contrast previous curves partially observed images meaning time curve-based sfm demonstrated realistic scenes algorithm based bundle adjustment initial estimate poor estimate correspondences improved including curves suggesting method benefit existing systems image motion analysis;image reconstruction;image texture; curves; reconstruction;bundle adjustment;camera parameters;camera position estimation;curve-based sfm;low-texture scenes;structure motion;calibration;cameras;data models;image reconstruction;shape;splines mathematics ;three-dimensional displays 
attributed grammars joint estimation human attributes pose paper developing compositional models explicit representing pose attributes tackling tasks attribute recognition pose estimation localization jointly trend cnn-based approaches training testing tasks separately amount data conventional attribute models typically region-based attribute classifiers pre-trained pose estimator explicitly detecting object correlations attributes contrast approach jointly represents object semantic attributes unified compositional hierarchy apply attributed grammar model task human parsing simultaneously performing localization attribute recognition modeling helps performance improvements pose-estimation task outperforms existing methods attribute prediction task attribute grammars;object recognition;pose estimation;cnn;attribute recognition;attributed grammar model;human attributes;joint estimation;pose estimation;pretrained pose estimator;data models;geometry;grammar;hair;predictive models;torso;training 
real-time pose estimation piggybacked object detection object detector coupled pose estimation directly single compact simple model detector shares extracted image features pose estimator output classification candidate window consists object score likelihood map poses extension introduces negligible overhead detection detector capable real time operation evaluated proposed approach vehicle detection existing datasets viewpoint/pose annotation wcvp objects kitti collected traffic surveillance dataset cod fills gaps existing datasets public experimental proposed approach comparable state-of-the-art approaches terms accuracy considerably faster easily operating real time matlab c++ code source codes collected cod dataset public paper feature extraction;object detection;road vehicles;traffic engineering computing; objects;c++ code;cod k;kitti;matlab;wcvp;image features extraction;object detection;pose annotation;real-time pose estimation;vehicle detection;viewpoint annotation;cameras;detectors;feature extraction;real-time systems;three-dimensional displays;training 
understanding predicting image memorability scale progress estimating visual memorability limited scale lack variety benchmark data introduce experimental procedure objectively measure human memory allowing build lamem largest annotated image memorability dataset images diverse sources convolutional neural networks cnns fine-tuned deep features outperform features margin reaching rank correlation human consistency analysis responses high-level cnn layers objects regions positively negatively correlated memorability allowing create memorability maps image provide concrete method perform image memorability manipulation demonstrates robustly estimate memorability images classes positioning memorability deep memorability features prime candidates estimate utility cognitive systems model data at: http://memorability csail mit correlation methods;feature extraction;image retrieval;neural nets;prediction theory;lamem;convolutional neural networks;deep memorability features;high-level cnn layers;human consistency;human memory;image memorability prediction;image memorability understanding;largest annotated image memorability dataset;memorability maps;rank correlation;visual memorability;benchmark testing;computer vision;correlation;delay effects;delays;games;visualization 
multiple granularity descriptors fine-grained categorization fine-grained categorization aims distinguish subordinate-level categories bird species dog breeds extremely challenging task main issues: localize discriminative regions recognition learn sophisticated features representation easy handle insufficient labeled data leverage subordinate-level object labels ontology tree free\ labels train series cnn-based classifiers specialized grain level internal representations networks region allowing construction multi-grained descriptors encode informative discriminative features covering grain levels multiple granularity framework learned weakest supervision requiring image-level label avoiding labor-intensive bounding box annotations experimental challenging fine-grained image datasets demonstrate approach outperforms state-of-the-art algorithms including requiring strong labels image classification;ontologies artificial intelligence ;trees mathematics ;cnn-based classifiers;fine-grained categorization;fine-grained image datasets;labor-intensive bounding box;multiple granularity descriptors;ontology tree;part annotations;birds;computer vision;feature extraction;heating;ontologies;semantics;vegetation 
guiding long-short term memory model image caption generation focus image caption generation propose extension short term memory lstm model coin glstm short add semantic extracted image extra input unit lstm block aim guiding model solutions tightly coupled image content additionally explore length normalization strategies beam search avoid bias short sentences benchmark datasets flickr flickr ms coco par current state-of-the-art feature extraction;image processing;modelling;semantic networks;lstm model;image caption generation;length normalization strategy;long-short term memory model;semantic extraction;computer architecture;logic gates;microprocessors;pipelines;semantics;training;visualization 
noticeable differences visual attributes explore predicting noticeable differences\ visual attribute pairs images attribute sporty difference indistinguishable human observers existing relative attribute models unequipped infer partial data attempting map relative attribute ranks equality predictions non-trivial span indistinguishable pairs attribute space vary feature space develop bayesian local learning strategy infer images indistinguishable attribute ut-zap shoes lfw- datasets outperform variety alternative methods addition practical impact fine-grained visual search bayes methods;face recognition;feature extraction;image retrieval;learning artificial intelligence ;bayesian local learning strategy;lfw- dataset;ut-zap shoes dataset;attribute space;feature space;fine-grained visual search;just noticeable differences prediction;relative attribute mapping;visual attributes;bayes methods;footwear;image color analysis;image recognition;observers;training;visualization 
vqa: visual question answering propose task free-form open-ended visual question answering vqa image natural language question image task provide accurate natural language answer mirroring real-world scenarios helping visually impaired questions answers open-ended visual questions selectively target image including background details underlying context result system succeeds vqa typically detailed understanding image complex reasoning system producing generic image captions vqa amenable automatic evaluation open-ended answers closed set answers provided multiple-choice format provide dataset images questions answers visualqa org numerous baselines vqa provided compared human performance question answering retrieval ;free-form vqa;generic image captions;human performance;multiple-choice format;natural language answer;open-ended answers;open-ended visual question answering;cognition;glass;image color analysis;knowledge discovery;measurement;visualization 
localize anytime: multi-task point-retrieval approach image-based localization essential complement gps localization current image-based localization methods based d-to- d-to- correspondences ignore real scene geometric attributes main contribution paper model reconstructed short video query realize d-to- localization multi-task retrieval framework firstly model query enables efficiently select location candidates reconstruction model exploits correlations images based images captured views sfm share matching features exploring shared matching features multiple tasks images scene captured views visual feature's view-invariance property improved retrieval accuracy multi-task retrieval framework explore relationship descriptors extracts discriminant accurate d-to- correspondences retrieval apply multi-task learning mtl retrieval approach thermal images prove mtl retrieval framework superior performance thermal domain application exceptionally helpful cope localization environment limited light sources computer graphics;feature extraction;image matching;image reconstruction;video retrieval;video signal processing; d-to- d; model reconstruction; points; d-to- d; d-to- correspondences retrieval; d-to- localization;gps localization;mtl retrieval framework;sfm;image-based localization methods;localization problem;matching features;multitask learning retrieval;multitask retrieval framework;real scene geometric attributes;short video;thermal images;cameras;geometry;image reconstruction;solid modeling;surface reconstruction;three-dimensional displays;training 
dense optical flow prediction static image scene move direction move question considered non-semantic form action prediction convolutional neural network cnn based approach motion prediction static image cnn predicts future motion pixel image terms optical flow cnn model leverages data tens thousands realistic videos train model method relies absolutely human labeling predict motion based context scene cnn model assumptions underlying scene predict future optical flow diverse set scenarios outperform previous approaches margins image sequences;motion estimation;neural nets;cnn;convolutional neural network;dense optical flow prediction;future motion prediction;motion prediction;static image;context;neural networks;optical imaging;optical losses;predictive models;trajectory;videos 
unsupervised domain adaptation zero-shot learning zero-shot learning zsl considered special transfer learning source target domains tasks/label spaces target domain unlabelled providing guidance knowledge transfer zsl method typically assumes domains share common semantic representation space visual feature vector extracted image/video projected/embedded projection function existing approaches learn projection function source domain apply adaptation target domain based naive knowledge transfer learned projections prone domain shift paper zsl method proposed based unsupervised domain adaptation formulate regularised sparse coding framework target domain class labels' projections semantic space regularise learned target domain projection effectively overcoming projection domain shift extensive experiments object action recognition benchmark datasets proposed zsl method outperforms state-of-the-arts feature extraction;unsupervised learning;video signal processing;zsl method;naive knowledge transfer;projection function;regularised sparse coding;semantic representation space;transfer learning;unsupervised domain adaptation;visual feature vector;zero-shot learning;adaptation models;birds;encoding;feature extraction;prototypes;semantics;visualization 
visual madlibs: fill blank description generation question answering paper introduce dataset consisting focused natural language descriptions images dataset visual madlibs dataset collected automatically produced fill-in-the-blank templates designed gather targeted descriptions about: people objects appearances activities interactions inferences scene broader context provide analyses visual madlibs dataset demonstrate applicability description generation tasks: focused description generation multiple-choice question-answering images experiments joint-embedding deep learning methods promising tasks image retrieval;natural language processing;question answering retrieval ;description generation tasks;fill blank description generation;fill-in-the-blank templates;multiple-choice question-answering;natural language descriptions;targeted descriptions;visual madlibs dataset;computer vision;context;explosions;knowledge discovery;natural languages;videos;visualization 
actions attributes wholes investigate tasks action attribute classification develop part-based approach leveraging convolutional network features inspired advances computer vision detectors deep version poselets capture human body distinct set poses tasks action attribute classification train holistic convolutional neural networks adding leads top-performing tasks observe deeper networks addition demonstrate effectiveness approach replace oracle person detector default current evaluation protocol tasks state-of-the-art person detection system computer vision;convolution;feature extraction;image classification;neural nets;cnn;action classification;attribute classification;computer vision;convolutional network network;part detection;part-based approach;person detection system;birds;computer vision;detectors;feature extraction;legged locomotion;object detection;training 
deepbox: learning objectness convolutional networks existing object proposal approaches bottom-up cues rank proposals objectness\ level construct argue data-driven semantic approach ranking object proposals framework call deepbox convolutional neural networks cnns rerank proposals bottom-up method four-layer cnn architecture larger networks task evaluating objectness faster deepbox improves bottom-up ranking achieving recall proposals achieved bottom-up methods improvement generalizes categories cnn leads -point gain detection map implementation achieves performance running ms image neural nets;object detection;deepbox;bottom-up ranking;convolutional neural network;four-layer cnn architecture;learning objectness;object proposals;computer architecture;image edge detection;image segmentation;proposals;semantics;training 
active object localization deep reinforcement learning active detection model localizing objects scenes model class-specific agent focus attention candidate regions identifying correct location target object agent learns deform bounding box simple transformation actions goal determining specific location target objects top-down reasoning proposed localization agent trained deep reinforcement learning evaluated pascal voc dataset agents guided proposed model localize single instance object analyzing regions image detection systems object proposals object localization learning artificial intelligence ;object detection;object recognition;pascal voc ;active detection model;active object localization;deep reinforcement learning;computational modeling;history;learning artificial intelligence ;prediction algorithms;proposals;search problems;transforms 
scene-domain active models object representation paper enhancing expressivity robustness part-based models object representation common scenario training data based images propose scene-domain active models sdapm reconstruct characterize geometric statistics object's scene-domain training data image-domain top explicitly model handle occlusions sdapm developed learning inference algorithms model rich object descriptions including object localization landmark shape camera viewpoint offers effective representation image understanding tasks object detection landmark shape viewpoint estimation images experiments tasks sdapm outperforms previous part-based models demonstrates potential proposed technique computational geometry;image representation;inference mechanisms;learning artificial intelligence ;object detection;statistics; images; object localization; geometric statistics; landmark shape viewpoint estimation; scene-domain;image understanding tasks;inference algorithms;learning algorithms;object detection;object representation;scene-domain active models;cameras;data models;deformable models;shape;solid modeling;three-dimensional displays;training data 
unified multiplicative framework attribute learning attributes mid-level semantic properties objects visual attributes benefit traditional learning computer vision community attribute learning challenging attributes predictable directly input images variation visual attributes categories paper propose unified multiplicative framework attribute learning tackles key images category jointly projected shared feature space latent factors disentangled multiplied attribute prediction attribute classifier category-specific shared categories method leverage auxiliary data enhance predictive ability attribute classifiers reducing effort instance-level attribute annotation extent experimental method achieves superior performance instance-level category-level attribute prediction zero-shot learning based attributes method improves state-of-the-art performance awa dataset achieves comparable performance cub dataset computer vision;image classification;learning artificial intelligence ;awa dataset;cub dataset;attribute classifier;attribute learning;category-level attribute prediction;computer vision community;instance-level;unified multiplicative framework;zero-shot learning;correlation;learning systems;object recognition;predictive models;semantics;training;visualization 
contractive rectifier networks nonlinear maximum margin classification optimal nonlinear separating boundary maximum margin input data space paper proposes contractive rectifier networks crns hidden-layer transformations restricted contraction mappings contractive constraints ensure achieved separating margin input space larger equal separating margin output layer training proposed crns formulated linear support vector machine svm output layer combined contractive hidden layers effective algorithms proposed address optimization challenges arising contraction constraints experimental mnist cifar- cifar- mit- datasets demonstrate proposed contractive rectifier networks consistently outperform conventional unconstrained rectifier network counterparts image classification;neural nets;nonlinear programming;object recognition;support vector machines;crn;svm;contraction mapping;contractive rectifier network;deep learning network;hidden-layer transformation;nonlinear maximum margin classification;object classification;optimization;support vector machine;aerospace electronics;australia;neurons;nonlinear distortion;support vector machines;training 
augmenting strong supervision web data fine-grained categorization propose method fine-grained object recognition employs part-level annotations deep convolutional neural networks cnns unified framework schemes boost recognition performance difficulty acquiring detailed annotations supervised fine-grained datasets pace rapid evolution cnn architectures paper solve exploiting inexhaustible web data proposed method improves classification accuracy ways: discriminative cnn feature representations generated training set augmented collecting patches weakly supervised web images robust object classifiers learned multi-instance learning algorithm jointly strong weak datasets simplicity proposed method delivers remarkable performance improvement cub dataset compared baseline part-based r-cnn methods achieves accuracy dataset absence test image annotations internet;feature extraction;image classification;image retrieval;learning artificial intelligence ;neural net architecture;object recognition;performance evaluation;cnn architectures;cub dataset;web data;classification accuracy improvement;deep convolutional neural networks;discriminative cnn feature representations;fine-grained categorization;fine-grained object recognition;multiinstance learning algorithm;part-level annotations;performance improvement;recognition performance;robust object classifiers;strong supervision augmentation;strongly supervised fine-grained datasets;weakly supervised web images;computer architecture;detectors;feature extraction;proposals;robustness;training;training data 
learning child: fast visual concept learning sentence descriptions images paper address task learning visual concepts interactions concepts images sentence descriptions linguistic context visual features method efficiently hypothesize semantic meaning add word dictionary images concepts method image captioning module based improvements propose transposed weight sharing scheme improves performance image captioning model suitable concept learning task propose methods prevent overfitting concepts addition concept datasets constructed task publicly project experiments method effectively learns visual concepts examples disturbing learned concepts project is: stat ucla edu/junhua mao/projects/child_learning html image processing;learning artificial intelligence ;semantic networks;image captioning module;image sentence description;semantic meaning;visual concept learning;weight sharing scheme;word dictionary;adaptation models;computational modeling;computer vision;dictionaries;semantics;training;visualization 
learning common sense visual abstraction common sense essential building intelligent machines commonsense knowledge explicitly stated human-generated text learnt mining web unwritten unnecessary unnatural write commonsense unwritten commonsense knowledge unseen visual structure modeled commonsense knowledge machines learn common sense simply observing visual requires automatic accurate detection objects attributes poses interactions objects remain challenging key insight visual common sense depicted visual content semantic features relevant low-level pixel photorealism learn common sense explore human-generated abstract scenes clipart learning common sense reason plausibility interaction relation pair nouns measuring similarity relation nouns relations nouns abstract scenes commonsense knowledge learn complementary learnt sources text data mining;learning artificial intelligence ;web mining;common sense learning;human-generated text;intelligent machines;semantic features;visual abstraction;visual common sense;visual content;cognition;data mining;grounding;libraries;semantics;training;visualization 
domain generalization object recognition multi-task autoencoders domain generalization knowledge acquired domains training data apply unseen domains propose feature learning algorithm multi-task autoencoder mtae generalization performance cross-domain object recognition algorithm extends standard denoising autoencoder framework substituting artificially induced corruption naturally occurring inter-domain variability appearance objects reconstructing images noisy versions mtae learns transform original image analogs multiple domains learns features robust variations domains learnt features inputs classifier evaluated performance algorithm benchmark image recognition datasets task learn features multiple datasets predict image label unseen datasets denoising mtae outperforms alternative autoencoder-based models current state-of-the-art algorithms domain generalization image denoising;learning artificial intelligence ;object recognition;mtae;cross-domain object recognition;domain generalization;feature learning algorithm;image recognition;multitask autoencoder;standard denoising autoencoder;feature extraction;image reconstruction;noise reduction;object recognition;robustness;standards;training 
square localization efficient accurate object detection key contribution paper compact square object localization relaxes exhaustive sliding window testing windows combinations aspect ratios square object localization category scalable binary search strategy scales test reduced empirically log min rounds sliding cnns image height width training phase square cnn models object co-presence priors learned testing phase sliding cnn models applied produces set response maps effectively filtered learned co-presence prior output final bounding boxes localizing object performed extensive experimental evaluation voc datasets demonstrate efficient square localization output precise bounding boxes improve final detection result convolution;image filtering;neural nets;object detection;search problems;binary search strategy;convolutional neural network;object detection;response map filtering;sliding cnn model;square object localization;computer vision;graphics processing units;object detection;proposals;search problems;testing;training 
box aggregation proposal decimation: mile object detection regions-with-convolutional-neural-network rcnn commonly employed object detection pipeline main steps proposal generation convolutional neural network cnn feature extraction intensively investigated focus step system aggregate thousands scored box proposals final object prediction call proposal decimation step enhanced simple box aggregation function statistical properties proposals respect ground truth objects method extremely light-weight computation yields improvement map pascal voc test explain statistics paper convolution;feature extraction;neural nets;object detection;statistical analysis;pascal voc test;rcnn;box aggregation function;feature extraction;ground truth objects;object detection;object prediction;proposal decimation;regions-with-convolutional-neural-network;statistical properties;computational modeling;correlation;feature extraction;mercury metals ;object detection;pipelines;proposals 
deepproposal: hunting objects cascading deep convolutional layers paper evaluate quality activation layers convolutional neural network cnn generation object proposals generate hypotheses sliding-window fashion activation layers final convolutional layers object recall poor localization coarseness feature maps layers network localize object reduced recall based observation design method proposing object locations based cnn features combines worlds build inverse cascade final initial convolutional layers cnn selects promising object locations refines boxes coarse-to-fine manner method efficient features extracted detection ii aggregates features integral images iii avoids dense evaluation proposals inverse coarse-to-fine cascade method accurate outperforms proposed object proposals approaches plugged cnn-based detector produces state-of-the-art detection performance feature extraction;neural nets;object detection;cnn features;cnn-based detector;deepproposal;activation layer quality evaluation;convolutional neural network;deep convolutional layers;feature extraction;feature map coarseness;hypothesis generation;integral images;inverse cascade;object localization;object proposal generation;state-of-the-art detection performance;aggregates;detectors;feature extraction;image edge detection;object detection;pipelines;proposals 
semantic segmentation object clique potential propose object clique potential semantic segmentation object clique potential addresses misclassified object-part issues arising solutions based fully-convolutional networks object clique set compared yielded segment-proposal-based approaches size method consume notably computation system design model formation object clique potential regarded functional complement local-appearance-based crf models synergy effective approaches performance improvement extensive experiments verify method image recognition;image segmentation;object recognition;fully-convolutional networks;local-appearance-based crf models;model formation;object clique potential;object clique set;semantic segmentation;synergy;computational modeling;image segmentation;labeling;object detection;proposals;semantics;training 
automatic concept discovery parallel text visual corpora humans connect language vision perceive build connection computers visual concepts text terms relate visually discriminative entities propose automatic visual concept discovery algorithm parallel text visual corpora filters text terms based visual discriminative power images concepts visual semantic similarities illustrate applications discovered concepts bidirectional image sentence retrieval task image tagging task discovered concepts outperform sets manually selected concepts achieves state-of-the-art performance retrieval task image retrieval;information filtering;parallel processing;text analysis;automatic visual concept discovery algorithm;bidirectional image;image tagging task;parallel text corpora;parallel visual corpora;semantic similarities;sentence retrieval task;text term filtering;visual similarities;visually discriminative entities;bicycles;detectors;roads;semantics;visualization;vocabulary 
simpler non-parametric methods provide multiple-instance learning multiple-instance learning mil unique learning training data labels collections objects called bags individual objects called instances plethora approaches developed solve popular methods diverse density milis dd-svm methods computer vision attempted fairly sophisticated solutions solve unique configurations mil space paper analyze mil feature space modified versions traditional non-parametric techniques parzen window k-nearest-neighbour develop learning approach employing distances k-nearest neighbours feature space methods published methods benchmark datasets compare contrast analysis well-established diverse-density approach variants literature benchmark datasets including musk andrews' corel datasets diabetic retinopathy pathology diagnosis dataset experimental demonstrate enjoying intuitive interpretation supporting fast learning method potential delivering improved performance complex data arising real-world applications computer vision;image classification;learning artificial intelligence ;optimisation;support vector machines;mil;svm;computer vision;image classification;multiple-instance learning;nonparametric method;optimization;support vector machine;benchmark testing;computer vision;noise measurement;pathology;prototypes;support vector machines;training 
monocular object instance segmentation depth cnns paper tackle instance-level segmentation depth single monocular image goal advantage convolutional neural nets train directly predict instance-level segmentations instance encodes depth image patches provide coherent single explanation image develop markov random field takes input predictions convolutional neural nets applied overlapping patches resolutions output connected component algorithm aims predict accurate instance-level segmentation depth demonstrate effectiveness approach challenging kitti benchmark performance tasks markov processes;image resolution;image segmentation;cnn;kitti benchmark;markov random field;coherent single explanation;connected component algorithm;depth ordering;image patch;instance encode;monocular object instance segmentation;single monocular image;automobiles;image resolution;image segmentation;labeling;minimization;neural networks;three-dimensional displays 
multimodal convolutional neural networks matching image sentence paper propose multimodal convolutional neural networks m-cnns matching image sentence m-cnn end-to-end framework convolutional architectures exploit image representation word composition matching relations modalities consists image cnn encoding image content matching cnn modeling joint representation image sentence matching cnn composes semantic fragments learns inter-modal relations image composed fragments levels exploit matching relations image sentence experimental demonstrate proposed m-cnns effectively capture image sentence matching proposed m-cnns outperform state-of-the-art approaches bidirectional image sentence retrieval flickr flickr datasets image matching;image representation;image retrieval;natural language processing;neural nets;flickr dataset;flickr dataset;image content;image encoding;image matching;image representation;image retrieval;m-cnn;multimodal convolutional neural network;sentence matching;sentence retrieval;word composition;computer architecture;convolution;grounding;image representation;natural languages;neural networks;semantics 
structural kernel learning scale multiclass object co-detection exploiting contextual relationships images proven key improve object detection object co-detection algorithms fail exploit correlations multiple classes scalability reasons limited modeling object instance similarity low-dimensional hand-crafted features address multiclass object co-detection scale datasets formulate co-detection joint multiclass labeling object candidates class-independent manner exploit correlations objects build fully-connected crf candidates explicitly incorporates geometric layout relations object classes similarity relations multiple images introduce structural boosting algorithm exploits rich high-dimensional deep network features learn object similarity fully-connected crf experiments pascal voc evidences benefits approach object detection rcnn single-image crf methods state-of-the-art co-detection algorithms learning artificial intelligence ;object detection;rcnn;conditional random fields;fully-connected crf;joint multiclass labeling;multiclass object co-detection;object similarity learning;single-image crf methods;structural boosting algorithm;structural kernel learning;computer vision;conferences 
flickr entities: collecting region-to-phrase correspondences richer image-to-sentence models flickr dataset standard benchmark sentence-based image description paper flickr entities augments captions flickr coreference chains linking mentions entities images manually annotated bounding boxes entity annotation essential continued progress automatic image description grounded language understanding experiments demonstrating annotations text-to-image reference resolution task localizing textual entity mentions image bidirectional image-sentence retrieval experiments confirm improve accuracy state-of-the-art retrieval methods training explicit region-to-phrase correspondence time accurately inferring correspondence image caption remains challenging image resolution;image retrieval;text analysis;flickr dataset;flickr entities;automatic image description;bidirectional image-sentence retrieval;coreference chains;grounded language understanding;image-to-sentence models;manually annotated bounding boxes;region-to-phrase correspondences;sentence-based image description;text-to-image reference resolution;textual entity mentions;benchmark testing;glass;grounding;image color analysis;image resolution;standards;training 
predicting depth surface normals semantic labels common multi-scale convolutional architecture paper address computer vision tasks single basic architecture: depth prediction surface normal estimation semantic labeling multiscale convolutional network adapt easily task modifications regressing input image output map directly method progressively refines predictions sequence scales captures image details superpixels low-level segmentation achieve state-of-the-art performance benchmarks tasks computer vision;convolution;prediction theory;computer vision tasks;depth prediction;multiscale convolutional architecture;multiscale convolutional network;semantic labeling;surface normal estimation;adaptation models;estimation;image segmentation;labeling;semantics;spatial resolution 
attentionnet: aggregating weak directions accurate object detection detection method deep convolutional neural network cnn named attentionnet cast object detection iterative classification suitable form cnn attentionnet quantized weak directions target object ensemble iterative predictions attentionnet converges accurate object boundary box attentionnet unified network object detection detects objects separated models object proposal post bounding-box regression evaluate attentionnet human detection task achieve state-of-the-art performance ap pascal voc -layered architecture convolution;learning artificial intelligence ;neural nets;object detection;attentionnet;cnn;deep convolutional neural network;object detection;agriculture;computer architecture;computer vision;object detection;predictive models;proposals;training 
common subspace model similarity: phrase learning caption generation images generating captions images fundamental combines computer vision natural language processing focus descriptive phrases white dog\ explain visual composites input image phrases express objects attributes events relations reduce visual complexity caption input image generated connecting estimated phrases grammar model phrases combinations phrases larger single accuracy phrase estimation suffers training samples phrase paper propose phrase-learning method: common subspace model similarity cosmos overcome shortage training samples cosmos obtains subspace feature vectors phrase mapped mutually close classifiers phrase learned training samples shared co-occurring phrases experimental demonstrate system accurate earlier accuracy increases dataset web increases feature extraction;learning artificial intelligence ;cosmos;caption generation;common subspace model similarity;cooccurring phrases;feature vectors;grammar model;phrase-learning method;feature extraction;grammar;learning systems;neural networks;scalability;training;visualization 
d-assisted feature synthesis views object comparing images views long-standing challenging computer vision visual features stable view paper single input image object synthesize features views leveraging existing modestly-sized model collection identical objects accomplish study relationship image patches views object seeking call surrogate patches -- patches view feature content predicts features patch view based surrogate relationships create feature sets views latent object patch basis providing augmented multi-view representation object provide theoretical empirical analysis feature synthesis process evaluate augmented features fine-grained image retrieval/recognition instance retrieval tasks experimental synthesized features enable view-independent comparison images perform traditional approaches respect image recognition;image representation;image retrieval; d-assisted feature synthesis;image patches;image recognition;image retrieval;instance retrieval;object augmented multiview representation;computer vision;correlation;estimation;shape;solid modeling;three-dimensional displays;visualization 
render cnn: viewpoint estimation images cnns trained rendered model views object viewpoint estimation images essential task computer vision issues hinder progress: scarcity training data viewpoint annotations lack powerful features inspired growing availability models propose framework address issues combining render-based image synthesis cnns convolutional neural networks models potential generating images variation exploited deep cnn learning capacity goal propose scalable overfit-resistant image synthesis pipeline cnn tailored viewpoint estimation task experimentally viewpoint estimation pipeline outperform state-of-the-art methods pascal d+ benchmark computer vision;neural nets;rendering computer graphics ;solid modelling; model view rendering;cnn;computer vision;convolutional neural networks;object viewpoint estimation;overfit-resistant image synthesis pipeline;render-based image synthesis;viewpoint estimation task;computational modeling;deformable models;estimation;pipelines;solid modeling;three-dimensional displays;training 
lost shopping monocular localization indoor spaces paper propose approach localization indoor spaces store shopping malls takes single image floor plan environment input formulate localization inference markov random field jointly reasons text detection localizing shop's names image precise bounding boxes shop facade segmentation camera's rotation translation entire shopping mall power approach prior appearance exploits text detections shop names method applicable variety domains robust store appearance variation countries seasons illumination conditions demonstrate performance approach dataset collected shopping malls power holistic reasoning markov processes;cameras;cartography;image segmentation;markov random field;bounding box;camera rotation;camera translation;floor plan;holistic reasoning;illumination condition;indoor space;localization problem;lost shopping;monocular localization;shop facade segmentation;store appearance variation;store shopping mall;text detection;cameras;global positioning system;image segmentation;layout;lighting;robustness;three-dimensional displays 
camera pose voting large-scale image-based localization image-based localization approaches aim determine camera pose image finding correct d- correspondences query image features scene model harder size model increases current state-of-the-art methods combine elaborate matching schemes camera pose estimation techniques handle fractions wrong matches study benefits limitations spatial verification compared appearance-based filtering propose voting-based pose estimation strategy exhibits complexity matches facilitates matches previous approaches complexity grows quadratically outlier rejection formulation enables evaluate pose estimation -to-many matches surpass state-of-the-art time matches automatically lead performance cameras;computational complexity;image filtering;image matching;pose estimation; d- correspondences;appearance-based filtering;camera pose estimation techniques;camera pose voting;image matching schemes;large-scale image-based localization;outlier rejection formulation;query image features;spatial verification;voting-based pose estimation strategy;cameras;computational modeling;gravity;shape;solid modeling;three-dimensional displays 
mantra: minimum maximum latent structural svm image classification ranking propose weakly supervised learning wsl framework dedicated learn discriminative detectors images annotated global label wsl method encompasses main contributions firstly introduce structured output latent variable model minimum maximum latent structural svm mantra prediction relies pair latent variables: h+ resp h- positive resp negative evidence output instantiate mantra visual recognition tasks: multi-class classification ranking ranking propose efficient solutions solve inference loss-augmented finally extensive experiments highlight relevance proposed method: mantra outperforms state-of-the art datasets convolution;image classification;learning artificial intelligence ;minimax techniques;neural nets;support vector machines;cnn;mantra;wsl framework;convolutional neural network;image classification;image ranking;minimum maximum latent structural svm;support vector machine;weakly supervised learning;detectors;libraries;optimization;predictive models;support vector machines;training;visualization 
deepdriving: learning affordance direct perception autonomous driving major paradigms vision-based autonomous driving systems: mediated perception approaches parse entire scene driving decision behavior reflex approaches directly map input image driving action regressor paper propose paradigm: direct perception approach estimate affordance driving propose map input image key perception indicators directly relate affordance road/traffic driving representation set compact complete descriptions scene enable simple controller drive autonomously falling extremes mediated perception behavior reflex argue direct perception representation level abstraction demonstrate train deep convolutional neural network recording hours human driving video game model drive car diverse set virtual environments train model car distance estimation kitti dataset direct perception approach generalize real driving images source code data project website computer games;computer vision;neural nets;road traffic;traffic engineering computing;virtual reality;kitti dataset;behavior reflex;car distance estimation;convolutional neural network;direct perception representation;mediated perception approaches;road state;traffic state;video game;virtual environments;vision-based autonomous driving systems;automobiles;games;neural networks;roads;robots;testing;training 
active transfer learning zero-shot priors: reusing datasets future tasks reuse existing knowledge form datasets solving unrelated target task set unlabeled data contribution answer question context image classification frame quest active learning zero-shot classifiers guide learning process linking task existing classifiers revisiting dual formulation adaptive svm reveal basic conditions choose greedily relevant samples annotated basis propose effective active learning algorithm learns target classification model minimum human labeling effort extensive experiments challenging datasets approach compared state-of-the-art active learning methodologies potential reuse datasets minimal effort future tasks image classification;learning artificial intelligence ;support vector machines;active learning algorithm;active learning methodology;active learning problem;active transfer learning;adaptive svm;dual formulation;human labeling effort;image classification;learning process;target classification model;target task;unlabeled data;zero-shot classifier;zero-shot prior;computer vision;context;focusing;labeling;predictive models;support vector machines;training 
hd-cnn: hierarchical deep convolutional neural networks scale visual recognition image classification visual separability object categories highly uneven categories difficult distinguish difficult categories demand dedicated classifiers existing deep convolutional neural networks cnn trained flat n-way classifiers efforts leverage hierarchical structure categories paper introduce hierarchical deep cnns hd-cnns embedding deep cnns two-level category hierarchy hd-cnn separates easy classes coarse category classifier distinguishing difficult classes fine category classifiers hdcnn training component-wise pretraining global fine-tuning multinomial logistic loss regularized coarse category consistency term addition conditional executions fine category classifiers layer parameter compression hd-cnns scalable largescale visual recognition achieve state-of-the-art cifar large-scale imagenet -class benchmark datasets experiments build two-level hd-cnns lower top- error standard cnns convolution;image classification;neural nets;cifar datasets;hd-cnn;imagenet -class benchmark datasets;coarse category classifier;fine category classifiers;flat n-way classifiers;hierarchical deep convolutional neural networks;image classification;large scale visual recognition;layer parameter compression;visual separability;computer architecture;feature extraction;neural networks;probabilistic logic;training;training data;visualization 
learning structure deep convolutional networks develop method automatically learning aspects structure deep model improve performance labeled training data scarce propose convolutional neural network model indian buffet process ibp prior termed ibpcnn ibpcnn automatically adapts structure provided training data achieves optimal balance model complexity data fidelity training loss offers generalization performance proposed ibpcnn captures complicated data distribution unsupervised generative ibpcnn exploit unlabeled data -- collected low cost -- learn structure determining structure ibpcnn learns parameters tasks end-to-end fashion produces discriminative compact representations evaluate performance ibpcnn fully-and semi-supervised image classification tasks ibpcnn surpasses standard cnn models benchmark datasets size efficiency computational complexity;image classification;learning artificial intelligence ;neural nets;cnn models;ibp prior;indian buffet process;convolutional neural network model;data distribution;data fidelity;deep convolutional networks;ibpcnn;model complexity;semisupervised image classification;training loss;adaptation models;complexity theory;convolutional codes;data models;neural networks;training;training data 
flownet: learning optical flow convolutional networks convolutional neural networks cnns successful variety computer vision tasks linked recognition optical flow estimation tasks cnns succeeded paper construct cnns capable solving optical flow estimation supervised learning task propose compare architectures: generic architecture including layer correlates feature vectors image locations existing ground truth data sets train cnn generate synthetic flying chairs dataset networks trained unrealistic data generalize existing datasets sintel kitti achieving competitive accuracy frame rates fps computer vision;image sequences;learning artificial intelligence ;neural nets;cnn;flownet;computer vision;convolutional neural network;generic architecture;ground truth data;optical flow learning;opticalflow estimation problem;supervised learning;synthetic flying chair dataset;computer architecture;correlation;image resolution;neural networks;optical computing;optical fiber networks;optical imaging 
learning semi-supervised representation unified optimization framework semi-supervised learning art approaches semi-supervised learning ssl follow two-stage framework -- constructing affinity matrix data propagating partial labels affinity matrix infer unknown labels two-stage framework successful applications solving subproblems separately suboptimal exploit correlation affinity labels paper formulate stages ssl unified optimization framework learns affinity matrix unknown labels simultaneously unified framework labels estimated labels learn affinity matrix infer unknown labels solve unified optimization alternating direction method multipliers combined label propagation extensive experiments synthetic data set benchmark data sets demonstrate effectiveness approach learning artificial intelligence ;optimisation;pattern classification;ssl;affinity matrix;data classification;label propagation;multipliers alternating direction method;semisupervised representation learning;unified optimization framework;buildings;correlation;heating;kernel;optimization;semisupervised learning;sparse matrices 
context-guided diffusion label propagation graphs existing approaches diffusion graphs label propagation focused isotropic diffusion induced commonly-used graph laplacian regularizer inspired success diffusivity tensors anisotropic diffusion image processing anisotropic diffusion graphs label propagation algorithm develop positive definite diffusivity operators vector bundles riemannian manifolds discretize diffusivity operators graphs enables easily define robust diffusivity operators improve semi-supervised learning performance existing diffusion algorithms graph theory;image processing;learning artificial intelligence ;riemannian manifolds;anisotropic diffusion;context-guided diffusion;diffusivity tensors;graph laplacian regularizer;image processing;label propagation;positive definite diffusivity operators;semisupervised learning performance;vector bundles;anisotropic magnetoresistance;diffusion processes;eigenvalues eigenfunctions;image edge detection;laplace equations;manifolds;semisupervised learning 
learning rank based subsequences supervised learning rank algorithm effectively images exploiting structure image sequences supervised learning rank literature ranking approached analysing pairs images optimizing list-wise surrogate loss function sequences propose midrank learns moderately sized sub-sequences sub-sequences structural ranking leads learnability training generalization testing exploiting sub-sequences proposed midrank improves ranking accuracy considerably extensive array image ranking applications datasets image sequences;learning artificial intelligence ;midrank;image ranking application;image sequence;learnability;list-wise surrogate loss function;rank algorithm;rank literature;ranking accuracy;structural ranking information;supervised learning;image sequences;loss measurement;optimization;supervised learning;testing;training 
unsupervised learning visual representations videos strong supervision learning visual representation millions semantically-labeled images train convolutional neural network cnn paper simple surprisingly powerful approach unsupervised learning cnn hundreds thousands unlabeled videos web learn visual representations key idea visual tracking supervision patches connected track visual representation deep feature space belong object object design siamese-triplet network ranking loss function train cnn representation single image imagenet unlabeled videos voc dataset train ensemble unsupervised networks achieves map bounding box regression performance tantalizingly close imagenet-supervised counterpart ensemble achieves map unsupervised network perform competitively tasks surface-normal estimation image representation;neural nets;unsupervised learning;video signal processing;cnn;imagenet;siamese-triplet network;voc dataset;convolutional neural network;deep feature space;semantically-labeled images;unsupervised learning;visual representation;clustering algorithms;semantics;tracking;training;unsupervised learning;videos;visualization 
nonparametric bayesian approach stacked convolutional independent component analysis unsupervised feature learning algorithms based convolutional formulations independent components analysis ica demonstrated yield state-of-the-art action recognition benchmarks existing approaches latent components features automatically inferred data unsupervised manner disadvantage state-of-the-art considerable burden imposed researchers practitioners resort tedious cross-validation procedures optimal latent features resolve issues paper introduce convolutional nonparametric bayesian sparse ica architecture overcomplete feature learning high-dimensional data method utilizes indian buffet process prior facilitate inference latent features hybrid variational inference algorithm scalable massive datasets model naturally deep unsupervised hierarchical feature extractors greedily stacking successive model layers existing approaches addition inference model completely heuristics-free obviates tedious parameter tuning major challenge deep learning approaches faced evaluate method action recognition benchmarks exhibit advantages state-of-the-art bayes methods;convolution;feature extraction;image recognition;independent component analysis;indian buffet process;action recognition benchmarks;convolutional formulations;convolutional nonparametric bayesian sparse ica architecture;cross-validation procedures;deep learning;deep unsupervised hierarchical feature extractors;heuristics-free;high-dimensional data;hybrid variational inference algorithm;massive datasets;nonparametric bayesian approach;overcomplete feature learning;parameter tuning;stacked convolutional independent component analysis;successive model layers;unsupervised feature learning algorithms;adaptation models;bayes methods;data models;feature extraction;inference algorithms;machine learning;training 
robust principal component analysis graphs principal component analysis pca tool linear dimensionality reduction clustering highly sensitive outliers scale respect data samples robust pca solves issue sparse penalty term issue handled matrix factorization model non-convex pca based clustering enhanced graph data similarity article introduce model called 'robust pca graphs' incorporates spectral graph regularization robust pca framework proposed model benefits robustness principal components occlusions missing values enhanced low-rank recovery improved clustering property graph smoothness assumption low-rank matrix convexity optimization extensive experiments benchmark video artificial datasets corruptions reveal model outperforms state-of-the-art models clustering low-rank recovery tasks data reduction;graph theory;matrix decomposition;optimisation;principal component analysis;data similarity;linear dimensionality reduction clustering;matrix factorization model;robust pca;robust principal component analysis;spectral graph regularization;benchmark testing;data models;laplace equations;manifolds;principal component analysis;robustness;sparse matrices 
